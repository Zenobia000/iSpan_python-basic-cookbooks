{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 04: æ–‡å­—è™•ç†å·¥å…·ç®± - å®Œæ•´åƒè€ƒè§£ç­”\n",
    "\n",
    "## ğŸ“š è§£ç­”èªªæ˜\n",
    "\n",
    "æœ¬æª”æ¡ˆæä¾› Milestone 04 çš„å®Œæ•´åƒè€ƒå¯¦ä½œï¼Œå±•ç¤ºï¼š\n",
    "- **Ch12-15** æ ¸å¿ƒæ¦‚å¿µçš„æ­£ç¢ºæ‡‰ç”¨\n",
    "- æœ€ä½³å¯¦è¸çš„ç¨‹å¼è¨­è¨ˆæ–¹æ³•\n",
    "- å®Œæ•´çš„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶\n",
    "- è©³ç´°çš„ç¨‹å¼ç¢¼è¨»è§£èˆ‡èªªæ˜\n",
    "\n",
    "### ğŸ¯ å­¸ç¿’é‡é»\n",
    "- **Ch12**: å‡½å¼è¨­è¨ˆã€æ¨¡çµ„åŒ–ã€æ–‡ä»¶å­—ä¸²\n",
    "- **Ch13**: ä½œç”¨åŸŸã€é–‰åŒ…ã€å…§åµŒå‡½å¼\n",
    "- **Ch14**: map/filter/reduceã€Lambdaã€é«˜éšå‡½å¼\n",
    "- **Ch15**: éè¿´æ€ç¶­ã€çµ‚æ­¢æ¢ä»¶ã€åˆ†æ²»æ³•\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ æ¨¡çµ„ 1: æ–‡å­—çµ±è¨ˆæ¨¡çµ„ (Statistics)\n",
    "\n",
    "### ğŸ’¡ Ch12 å±•ç¤ºé‡é»ï¼šå‡½å¼è¨­è¨ˆåŸºç¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_characters(text):\n",
    "    \"\"\"\n",
    "    çµ±è¨ˆæ–‡å­—å­—å…ƒæ•¸é‡ï¼ˆå«ç©ºæ ¼èˆ‡æ¨™é»ï¼‰\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - å–®ä¸€è·è²¬åŸå‰‡ï¼šåªè² è²¬å­—å…ƒçµ±è¨ˆ\n",
    "    - æ¸…æ¥šçš„è¼¸å…¥è¼¸å‡ºè¦æ ¼\n",
    "    - å®Œæ•´çš„é‚Šç•Œæ¢ä»¶è™•ç†\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦çµ±è¨ˆçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: åŒ…å«å„ç¨®å­—å…ƒçµ±è¨ˆè³‡è¨Š\n",
    "        {\n",
    "            'total_chars': int,      # ç¸½å­—å…ƒæ•¸\n",
    "            'alphabetic': int,       # å­—æ¯æ•¸é‡\n",
    "            'numeric': int,          # æ•¸å­—æ•¸é‡\n",
    "            'whitespace': int,       # ç©ºç™½å­—å…ƒæ•¸\n",
    "            'punctuation': int       # æ¨™é»ç¬¦è™Ÿæ•¸\n",
    "        }\n",
    "    \n",
    "    ç¯„ä¾‹:\n",
    "        >>> count_characters(\"Hello, World! 123\")\n",
    "        {'total_chars': 17, 'alphabetic': 10, 'numeric': 3, 'whitespace': 2, 'punctuation': 2}\n",
    "    \"\"\"\n",
    "    # Ch12 æœ€ä½³å¯¦è¸ï¼šé‚Šç•Œæ¢ä»¶è™•ç†\n",
    "    if not text:  # è™•ç†ç©ºå­—ä¸²æˆ– None\n",
    "        return {\n",
    "            'total_chars': 0,\n",
    "            'alphabetic': 0,\n",
    "            'numeric': 0,\n",
    "            'whitespace': 0,\n",
    "            'punctuation': 0\n",
    "        }\n",
    "    \n",
    "    # Ch12 æœ€ä½³å¯¦è¸ï¼šæ¸…æ¥šçš„è®Šæ•¸å‘½å\n",
    "    total_chars = len(text)\n",
    "    alphabetic = 0\n",
    "    numeric = 0\n",
    "    whitespace = 0\n",
    "    \n",
    "    # Ch12 æœ€ä½³å¯¦è¸ï¼šé‚è¼¯æ¸…æ™°çš„è™•ç†æµç¨‹\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            alphabetic += 1\n",
    "        elif char.isdigit():\n",
    "            numeric += 1\n",
    "        elif char.isspace():\n",
    "            whitespace += 1\n",
    "        # å…¶ä»–å­—å…ƒè¦–ç‚ºæ¨™é»ç¬¦è™Ÿ\n",
    "    \n",
    "    # è¨ˆç®—æ¨™é»ç¬¦è™Ÿæ•¸é‡ï¼ˆç¸½æ•¸æ¸›å»å…¶ä»–é¡å‹ï¼‰\n",
    "    punctuation = total_chars - alphabetic - numeric - whitespace\n",
    "    \n",
    "    return {\n",
    "        'total_chars': total_chars,\n",
    "        'alphabetic': alphabetic,\n",
    "        'numeric': numeric,\n",
    "        'whitespace': whitespace,\n",
    "        'punctuation': punctuation\n",
    "    }\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    çµ±è¨ˆè©å½™æ•¸é‡èˆ‡è©é »\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - è¤‡é›œé‚è¼¯çš„åˆ†æ­¥é©Ÿè™•ç†\n",
    "    - è³‡æ–™æ¸…ç†èˆ‡é©—è­‰\n",
    "    - å­—å…¸æ“ä½œçš„æœ€ä½³å¯¦è¸\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: è©å½™çµ±è¨ˆçµæœ\n",
    "    \"\"\"\n",
    "    if not text.strip():  # è™•ç†ç©ºç™½å­—ä¸²\n",
    "        return {\n",
    "            'total_words': 0,\n",
    "            'unique_words': 0,\n",
    "            'word_frequency': {},\n",
    "            'average_word_length': 0.0\n",
    "        }\n",
    "    \n",
    "    # æ­¥é©Ÿ 1: åˆ†å‰²ä¸¦æ­£è¦åŒ–æ–‡å­—\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # æ­¥é©Ÿ 2: æ¸…ç†è©å½™ï¼ˆç§»é™¤æ¨™é»ç¬¦è™Ÿï¼‰\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        # åªä¿ç•™å­—æ¯æ•¸å­—å­—å…ƒ\n",
    "        clean_word = ''.join(c for c in word if c.isalnum())\n",
    "        if clean_word:  # æ’é™¤ç©ºè©å½™\n",
    "            clean_words.append(clean_word)\n",
    "    \n",
    "    # æ­¥é©Ÿ 3: è¨ˆç®—è©é »\n",
    "    word_frequency = {}\n",
    "    for word in clean_words:\n",
    "        # Ch12 æœ€ä½³å¯¦è¸ï¼šä½¿ç”¨ dict.get() æ–¹æ³•\n",
    "        word_frequency[word] = word_frequency.get(word, 0) + 1\n",
    "    \n",
    "    # æ­¥é©Ÿ 4: è¨ˆç®—å¹³å‡è©é•·\n",
    "    total_word_length = sum(len(word) for word in clean_words)\n",
    "    average_word_length = total_word_length / len(clean_words) if clean_words else 0.0\n",
    "    \n",
    "    return {\n",
    "        'total_words': len(clean_words),\n",
    "        'unique_words': len(word_frequency),\n",
    "        'word_frequency': word_frequency,\n",
    "        'average_word_length': round(average_word_length, 2)\n",
    "    }\n",
    "\n",
    "def count_lines(text):\n",
    "    \"\"\"\n",
    "    çµ±è¨ˆè¡Œæ•¸ç›¸é—œè³‡è¨Š\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - å­—ä¸²è™•ç†çš„ç´°ç¯€è€ƒé‡\n",
    "    - çµ±è¨ˆè¨ˆç®—çš„æº–ç¢ºæ€§\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: è¡Œæ•¸çµ±è¨ˆçµæœ\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\n",
    "            'total_lines': 0,\n",
    "            'non_empty_lines': 0,\n",
    "            'empty_lines': 0,\n",
    "            'average_line_length': 0.0\n",
    "        }\n",
    "    \n",
    "    # åˆ†å‰²ç‚ºè¡Œåˆ—è¡¨\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # çµ±è¨ˆéç©ºè¡Œèˆ‡è¨ˆç®—ç¸½é•·åº¦\n",
    "    non_empty_lines = 0\n",
    "    total_line_length = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        total_line_length += len(line)\n",
    "        if line.strip():  # éç©ºç™½è¡Œ\n",
    "            non_empty_lines += 1\n",
    "    \n",
    "    empty_lines = len(lines) - non_empty_lines\n",
    "    average_line_length = total_line_length / len(lines) if lines else 0.0\n",
    "    \n",
    "    return {\n",
    "        'total_lines': len(lines),\n",
    "        'non_empty_lines': non_empty_lines,\n",
    "        'empty_lines': empty_lines,\n",
    "        'average_line_length': round(average_line_length, 2)\n",
    "    }\n",
    "\n",
    "def analyze_text_statistics(text):\n",
    "    \"\"\"\n",
    "    ç¶œåˆæ–‡å­—çµ±è¨ˆåˆ†æï¼ˆæ•´åˆä¸Šè¿°æ‰€æœ‰å‡½å¼ï¼‰\n",
    "    \n",
    "    å±•ç¤º Ch12 æ ¸å¿ƒæ¦‚å¿µï¼šå‡½å¼çµ„åˆ\n",
    "    - å°‡è¤‡é›œåŠŸèƒ½åˆ†è§£ç‚ºå°å‡½å¼\n",
    "    - é€éçµ„åˆå¯¦ç¾æ›´é«˜å±¤æ¬¡çš„åŠŸèƒ½\n",
    "    - ç¨‹å¼ç¢¼é‡ç”¨èˆ‡æ¨¡çµ„åŒ–\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: å®Œæ•´çµ±è¨ˆå ±å‘Š\n",
    "    \"\"\"\n",
    "    # Ch12 å‡½å¼çµ„åˆï¼šå‘¼å«å·²å®šç¾©çš„å°å‡½å¼\n",
    "    return {\n",
    "        'characters': count_characters(text),\n",
    "        'words': count_words(text),\n",
    "        'lines': count_lines(text)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” æ¨¡çµ„ 2: æ–‡å­—æœå°‹æ¨¡çµ„ (Search)\n",
    "\n",
    "### ğŸ’¡ Ch14 å±•ç¤ºé‡é»ï¼šé«˜éšå‡½å¼èˆ‡ Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keyword(text, keyword, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    åœ¨æ–‡å­—ä¸­æœå°‹é—œéµå­—\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - åƒæ•¸é è¨­å€¼çš„ä½¿ç”¨\n",
    "    - å½ˆæ€§çš„å‡½å¼ä»‹é¢è¨­è¨ˆ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æœå°‹çš„æ–‡å­—\n",
    "        keyword (str): é—œéµå­—\n",
    "        case_sensitive (bool): æ˜¯å¦å€åˆ†å¤§å°å¯«\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: æœå°‹çµæœ\n",
    "    \"\"\"\n",
    "    if not text or not keyword:\n",
    "        return {\n",
    "            'keyword': keyword,\n",
    "            'count': 0,\n",
    "            'positions': [],\n",
    "            'line_numbers': [],\n",
    "            'context_lines': []\n",
    "        }\n",
    "    \n",
    "    # è™•ç†å¤§å°å¯«æ•æ„Ÿæ€§\n",
    "    search_text = text if case_sensitive else text.lower()\n",
    "    search_keyword = keyword if case_sensitive else keyword.lower()\n",
    "    \n",
    "    # æ‰¾å‡ºæ‰€æœ‰å‡ºç¾ä½ç½®\n",
    "    positions = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        pos = search_text.find(search_keyword, start)\n",
    "        if pos == -1:\n",
    "            break\n",
    "        positions.append(pos)\n",
    "        start = pos + 1\n",
    "    \n",
    "    # æ‰¾å‡ºé—œéµå­—æ‰€åœ¨çš„è¡Œè™Ÿèˆ‡è¡Œå…§å®¹\n",
    "    lines = text.splitlines()\n",
    "    line_numbers = []\n",
    "    context_lines = []\n",
    "    \n",
    "    for i, line in enumerate(lines, 1):\n",
    "        check_line = line if case_sensitive else line.lower()\n",
    "        if search_keyword in check_line:\n",
    "            line_numbers.append(i)\n",
    "            context_lines.append(line)\n",
    "    \n",
    "    return {\n",
    "        'keyword': keyword,\n",
    "        'count': len(positions),\n",
    "        'positions': positions,\n",
    "        'line_numbers': line_numbers,\n",
    "        'context_lines': context_lines\n",
    "    }\n",
    "\n",
    "def find_multiple_keywords(text, keywords, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    åŒæ™‚æœå°‹å¤šå€‹é—œéµå­—\n",
    "    \n",
    "    å±•ç¤º Ch14 é‡é»ï¼šmap å‡½å¼æ‡‰ç”¨\n",
    "    - å°‡å–®ä¸€æ“ä½œå¥—ç”¨åˆ°åˆ—è¡¨çš„æ¯å€‹å…ƒç´ \n",
    "    - å‡½å¼å¼ç¨‹å¼è¨­è¨ˆçš„æ€ç¶­\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æœå°‹çš„æ–‡å­—\n",
    "        keywords (list): é—œéµå­—åˆ—è¡¨\n",
    "        case_sensitive (bool): æ˜¯å¦å€åˆ†å¤§å°å¯«\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: {keyword: search_result} çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    if not keywords:\n",
    "        return {}\n",
    "    \n",
    "    # Ch14 é‡é»ï¼šä½¿ç”¨ map å‡½å¼\n",
    "    # å»ºç«‹æœå°‹å‡½å¼ï¼ˆéƒ¨åˆ†æ‡‰ç”¨çš„æ¦‚å¿µï¼‰\n",
    "    def search_single_keyword(keyword):\n",
    "        return find_keyword(text, keyword, case_sensitive)\n",
    "    \n",
    "    # ä½¿ç”¨ map å‡½å¼æ‰¹æ¬¡æœå°‹æ‰€æœ‰é—œéµå­—\n",
    "    search_results = list(map(search_single_keyword, keywords))\n",
    "    \n",
    "    # è½‰æ›ç‚ºå­—å…¸æ ¼å¼\n",
    "    result_dict = {}\n",
    "    for result in search_results:\n",
    "        result_dict[result['keyword']] = result\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def search_in_lines(text, pattern, filter_func=None):\n",
    "    \"\"\"\n",
    "    æŒ‰è¡Œæœå°‹ä¸¦æ”¯æ´è‡ªè¨‚éæ¿¾æ¢ä»¶\n",
    "    \n",
    "    å±•ç¤º Ch14 é‡é»ï¼šfilter å‡½å¼èˆ‡ Lambda æ‡‰ç”¨\n",
    "    - ä½¿ç”¨ filter å‡½å¼éæ¿¾çµæœ\n",
    "    - Lambda å‡½å¼ä½œç‚ºéæ¿¾æ¢ä»¶\n",
    "    - é«˜éšå‡½å¼çš„å¯¦éš›æ‡‰ç”¨\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æœå°‹çš„æ–‡å­—\n",
    "        pattern (str): æœå°‹æ¨¡å¼\n",
    "        filter_func (callable): è‡ªè¨‚éæ¿¾å‡½å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: ç¬¦åˆæ¢ä»¶çš„è¡Œåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if not text or not pattern:\n",
    "        return []\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "    matching_lines = []\n",
    "    \n",
    "    # æ‰¾å‡ºåŒ…å«æ¨¡å¼çš„è¡Œ\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        if pattern.lower() in line.lower():\n",
    "            line_info = {\n",
    "                'line_number': i,\n",
    "                'content': line,\n",
    "                'match_count': line.lower().count(pattern.lower())\n",
    "            }\n",
    "            matching_lines.append(line_info)\n",
    "    \n",
    "    # Ch14 é‡é»ï¼šä½¿ç”¨ filter å‡½å¼é€²è¡Œé€²ä¸€æ­¥éæ¿¾\n",
    "    if filter_func:\n",
    "        # ä½¿ç”¨ lambda å‡½å¼æå–è¡Œå…§å®¹é€²è¡Œéæ¿¾\n",
    "        matching_lines = list(filter(\n",
    "            lambda line_info: filter_func(line_info['content']), \n",
    "            matching_lines\n",
    "        ))\n",
    "    \n",
    "    return matching_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ¨¡çµ„ 3: æ–‡å­—è½‰æ›æ¨¡çµ„ (Transform)\n",
    "\n",
    "### ğŸ’¡ Ch13 å±•ç¤ºé‡é»ï¼šä½œç”¨åŸŸèˆ‡é–‰åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_case(text, mode='lower'):\n",
    "    \"\"\"\n",
    "    è½‰æ›æ–‡å­—å¤§å°å¯«\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - åƒæ•¸é è¨­å€¼\n",
    "    - éŒ¯èª¤è™•ç†èˆ‡é©—è­‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦è½‰æ›çš„æ–‡å­—\n",
    "        mode (str): è½‰æ›æ¨¡å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: è½‰æ›å¾Œçš„æ–‡å­—\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # ä½¿ç”¨å­—å…¸æ˜ å°„ç°¡åŒ–æ¢ä»¶åˆ¤æ–·\n",
    "    transform_methods = {\n",
    "        'lower': text.lower,\n",
    "        'upper': text.upper,\n",
    "        'title': text.title,\n",
    "        'capitalize': text.capitalize\n",
    "    }\n",
    "    \n",
    "    if mode in transform_methods:\n",
    "        return transform_methods[mode]()\n",
    "    else:\n",
    "        raise ValueError(f\"ç„¡æ•ˆçš„è½‰æ›æ¨¡å¼: {mode}ã€‚æ”¯æ´çš„æ¨¡å¼: {list(transform_methods.keys())}\")\n",
    "\n",
    "def replace_text_advanced(text, replacements, use_regex=False):\n",
    "    \"\"\"\n",
    "    é€²éšæ–‡å­—æ›¿æ›åŠŸèƒ½\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - å¯é¸åŠŸèƒ½çš„åƒæ•¸è¨­è¨ˆ\n",
    "    - æ¨¡çµ„åŒ¯å…¥çš„æ™‚æ©Ÿ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): åŸå§‹æ–‡å­—\n",
    "        replacements (dict): æ›¿æ›è¦å‰‡ {old: new}\n",
    "        use_regex (bool): æ˜¯å¦ä½¿ç”¨æ­£è¦è¡¨é”å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: æ›¿æ›å¾Œçš„æ–‡å­—\n",
    "    \"\"\"\n",
    "    if not text or not replacements:\n",
    "        return text\n",
    "    \n",
    "    result = text\n",
    "    \n",
    "    if use_regex:\n",
    "        # åªåœ¨éœ€è¦æ™‚åŒ¯å…¥æ­£è¦è¡¨é”å¼æ¨¡çµ„\n",
    "        import re\n",
    "        for pattern, replacement in replacements.items():\n",
    "            try:\n",
    "                result = re.sub(pattern, replacement, result)\n",
    "            except re.error as e:\n",
    "                print(f\"æ­£è¦è¡¨é”å¼éŒ¯èª¤ '{pattern}': {e}\")\n",
    "                continue\n",
    "    else:\n",
    "        # ç°¡å–®å­—ä¸²æ›¿æ›\n",
    "        for old, new in replacements.items():\n",
    "            result = result.replace(old, new)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_text_transformer(config):\n",
    "    \"\"\"\n",
    "    å»ºç«‹å®¢è£½åŒ–æ–‡å­—è½‰æ›å™¨\n",
    "    \n",
    "    å±•ç¤º Ch13 æ ¸å¿ƒæ¦‚å¿µï¼šé–‰åŒ… (Closure)\n",
    "    - å¤–å±¤å‡½å¼å®šç¾©é…ç½®è®Šæ•¸\n",
    "    - å…§å±¤å‡½å¼å­˜å–å¤–å±¤è®Šæ•¸\n",
    "    - å›å‚³å…§å±¤å‡½å¼å½¢æˆé–‰åŒ…\n",
    "    - é–‰åŒ…ä¿å­˜é…ç½®ç‹€æ…‹\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        config (dict): è½‰æ›è¨­å®š\n",
    "        {\n",
    "            'case_mode': str,           # å¤§å°å¯«æ¨¡å¼\n",
    "            'remove_punctuation': bool, # æ˜¯å¦ç§»é™¤æ¨™é»\n",
    "            'replace_numbers': bool,    # æ˜¯å¦æ›¿æ›æ•¸å­—\n",
    "            'custom_replacements': dict # è‡ªè¨‚æ›¿æ›è¦å‰‡\n",
    "        }\n",
    "    \n",
    "    å›å‚³:\n",
    "        function: è½‰æ›å‡½å¼ï¼ˆé–‰åŒ…ï¼‰\n",
    "    \"\"\"\n",
    "    # Ch13 é‡é»ï¼šå¤–å±¤å‡½å¼è®Šæ•¸ï¼ˆé–‰åŒ…ç’°å¢ƒï¼‰\n",
    "    # é€™äº›è®Šæ•¸æœƒè¢«å…§å±¤å‡½å¼ã€Œè¨˜ä½ã€\n",
    "    case_mode = config.get('case_mode', 'none')\n",
    "    remove_punctuation = config.get('remove_punctuation', False)\n",
    "    replace_numbers = config.get('replace_numbers', False)\n",
    "    custom_replacements = config.get('custom_replacements', {})\n",
    "    \n",
    "    def transformer(text):\n",
    "        \"\"\"\n",
    "        å…§å±¤å‡½å¼ - æ–‡å­—è½‰æ›å™¨\n",
    "        \n",
    "        å±•ç¤º Ch13 é–‰åŒ…çš„ç‰¹æ€§ï¼š\n",
    "        - å¯ä»¥å­˜å–å¤–å±¤å‡½å¼çš„è®Šæ•¸\n",
    "        - å¤–å±¤è®Šæ•¸åœ¨å‡½å¼å›å‚³å¾Œä»ç„¶ä¿æŒ\n",
    "        - æ¯æ¬¡å‘¼å« create_text_transformer éƒ½æœƒç”¢ç”Ÿç¨ç«‹çš„é–‰åŒ…\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        result = text\n",
    "        \n",
    "        # ä½¿ç”¨å¤–å±¤è®Šæ•¸ case_mode\n",
    "        if case_mode != 'none':\n",
    "            try:\n",
    "                result = transform_case(result, case_mode)\n",
    "            except ValueError:\n",
    "                pass  # å¿½ç•¥ç„¡æ•ˆçš„å¤§å°å¯«æ¨¡å¼\n",
    "        \n",
    "        # ä½¿ç”¨å¤–å±¤è®Šæ•¸ remove_punctuation\n",
    "        if remove_punctuation:\n",
    "            # åªä¿ç•™å­—æ¯ã€æ•¸å­—å’Œç©ºæ ¼\n",
    "            result = ''.join(c for c in result if c.isalnum() or c.isspace())\n",
    "        \n",
    "        # ä½¿ç”¨å¤–å±¤è®Šæ•¸ replace_numbers\n",
    "        if replace_numbers:\n",
    "            result = ''.join(c if not c.isdigit() else '#' for c in result)\n",
    "        \n",
    "        # ä½¿ç”¨å¤–å±¤è®Šæ•¸ custom_replacements\n",
    "        if custom_replacements:\n",
    "            result = replace_text_advanced(result, custom_replacements)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Ch13 é‡é»ï¼šå›å‚³å…§å±¤å‡½å¼ï¼Œå½¢æˆé–‰åŒ…\n",
    "    return transformer\n",
    "\n",
    "def batch_transform_texts(text_list, transform_func):\n",
    "    \"\"\"\n",
    "    æ‰¹æ¬¡è™•ç†æ–‡å­—åˆ—è¡¨\n",
    "    \n",
    "    å±•ç¤º Ch14 é‡é»ï¼šmap å‡½å¼æ‡‰ç”¨\n",
    "    - å°‡è½‰æ›å‡½å¼å¥—ç”¨åˆ°æ¯å€‹æ–‡å­—\n",
    "    - å‡½å¼ä½œç‚ºåƒæ•¸å‚³é\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text_list (list): æ–‡å­—åˆ—è¡¨\n",
    "        transform_func (callable): è½‰æ›å‡½å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: è½‰æ›å¾Œçš„æ–‡å­—åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if not text_list or not transform_func:\n",
    "        return text_list or []\n",
    "    \n",
    "    # Ch14 é‡é»ï¼šä½¿ç”¨ map å‡½å¼é€²è¡Œæ‰¹æ¬¡è™•ç†\n",
    "    return list(map(transform_func, text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ æ¨¡çµ„ 4: æ–‡å­—æ ¼å¼åŒ–æ¨¡çµ„ (Format)\n",
    "\n",
    "### ğŸ’¡ Ch12 å±•ç¤ºé‡é»ï¼šå‡½å¼è¨­è¨ˆèˆ‡åƒæ•¸è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_text(text, width=80, alignment='left'):\n",
    "    \"\"\"\n",
    "    æ–‡å­—å°é½Šè™•ç†\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - åˆç†çš„é è¨­åƒæ•¸å€¼\n",
    "    - å¤šè¡Œæ–‡å­—çš„è™•ç†ç­–ç•¥\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦å°é½Šçš„æ–‡å­—\n",
    "        width (int): å°é½Šå¯¬åº¦\n",
    "        alignment (str): å°é½Šæ–¹å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: å°é½Šå¾Œçš„æ–‡å­—\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ' ' * width\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "    aligned_lines = []\n",
    "    \n",
    "    # å°é½Šæ–¹æ³•æ˜ å°„\n",
    "    align_methods = {\n",
    "        'left': lambda line: line.ljust(width),\n",
    "        'right': lambda line: line.rjust(width),\n",
    "        'center': lambda line: line.center(width)\n",
    "    }\n",
    "    \n",
    "    if alignment not in align_methods:\n",
    "        raise ValueError(f\"ç„¡æ•ˆçš„å°é½Šæ–¹å¼: {alignment}ã€‚æ”¯æ´: {list(align_methods.keys())}\")\n",
    "    \n",
    "    align_func = align_methods[alignment]\n",
    "    \n",
    "    for line in lines:\n",
    "        # è™•ç†éé•·çš„è¡Œ\n",
    "        if len(line) > width:\n",
    "            aligned_lines.append(line[:width])  # æˆªæ–·\n",
    "        else:\n",
    "            aligned_lines.append(align_func(line))\n",
    "    \n",
    "    return '\\n'.join(aligned_lines)\n",
    "\n",
    "def indent_lines(text, indent_size=4, indent_char=' '):\n",
    "    \"\"\"\n",
    "    ç‚ºæ–‡å­—è¡Œæ·»åŠ ç¸®æ’\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - å½ˆæ€§çš„åƒæ•¸è¨­è¨ˆ\n",
    "    - å­—ä¸²æ“ä½œçš„æœ€ä½³å¯¦è¸\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦ç¸®æ’çš„æ–‡å­—\n",
    "        indent_size (int): ç¸®æ’å¤§å°\n",
    "        indent_char (str): ç¸®æ’å­—å…ƒ\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: ç¸®æ’å¾Œçš„æ–‡å­—\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # å»ºç«‹ç¸®æ’å­—ä¸²\n",
    "    indent = indent_char * indent_size\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "    indented_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # åªå°éç©ºè¡Œé€²è¡Œç¸®æ’\n",
    "        if line.strip():\n",
    "            indented_lines.append(indent + line)\n",
    "        else:\n",
    "            indented_lines.append(line)  # ä¿æŒç©ºè¡Œ\n",
    "    \n",
    "    return '\\n'.join(indented_lines)\n",
    "\n",
    "def create_columns(text_list, columns=2, separator='  |  '):\n",
    "    \"\"\"\n",
    "    å°‡æ–‡å­—åˆ—è¡¨æ ¼å¼åŒ–ç‚ºå¤šæ¬„é¡¯ç¤º\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - è¤‡é›œä½ˆå±€é‚è¼¯çš„åˆ†è§£\n",
    "    - åˆ—è¡¨åˆ†ç‰‡æ“ä½œ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text_list (list): æ–‡å­—åˆ—è¡¨\n",
    "        columns (int): æ¬„æ•¸\n",
    "        separator (str): æ¬„åˆ†éš”ç¬¦\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: æ ¼å¼åŒ–å¾Œçš„å¤šæ¬„æ–‡å­—\n",
    "    \"\"\"\n",
    "    if not text_list or columns <= 0:\n",
    "        return ''\n",
    "    \n",
    "    # è¨ˆç®—æ¯æ¬„çš„æœ€å¤§å¯¬åº¦\n",
    "    max_width = max(len(str(item)) for item in text_list)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    # åˆ†çµ„è™•ç†ï¼Œæ¯çµ„åŒ…å« columns å€‹å…ƒç´ \n",
    "    for i in range(0, len(text_list), columns):\n",
    "        row_items = text_list[i:i + columns]\n",
    "        \n",
    "        # æ ¼å¼åŒ–æ¯å€‹å…ƒç´ ï¼ˆå·¦å°é½Šï¼‰\n",
    "        formatted_items = [str(item).ljust(max_width) for item in row_items]\n",
    "        \n",
    "        # é€£æ¥ç‚ºä¸€è¡Œ\n",
    "        row_text = separator.join(formatted_items)\n",
    "        rows.append(row_text)\n",
    "    \n",
    "    return '\\n'.join(rows)\n",
    "\n",
    "def format_table(data, headers=None, align='left'):\n",
    "    \"\"\"\n",
    "    æ ¼å¼åŒ–è³‡æ–™ç‚ºè¡¨æ ¼\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - è¤‡é›œè¡¨æ ¼ä½ˆå±€çš„è™•ç†\n",
    "    - å‹•æ…‹å¯¬åº¦è¨ˆç®—\n",
    "    - å¯é¸åƒæ•¸çš„è™•ç†\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        data (list): è³‡æ–™åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ ç‚ºä¸€è¡Œè³‡æ–™\n",
    "        headers (list): è¡¨é ­åˆ—è¡¨\n",
    "        align (str): å°é½Šæ–¹å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: æ ¼å¼åŒ–çš„è¡¨æ ¼å­—ä¸²\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return ''\n",
    "    \n",
    "    # æº–å‚™å®Œæ•´è³‡æ–™ï¼ˆåŒ…å«è¡¨é ­ï¼‰\n",
    "    all_rows = []\n",
    "    if headers:\n",
    "        all_rows.append(headers)\n",
    "    all_rows.extend(data)\n",
    "    \n",
    "    # è¨ˆç®—æ¯æ¬„çš„æœ€å¤§å¯¬åº¦\n",
    "    if not all_rows:\n",
    "        return ''\n",
    "    \n",
    "    num_cols = len(all_rows[0])\n",
    "    col_widths = [0] * num_cols\n",
    "    \n",
    "    # å‹•æ…‹è¨ˆç®—æ¬„å¯¬\n",
    "    for row in all_rows:\n",
    "        for i, cell in enumerate(row):\n",
    "            if i < len(col_widths):\n",
    "                col_widths[i] = max(col_widths[i], len(str(cell)))\n",
    "    \n",
    "    # å°é½Šæ–¹æ³•æ˜ å°„\n",
    "    align_methods = {\n",
    "        'left': lambda cell, width: str(cell).ljust(width),\n",
    "        'right': lambda cell, width: str(cell).rjust(width),\n",
    "        'center': lambda cell, width: str(cell).center(width)\n",
    "    }\n",
    "    \n",
    "    if align not in align_methods:\n",
    "        align = 'left'  # é è¨­ä½¿ç”¨å·¦å°é½Š\n",
    "    \n",
    "    align_func = align_methods[align]\n",
    "    \n",
    "    # æ ¼å¼åŒ–æ¯ä¸€è¡Œ\n",
    "    formatted_rows = []\n",
    "    \n",
    "    for i, row in enumerate(all_rows):\n",
    "        formatted_cells = []\n",
    "        for j, cell in enumerate(row):\n",
    "            if j < len(col_widths):\n",
    "                formatted_cell = align_func(cell, col_widths[j])\n",
    "                formatted_cells.append(formatted_cell)\n",
    "        \n",
    "        row_text = ' | '.join(formatted_cells)\n",
    "        formatted_rows.append(row_text)\n",
    "        \n",
    "        # åœ¨è¡¨é ­å¾Œæ·»åŠ åˆ†éš”ç·š\n",
    "        if i == 0 and headers:\n",
    "            separator_line = '-' * len(row_text)\n",
    "            formatted_rows.append(separator_line)\n",
    "    \n",
    "    return '\\n'.join(formatted_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ æ¨¡çµ„ 5: æ–‡å­—åˆ†ææ¨¡çµ„ (Analysis)\n",
    "\n",
    "### ğŸ’¡ Ch12 å±•ç¤ºé‡é»ï¼šè¤‡é›œé‚è¼¯çš„å‡½å¼åˆ†è§£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentences(text):\n",
    "    \"\"\"\n",
    "    å¥å­åˆ†æ\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - æ­£è¦è¡¨é”å¼çš„æ‡‰ç”¨\n",
    "    - è³‡æ–™æ¸…ç†çš„é‡è¦æ€§\n",
    "    - çµ±è¨ˆè¨ˆç®—çš„æº–ç¢ºæ€§\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: å¥å­åˆ†æçµæœ\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return {\n",
    "            'sentence_count': 0,\n",
    "            'sentences': [],\n",
    "            'average_sentence_length': 0.0,\n",
    "            'longest_sentence': '',\n",
    "            'shortest_sentence': ''\n",
    "        }\n",
    "    \n",
    "    # ä½¿ç”¨æ­£è¦è¡¨é”å¼åˆ†å‰²å¥å­\n",
    "    import re\n",
    "    # ä»¥å¥è™Ÿã€é©šå˜†è™Ÿã€å•è™Ÿç‚ºåˆ†éš”ç¬¦\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    # æ¸…ç†å¥å­ï¼ˆç§»é™¤ç©ºç™½å¥å­ï¼‰\n",
    "    clean_sentences = []\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = sentence.strip()\n",
    "        if clean_sentence:  # æ’é™¤ç©ºå¥å­\n",
    "            clean_sentences.append(clean_sentence)\n",
    "    \n",
    "    if not clean_sentences:\n",
    "        return {\n",
    "            'sentence_count': 0,\n",
    "            'sentences': [],\n",
    "            'average_sentence_length': 0.0,\n",
    "            'longest_sentence': '',\n",
    "            'shortest_sentence': ''\n",
    "        }\n",
    "    \n",
    "    # è¨ˆç®—å¥å­é•·åº¦çµ±è¨ˆ\n",
    "    sentence_lengths = [len(sentence) for sentence in clean_sentences]\n",
    "    average_length = sum(sentence_lengths) / len(sentence_lengths)\n",
    "    \n",
    "    # æ‰¾å‡ºæœ€é•·å’Œæœ€çŸ­å¥å­\n",
    "    longest_sentence = max(clean_sentences, key=len)\n",
    "    shortest_sentence = min(clean_sentences, key=len)\n",
    "    \n",
    "    return {\n",
    "        'sentence_count': len(clean_sentences),\n",
    "        'sentences': clean_sentences,\n",
    "        'average_sentence_length': round(average_length, 2),\n",
    "        'longest_sentence': longest_sentence,\n",
    "        'shortest_sentence': shortest_sentence\n",
    "    }\n",
    "\n",
    "def detect_duplicates(text, min_length=5):\n",
    "    \"\"\"\n",
    "    æª¢æ¸¬é‡è¤‡ç‰‡æ®µ\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - æ¼”ç®—æ³•çš„å¯¦ä½œç­–ç•¥\n",
    "    - æ•ˆèƒ½è€ƒé‡èˆ‡å„ªåŒ–\n",
    "    - è³‡æ–™çµæ§‹çš„é¸æ“‡\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æª¢æ¸¬çš„æ–‡å­—\n",
    "        min_length (int): æœ€å°é‡è¤‡é•·åº¦\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: é‡è¤‡ç‰‡æ®µåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if not text or min_length <= 0:\n",
    "        return []\n",
    "    \n",
    "    # ä½¿ç”¨å­—å…¸è¨˜éŒ„ç‰‡æ®µè³‡è¨Š\n",
    "    fragments = {}\n",
    "    text_length = len(text)\n",
    "    \n",
    "    # ç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„å­å­—ä¸²\n",
    "    for i in range(text_length):\n",
    "        for j in range(i + min_length, min(text_length + 1, i + 50)):  # é™åˆ¶æœ€å¤§é•·åº¦é¿å…éå¤šç„¡æ„ç¾©ç‰‡æ®µ\n",
    "            fragment = text[i:j]\n",
    "            \n",
    "            # è·³éåªæœ‰ç©ºç™½çš„ç‰‡æ®µ\n",
    "            if not fragment.strip():\n",
    "                continue\n",
    "            \n",
    "            # çµ±è¨ˆç‰‡æ®µå‡ºç¾æ¬¡æ•¸\n",
    "            if fragment in fragments:\n",
    "                fragments[fragment]['count'] += 1\n",
    "                fragments[fragment]['positions'].append(i)\n",
    "            else:\n",
    "                fragments[fragment] = {\n",
    "                    'count': 1,\n",
    "                    'positions': [i]\n",
    "                }\n",
    "    \n",
    "    # éæ¿¾å‡ºçœŸæ­£çš„é‡è¤‡ç‰‡æ®µ\n",
    "    duplicates = []\n",
    "    for fragment, info in fragments.items():\n",
    "        if info['count'] > 1:\n",
    "            duplicates.append({\n",
    "                'fragment': fragment,\n",
    "                'count': info['count'],\n",
    "                'positions': info['positions']\n",
    "            })\n",
    "    \n",
    "    # æŒ‰é‡è¤‡æ¬¡æ•¸æ’åºï¼Œæ¬¡æ•¸ç›¸åŒå‰‡æŒ‰é•·åº¦æ’åº\n",
    "    duplicates.sort(key=lambda x: (x['count'], len(x['fragment'])), reverse=True)\n",
    "    \n",
    "    return duplicates[:20]  # é™åˆ¶å›å‚³æ•¸é‡ï¼Œé¿å…éå¤šçµæœ\n",
    "\n",
    "def calculate_readability_score(text):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ–‡å­—å¯è®€æ€§åˆ†æ•¸ï¼ˆç°¡åŒ–ç‰ˆï¼‰\n",
    "    \n",
    "    å±•ç¤º Ch12 æ¦‚å¿µï¼š\n",
    "    - å‡½å¼é–“çš„ç›¸ä¾æ€§ç®¡ç†\n",
    "    - è¤‡é›œè¨ˆç®—çš„åˆ†è§£\n",
    "    - æ¼”ç®—æ³•è¨­è¨ˆæ€ç¶­\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: å¯è®€æ€§åˆ†æçµæœ\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return {\n",
    "            'avg_sentence_length': 0.0,\n",
    "            'avg_word_length': 0.0,\n",
    "            'complexity_score': 0.0,\n",
    "            'difficulty_level': 'Easy'\n",
    "        }\n",
    "    \n",
    "    # ä½¿ç”¨å·²æœ‰çš„åˆ†æå‡½å¼ï¼ˆå‡½å¼é‡ç”¨ï¼‰\n",
    "    sentence_analysis = analyze_sentences(text)\n",
    "    word_analysis = count_words(text)\n",
    "    \n",
    "    # æå–é—œéµæŒ‡æ¨™\n",
    "    avg_sentence_length = sentence_analysis['average_sentence_length']\n",
    "    avg_word_length = word_analysis['average_word_length']\n",
    "    \n",
    "    # è¨ˆç®—è¤‡é›œåº¦åˆ†æ•¸ï¼ˆç°¡åŒ–çš„ Flesch å…¬å¼è®Šé«”ï¼‰\n",
    "    # è¤‡é›œåº¦è€ƒé‡ï¼šå¥é•·ã€è©é•·ã€è©å½™è±å¯Œåº¦\n",
    "    vocab_richness = (word_analysis['unique_words'] / \n",
    "                     max(word_analysis['total_words'], 1)) * 100\n",
    "    \n",
    "    complexity_score = (\n",
    "        avg_sentence_length * 0.3 +    # å¥å­é•·åº¦å½±éŸ¿\n",
    "        avg_word_length * 0.4 +        # è©å½™é•·åº¦å½±éŸ¿\n",
    "        vocab_richness * 0.3           # è©å½™è±å¯Œåº¦å½±éŸ¿\n",
    "    )\n",
    "    \n",
    "    # åˆ¤å®šé›£åº¦ç­‰ç´š\n",
    "    if complexity_score < 20:\n",
    "        difficulty_level = 'Easy'\n",
    "    elif complexity_score < 40:\n",
    "        difficulty_level = 'Medium'\n",
    "    else:\n",
    "        difficulty_level = 'Hard'\n",
    "    \n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'complexity_score': round(complexity_score, 2),\n",
    "        'difficulty_level': difficulty_level,\n",
    "        'vocab_richness': round(vocab_richness, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ¨¡çµ„ 6: éè¿´æª”æ¡ˆè™•ç† (Recursive) [é€²éš]\n",
    "\n",
    "### ğŸ’¡ Ch15 å±•ç¤ºé‡é»ï¼šéè¿´æ€ç¶­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_files_recursive(directory, pattern, file_extension='.txt', max_depth=5):\n",
    "    \"\"\"\n",
    "    éè¿´æœå°‹ç›®éŒ„ä¸­çš„æ–‡å­—æª”æ¡ˆ\n",
    "    \n",
    "    å±•ç¤º Ch15 æ ¸å¿ƒæ¦‚å¿µï¼šéè¿´æ€ç¶­çš„å®Œæ•´æ‡‰ç”¨\n",
    "    - åŸºæœ¬æƒ…æ³ (Base Case)\n",
    "    - éè¿´æƒ…æ³ (Recursive Case)\n",
    "    - ç‹€æ…‹æ”¹è®Š (State Change)\n",
    "    - é˜²è­·æ©Ÿåˆ¶ (Safety Mechanisms)\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        directory (str): æœå°‹ç›®éŒ„è·¯å¾‘\n",
    "        pattern (str): æœå°‹æ¨¡å¼\n",
    "        file_extension (str): æª”æ¡ˆå‰¯æª”å\n",
    "        max_depth (int): æœ€å¤§æœå°‹æ·±åº¦\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: æœå°‹çµæœåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    def _recursive_search(current_dir, current_depth):\n",
    "        \"\"\"\n",
    "        å…§éƒ¨éè¿´å‡½å¼\n",
    "        \n",
    "        å±•ç¤º Ch15 éè¿´çš„ä¸‰è¦ç´ :\n",
    "        1. åŸºæœ¬æƒ…æ³ï¼šé”åˆ°æœ€å¤§æ·±åº¦\n",
    "        2. éè¿´æƒ…æ³ï¼šå°å­ç›®éŒ„é€²è¡Œéè¿´æœå°‹\n",
    "        3. ç‹€æ…‹æ”¹è®Šï¼šæ·±åº¦éå¢\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Ch15 é‡é»ï¼šåŸºæœ¬æƒ…æ³ (Base Case)\n",
    "        # é˜²æ­¢ç„¡é™éè¿´çš„é—œéµæ©Ÿåˆ¶\n",
    "        if current_depth >= max_depth:\n",
    "            return results\n",
    "        \n",
    "        # éŒ¯èª¤è™•ç†ï¼šè™•ç†æ¬Šé™å•é¡Œ\n",
    "        try:\n",
    "            items = os.listdir(current_dir)\n",
    "        except (PermissionError, FileNotFoundError, OSError):\n",
    "            return results\n",
    "        \n",
    "        for item in items:\n",
    "            item_path = os.path.join(current_dir, item)\n",
    "            \n",
    "            # è™•ç†æª”æ¡ˆ\n",
    "            if os.path.isfile(item_path) and item.endswith(file_extension):\n",
    "                try:\n",
    "                    with open(item_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        \n",
    "                    if pattern in content:\n",
    "                        file_size = os.path.getsize(item_path)\n",
    "                        results.append({\n",
    "                            'file_path': item_path,\n",
    "                            'matches': content.count(pattern),\n",
    "                            'depth': current_depth,\n",
    "                            'file_size': file_size\n",
    "                        })\n",
    "                except (UnicodeDecodeError, PermissionError, OSError):\n",
    "                    continue  # è·³éç„¡æ³•è®€å–çš„æª”æ¡ˆ\n",
    "            \n",
    "            # Ch15 é‡é»ï¼šéè¿´æƒ…æ³ (Recursive Case)\n",
    "            elif os.path.isdir(item_path):\n",
    "                # ç‹€æ…‹æ”¹è®Šï¼šæ·±åº¦éå¢\n",
    "                sub_results = _recursive_search(item_path, current_depth + 1)\n",
    "                results.extend(sub_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.isdir(directory):\n",
    "        return []\n",
    "    \n",
    "    # é–‹å§‹éè¿´æœå°‹\n",
    "    return _recursive_search(directory, 0)\n",
    "\n",
    "def count_pattern_recursive(text, pattern):\n",
    "    \"\"\"\n",
    "    éè¿´è¨ˆç®—æ¨¡å¼å‡ºç¾æ¬¡æ•¸\n",
    "    \n",
    "    å±•ç¤º Ch15 éè¿´æ€ç¶­çš„æ•™å­¸ç¯„ä¾‹\n",
    "    æ³¨æ„ï¼šé€™æ˜¯ç‚ºäº†å±•ç¤ºéè¿´æ¦‚å¿µï¼Œå¯¦éš›æ‡‰ç”¨ä¸­ text.count() æ›´é«˜æ•ˆ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æœå°‹çš„æ–‡å­—\n",
    "        pattern (str): æœå°‹æ¨¡å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        int: æ¨¡å¼å‡ºç¾æ¬¡æ•¸\n",
    "    \"\"\"\n",
    "    # Ch15 é‡é»ï¼šåŸºæœ¬æƒ…æ³ (Base Cases)\n",
    "    \n",
    "    # åŸºæœ¬æƒ…æ³ 1: æ–‡å­—æˆ–æ¨¡å¼ç‚ºç©º\n",
    "    if not text or not pattern:\n",
    "        return 0\n",
    "    \n",
    "    # åŸºæœ¬æƒ…æ³ 2: æ–‡å­—é•·åº¦å°æ–¼æ¨¡å¼é•·åº¦\n",
    "    if len(text) < len(pattern):\n",
    "        return 0\n",
    "    \n",
    "    # Ch15 é‡é»ï¼šéè¿´æƒ…æ³ (Recursive Cases)\n",
    "    \n",
    "    if text.startswith(pattern):\n",
    "        # éè¿´æƒ…æ³ 1: æ‰¾åˆ°åŒ¹é…ï¼Œè¨ˆæ•¸ä¸¦ç¹¼çºŒæœå°‹\n",
    "        # ç‹€æ…‹æ”¹è®Šï¼šç§»é™¤å·²åŒ¹é…çš„éƒ¨åˆ†\n",
    "        return 1 + count_pattern_recursive(text[len(pattern):], pattern)\n",
    "    else:\n",
    "        # éè¿´æƒ…æ³ 2: æ²’æ‰¾åˆ°åŒ¹é…ï¼Œç§»å‹•ä¸€å€‹å­—å…ƒç¹¼çºŒæœå°‹\n",
    "        # ç‹€æ…‹æ”¹è®Šï¼šç§»é™¤ç¬¬ä¸€å€‹å­—å…ƒ\n",
    "        return count_pattern_recursive(text[1:], pattern)\n",
    "\n",
    "def parse_nested_structure(text, open_tag='[', close_tag=']'):\n",
    "    \"\"\"\n",
    "    éè¿´è§£æå·¢ç‹€çµæ§‹ï¼ˆå¦‚æ‹¬è™ŸåŒ¹é…ï¼‰\n",
    "    \n",
    "    å±•ç¤º Ch15 éè¿´åœ¨è¤‡é›œè³‡æ–™çµæ§‹è™•ç†ä¸­çš„æ‡‰ç”¨\n",
    "    - è™•ç†ä»»æ„æ·±åº¦çš„å·¢ç‹€çµæ§‹\n",
    "    - ç‹€æ…‹è¿½è¹¤èˆ‡å›å‚³\n",
    "    - è¤‡é›œçš„éè¿´é‚è¼¯\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦è§£æçš„æ–‡å­—\n",
    "        open_tag (str): é–‹å§‹æ¨™è¨˜\n",
    "        close_tag (str): çµæŸæ¨™è¨˜\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: è§£æçµæœ\n",
    "    \"\"\"\n",
    "    def _parse_recursive(text, pos, depth):\n",
    "        \"\"\"\n",
    "        å…§éƒ¨éè¿´è§£æå‡½å¼\n",
    "        \n",
    "        å±•ç¤ºè¤‡é›œéè¿´çš„å¯¦ä½œæŠ€å·§:\n",
    "        - å¤šé‡ç‹€æ…‹ç®¡ç†\n",
    "        - ä½ç½®è¿½è¹¤\n",
    "        - æ·±åº¦è¨˜éŒ„\n",
    "        \"\"\"\n",
    "        structure = []\n",
    "        current_text = ''\n",
    "        max_depth_seen = depth\n",
    "        \n",
    "        i = pos\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "            \n",
    "            if char == open_tag:\n",
    "                # é‡åˆ°é–‹å§‹æ¨™è¨˜ï¼šä¿å­˜ç•¶å‰æ–‡å­—ï¼Œé€²å…¥éè¿´\n",
    "                if current_text:\n",
    "                    structure.append(current_text)\n",
    "                    current_text = ''\n",
    "                \n",
    "                # Ch15 é‡é»ï¼šéè¿´è™•ç†å…§éƒ¨çµæ§‹\n",
    "                inner_structure, next_pos, inner_max_depth = _parse_recursive(\n",
    "                    text, i + 1, depth + 1\n",
    "                )\n",
    "                structure.append(inner_structure)\n",
    "                max_depth_seen = max(max_depth_seen, inner_max_depth)\n",
    "                i = next_pos\n",
    "                \n",
    "            elif char == close_tag:\n",
    "                # é‡åˆ°çµæŸæ¨™è¨˜ï¼šå®Œæˆç•¶å‰å±¤ç´š\n",
    "                if current_text:\n",
    "                    structure.append(current_text)\n",
    "                return structure, i + 1, max_depth_seen\n",
    "            \n",
    "            else:\n",
    "                # æ™®é€šå­—å…ƒï¼šç´¯ç©åˆ°ç•¶å‰æ–‡å­—\n",
    "                current_text += char\n",
    "                i += 1\n",
    "        \n",
    "        # è™•ç†æ–‡å­—çµå°¾\n",
    "        if current_text:\n",
    "            structure.append(current_text)\n",
    "        \n",
    "        return structure, len(text), max_depth_seen\n",
    "    \n",
    "    # æª¢æŸ¥çµæ§‹æ˜¯å¦å¹³è¡¡\n",
    "    open_count = text.count(open_tag)\n",
    "    close_count = text.count(close_tag)\n",
    "    is_balanced = (open_count == close_count)\n",
    "    \n",
    "    # é–‹å§‹éè¿´è§£æ\n",
    "    structure, _, max_depth = _parse_recursive(text, 0, 0)\n",
    "    \n",
    "    return {\n",
    "        'is_balanced': is_balanced,\n",
    "        'max_depth': max_depth,\n",
    "        'structure': structure,\n",
    "        'open_count': open_count,\n",
    "        'close_count': close_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§© æ¨¡çµ„ 7: å‡½å¼å¼ç¨‹å¼è¨­è¨ˆ (Functional) [é€²éš]\n",
    "\n",
    "### ğŸ’¡ Ch14 å±•ç¤ºé‡é»ï¼šé«˜éšå‡½å¼é€²éšæ‡‰ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_pipeline(*operations):\n",
    "    \"\"\"\n",
    "    å»ºç«‹æ–‡å­—è™•ç†æµæ°´ç·š\n",
    "    \n",
    "    å±•ç¤º Ch14 æ ¸å¿ƒæ¦‚å¿µï¼šå‡½å¼çµ„åˆ (Function Composition)\n",
    "    - å°‡å¤šå€‹å‡½å¼çµ„åˆæˆä¸€å€‹æ–°å‡½å¼\n",
    "    - å‡½å¼å¼ç¨‹å¼è¨­è¨ˆçš„æ ¸å¿ƒæ€ç¶­\n",
    "    - å¯è®Šåƒæ•¸çš„æ‡‰ç”¨\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        *operations: å¯è®Šæ•¸é‡çš„è™•ç†å‡½å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        function: çµ„åˆå¾Œçš„è™•ç†å‡½å¼\n",
    "    \n",
    "    ç¯„ä¾‹:\n",
    "        >>> pipeline = create_text_pipeline(\n",
    "        ...     lambda x: x.lower(),\n",
    "        ...     lambda x: x.replace(' ', '_'),\n",
    "        ...     lambda x: x.strip()\n",
    "        ... )\n",
    "        >>> pipeline(\"  Hello World  \")\n",
    "        \"hello_world\"\n",
    "    \"\"\"\n",
    "    def pipeline(text):\n",
    "        \"\"\"\n",
    "        æµæ°´ç·šè™•ç†å‡½å¼\n",
    "        \n",
    "        ä¾åºå¥—ç”¨æ‰€æœ‰æ“ä½œï¼Œæ¯å€‹æ“ä½œçš„è¼¸å‡º\n",
    "        æˆç‚ºä¸‹ä¸€å€‹æ“ä½œçš„è¼¸å…¥\n",
    "        \"\"\"\n",
    "        result = text\n",
    "        \n",
    "        # Ch14 é‡é»ï¼šä¾åºå¥—ç”¨æ‰€æœ‰å‡½å¼\n",
    "        for operation in operations:\n",
    "            if callable(operation):  # ç¢ºä¿æ˜¯å¯å‘¼å«çš„å‡½å¼\n",
    "                result = operation(result)\n",
    "            else:\n",
    "                raise TypeError(f\"æ“ä½œå¿…é ˆæ˜¯å¯å‘¼å«çš„å‡½å¼ï¼Œå¾—åˆ°: {type(operation)}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def apply_text_filters(text_list, *filters):\n",
    "    \"\"\"\n",
    "    ä¾åºå¥—ç”¨å¤šå€‹éæ¿¾å™¨\n",
    "    \n",
    "    å±•ç¤º Ch14 é‡é»ï¼šfilter å‡½å¼çš„é€²éšæ‡‰ç”¨\n",
    "    - éæ¿¾å™¨é€£é– (Filter Chaining)\n",
    "    - å‡½å¼å¼ç¨‹å¼è¨­è¨ˆçš„çµ„åˆæ€§\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text_list (list): æ–‡å­—åˆ—è¡¨\n",
    "        *filters: å¯è®Šæ•¸é‡çš„éæ¿¾å‡½å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: éæ¿¾å¾Œçš„æ–‡å­—åˆ—è¡¨\n",
    "    \n",
    "    ç¯„ä¾‹:\n",
    "        >>> apply_text_filters(\n",
    "        ...     [\"hello\", \"\", \"world\", \"python\", \"hi\"],\n",
    "        ...     lambda x: x != \"\",           # ç§»é™¤ç©ºå­—ä¸²\n",
    "        ...     lambda x: len(x) > 4         # ä¿ç•™é•·åº¦ > 4 çš„è©\n",
    "        ... )\n",
    "        [\"hello\", \"world\", \"python\"]\n",
    "    \"\"\"\n",
    "    result = text_list\n",
    "    \n",
    "    # Ch14 é‡é»ï¼šä¾åºå¥—ç”¨æ‰€æœ‰éæ¿¾å™¨\n",
    "    for filter_func in filters:\n",
    "        if callable(filter_func):\n",
    "            result = list(filter(filter_func, result))\n",
    "        else:\n",
    "            raise TypeError(f\"éæ¿¾å™¨å¿…é ˆæ˜¯å¯å‘¼å«çš„å‡½å¼ï¼Œå¾—åˆ°: {type(filter_func)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def reduce_text_analysis(text_list, analysis_func):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ reduce é€²è¡Œæ–‡å­—åˆ†æèšåˆ\n",
    "    \n",
    "    å±•ç¤º Ch14 é‡é»ï¼šreduce å‡½å¼çš„å¯¦éš›æ‡‰ç”¨\n",
    "    - ç´¯ç©å¼è¨ˆç®—\n",
    "    - è¤‡é›œè³‡æ–™èšåˆ\n",
    "    - å‡½å¼å¼ç¨‹å¼è¨­è¨ˆçš„å¨åŠ›\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text_list (list): æ–‡å­—åˆ—è¡¨\n",
    "        analysis_func (callable): åˆ†æå‡½å¼ (acc, text) -> new_acc\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: èšåˆåˆ†æçµæœ\n",
    "    \n",
    "    ç¯„ä¾‹:\n",
    "        >>> def word_counter(acc, text):\n",
    "        ...     words = len(text.split())\n",
    "        ...     return {\n",
    "        ...         'total_texts': acc.get('total_texts', 0) + 1,\n",
    "        ...         'total_words': acc.get('total_words', 0) + words\n",
    "        ...     }\n",
    "        >>> reduce_text_analysis([\"hello world\", \"python\"], word_counter)\n",
    "        {'total_texts': 2, 'total_words': 3}\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    \n",
    "    if not text_list:\n",
    "        return {}\n",
    "    \n",
    "    if not callable(analysis_func):\n",
    "        raise TypeError(\"åˆ†æå‡½å¼å¿…é ˆæ˜¯å¯å‘¼å«çš„\")\n",
    "    \n",
    "    def combine_analysis(acc, text):\n",
    "        \"\"\"\n",
    "        çµ„åˆåˆ†æçµæœçš„åŒ…è£å‡½å¼\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return analysis_func(acc, text)\n",
    "        except Exception as e:\n",
    "            print(f\"åˆ†æå‡½å¼åŸ·è¡ŒéŒ¯èª¤: {e}\")\n",
    "            return acc  # ç™¼ç”ŸéŒ¯èª¤æ™‚å›å‚³åŸç´¯ç©å€¼\n",
    "    \n",
    "    # Ch14 é‡é»ï¼šä½¿ç”¨ reduce é€²è¡Œç´¯ç©è¨ˆç®—\n",
    "    initial_value = {}\n",
    "    result = reduce(combine_analysis, text_list, initial_value)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_custom_filters():\n",
    "    \"\"\"\n",
    "    å»ºç«‹å¸¸ç”¨çš„ Lambda éæ¿¾å™¨é›†åˆ\n",
    "    \n",
    "    å±•ç¤º Ch14 é‡é»ï¼šé«˜éšå‡½å¼å·¥å»  (Higher-Order Function Factory)\n",
    "    - å‡½å¼è¿”å›å‡½å¼çš„æ¦‚å¿µ\n",
    "    - Lambda å‡½å¼çš„å¯¦éš›æ‡‰ç”¨\n",
    "    - é–‰åŒ…èˆ‡é«˜éšå‡½å¼çš„çµåˆ\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: éæ¿¾å™¨å­—å…¸\n",
    "    \n",
    "    ç¯„ä¾‹:\n",
    "        >>> filters = create_custom_filters()\n",
    "        >>> min_5_filter = filters['min_length'](5)\n",
    "        >>> min_5_filter('hello')  # True\n",
    "        >>> min_5_filter('hi')     # False\n",
    "    \"\"\"\n",
    "    # Ch14 é‡é»ï¼šå»ºç«‹éæ¿¾å™¨å·¥å» \n",
    "    filters = {\n",
    "        # åŸºæœ¬éæ¿¾å™¨ï¼ˆç›´æ¥ä½¿ç”¨ Lambdaï¼‰\n",
    "        'non_empty': lambda text: text.strip() != '',\n",
    "        'only_alpha': lambda text: text.isalpha(),\n",
    "        'only_numeric': lambda text: text.isdigit(),\n",
    "        'mixed_case': lambda text: text != text.lower() and text != text.upper(),\n",
    "        \n",
    "        # é«˜éšéæ¿¾å™¨å·¥å» ï¼ˆè¿”å›å‡½å¼çš„å‡½å¼ï¼‰\n",
    "        'min_length': lambda min_len: lambda text: len(text) >= min_len,\n",
    "        'max_length': lambda max_len: lambda text: len(text) <= max_len,\n",
    "        'exact_length': lambda length: lambda text: len(text) == length,\n",
    "        \n",
    "        'contains_keyword': lambda keyword: (\n",
    "            lambda text: keyword.lower() in text.lower()\n",
    "        ),\n",
    "        \n",
    "        'starts_with': lambda prefix: (\n",
    "            lambda text: text.lower().startswith(prefix.lower())\n",
    "        ),\n",
    "        \n",
    "        'ends_with': lambda suffix: (\n",
    "            lambda text: text.lower().endswith(suffix.lower())\n",
    "        ),\n",
    "        \n",
    "        # è¤‡é›œéæ¿¾å™¨ï¼ˆå¤šåƒæ•¸ï¼‰\n",
    "        'length_range': lambda min_len, max_len: (\n",
    "            lambda text: min_len <= len(text) <= max_len\n",
    "        ),\n",
    "        \n",
    "        'word_count_range': lambda min_words, max_words: (\n",
    "            lambda text: min_words <= len(text.split()) <= max_words\n",
    "        ),\n",
    "        \n",
    "        'contains_any': lambda keywords: (\n",
    "            lambda text: any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "        ),\n",
    "        \n",
    "        'contains_all': lambda keywords: (\n",
    "            lambda text: all(keyword.lower() in text.lower() for keyword in keywords)\n",
    "        ),\n",
    "        \n",
    "        'regex_match': lambda pattern: (\n",
    "            lambda text: __import__('re').search(pattern, text) is not None\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return filters\n",
    "\n",
    "def compose_functions(*functions):\n",
    "    \"\"\"\n",
    "    å‡½å¼çµ„åˆå™¨ - é€²éšå‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ¦‚å¿µ\n",
    "    \n",
    "    å±•ç¤º Ch14 é€²éšæ¦‚å¿µï¼šæ•¸å­¸å¼çš„å‡½å¼çµ„åˆ\n",
    "    - å³åˆ°å·¦çš„å‡½å¼çµ„åˆï¼ˆå¦‚æ•¸å­¸ä¸­çš„ f(g(x))ï¼‰\n",
    "    - reduce çš„é€²éšæ‡‰ç”¨\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        *functions: è¦çµ„åˆçš„å‡½å¼åˆ—è¡¨\n",
    "    \n",
    "    å›å‚³:\n",
    "        function: çµ„åˆå¾Œçš„å‡½å¼\n",
    "    \n",
    "    ç¯„ä¾‹:\n",
    "        >>> f = lambda x: x * 2\n",
    "        >>> g = lambda x: x + 1\n",
    "        >>> h = compose_functions(f, g)  # f(g(x))\n",
    "        >>> h(5)  # f(g(5)) = f(6) = 12\n",
    "        12\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    \n",
    "    if not functions:\n",
    "        return lambda x: x  # æ†ç­‰å‡½å¼\n",
    "    \n",
    "    def compose_two(f, g):\n",
    "        \"\"\"çµ„åˆå…©å€‹å‡½å¼ï¼šf(g(x))\"\"\"\n",
    "        return lambda x: f(g(x))\n",
    "    \n",
    "    # Ch14 é‡é»ï¼šä½¿ç”¨ reduce çµ„åˆå¤šå€‹å‡½å¼\n",
    "    # æ³¨æ„ï¼šé€™æ˜¯å³åˆ°å·¦çš„çµ„åˆé †åº\n",
    "    return reduce(compose_two, reversed(functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç³»çµ±æ•´åˆèˆ‡ä¸»ç¨‹å¼\n",
    "\n",
    "### ğŸ’¡ ç¶œåˆå±•ç¤ºï¼šCh12-15 æ¦‚å¿µæ•´åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_toolkit():\n",
    "    \"\"\"\n",
    "    å»ºç«‹æ–‡å­—è™•ç†å·¥å…·ç®±ä¸»ä»‹é¢\n",
    "    \n",
    "    å±•ç¤º Ch12 æ ¸å¿ƒæ¦‚å¿µï¼šæ¨¡çµ„åŒ–è¨­è¨ˆçš„å®Œæ•´æ‡‰ç”¨\n",
    "    - å°‡ç›¸é—œåŠŸèƒ½çµ„ç¹”ç‚ºæ¨¡çµ„\n",
    "    - ä½¿ç”¨å­—å…¸ä½œç‚º API ä»‹é¢\n",
    "    - å‡½å¼ä½œç‚ºä¸€ç­‰å…¬æ°‘çš„æ¦‚å¿µ\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: å·¥å…·ç®±å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡çµ„åŠŸèƒ½\n",
    "    \"\"\"\n",
    "    # Ch12 é‡é»ï¼šæ¨¡çµ„åŒ–è¨­è¨ˆ\n",
    "    # å°‡ç›¸é—œåŠŸèƒ½åˆ†çµ„ï¼Œä¾¿æ–¼ç®¡ç†å’Œä½¿ç”¨\n",
    "    toolkit = {\n",
    "        'statistics': {\n",
    "            'count_characters': count_characters,\n",
    "            'count_words': count_words,\n",
    "            'count_lines': count_lines,\n",
    "            'analyze_text_statistics': analyze_text_statistics\n",
    "        },\n",
    "        'search': {\n",
    "            'find_keyword': find_keyword,\n",
    "            'find_multiple_keywords': find_multiple_keywords,\n",
    "            'search_in_lines': search_in_lines\n",
    "        },\n",
    "        'transform': {\n",
    "            'transform_case': transform_case,\n",
    "            'replace_text_advanced': replace_text_advanced,\n",
    "            'create_text_transformer': create_text_transformer,\n",
    "            'batch_transform_texts': batch_transform_texts\n",
    "        },\n",
    "        'format': {\n",
    "            'align_text': align_text,\n",
    "            'indent_lines': indent_lines,\n",
    "            'create_columns': create_columns,\n",
    "            'format_table': format_table\n",
    "        },\n",
    "        'analysis': {\n",
    "            'analyze_sentences': analyze_sentences,\n",
    "            'detect_duplicates': detect_duplicates,\n",
    "            'calculate_readability_score': calculate_readability_score\n",
    "        },\n",
    "        # é€²éšæ¨¡çµ„\n",
    "        'recursive': {\n",
    "            'search_files_recursive': search_files_recursive,\n",
    "            'count_pattern_recursive': count_pattern_recursive,\n",
    "            'parse_nested_structure': parse_nested_structure\n",
    "        },\n",
    "        'functional': {\n",
    "            'create_text_pipeline': create_text_pipeline,\n",
    "            'apply_text_filters': apply_text_filters,\n",
    "            'reduce_text_analysis': reduce_text_analysis,\n",
    "            'create_custom_filters': create_custom_filters,\n",
    "            'compose_functions': compose_functions\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return toolkit\n",
    "\n",
    "def interactive_menu():\n",
    "    \"\"\"\n",
    "    äº’å‹•å¼é¸å–®ç³»çµ±\n",
    "    \n",
    "    å±•ç¤ºå®Œæ•´ç³»çµ±çš„ä½¿ç”¨è€…ä»‹é¢è¨­è¨ˆ\n",
    "    \"\"\"\n",
    "    toolkit = create_text_toolkit()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ”§ æ–‡å­—è™•ç†å·¥å…·ç®± v1.0\")\n",
    "        print(\"å±•ç¤º Ch12-15 æ ¸å¿ƒæ¦‚å¿µçš„å®Œæ•´æ‡‰ç”¨\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1. æ–‡å­—çµ±è¨ˆåˆ†æ    (Ch12: å‡½å¼è¨­è¨ˆåŸºç¤)\")\n",
    "        print(\"2. æ–‡å­—æœå°‹åŠŸèƒ½    (Ch14: é«˜éšå‡½å¼èˆ‡ map)\")\n",
    "        print(\"3. æ–‡å­—è½‰æ›å·¥å…·    (Ch13: ä½œç”¨åŸŸèˆ‡é–‰åŒ…)\")\n",
    "        print(\"4. æ–‡å­—æ ¼å¼åŒ–      (Ch12: åƒæ•¸è™•ç†)\")\n",
    "        print(\"5. æ–‡å­—æ·±åº¦åˆ†æ    (Ch12: è¤‡é›œé‚è¼¯åˆ†è§£)\")\n",
    "        print(\"6. éè¿´æª”æ¡ˆè™•ç†    (Ch15: éè¿´æ€ç¶­) [é€²éš]\")\n",
    "        print(\"7. å‡½å¼å¼è™•ç†      (Ch14: filter/reduce) [é€²éš]\")\n",
    "        print(\"8. åŸ·è¡Œå®Œæ•´æ¸¬è©¦\")\n",
    "        print(\"9. å±•ç¤ºæ¦‚å¿µæ•´åˆç¯„ä¾‹\")\n",
    "        print(\"0. çµæŸç¨‹å¼\")\n",
    "        \n",
    "        choice = input(\"\\nè«‹é¸æ“‡åŠŸèƒ½ (0-9): \")\n",
    "        \n",
    "        if choice == \"0\":\n",
    "            print(\"\\næ„Ÿè¬ä½¿ç”¨æ–‡å­—è™•ç†å·¥å…·ç®±ï¼\")\n",
    "            print(\"å¸Œæœ›é€™å€‹å°ˆæ¡ˆå¹«åŠ©æ‚¨ç†è§£ Ch12-15 çš„æ ¸å¿ƒæ¦‚å¿µã€‚\")\n",
    "            break\n",
    "        elif choice in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]:\n",
    "            handle_menu_choice(choice, toolkit)\n",
    "        elif choice == \"8\":\n",
    "            run_complete_test()\n",
    "        elif choice == \"9\":\n",
    "            demonstrate_concept_integration(toolkit)\n",
    "        else:\n",
    "            print(\"âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥ï¼\")\n",
    "\n",
    "def handle_menu_choice(choice, toolkit):\n",
    "    \"\"\"\n",
    "    è™•ç†é¸å–®é¸æ“‡\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        choice (str): ä½¿ç”¨è€…é¸æ“‡\n",
    "        toolkit (dict): å·¥å…·ç®±ç‰©ä»¶\n",
    "    \"\"\"\n",
    "    # é¸å–®å°æ‡‰çš„æ¸¬è©¦å‡½å¼\n",
    "    menu_map = {\n",
    "        \"1\": (\"çµ±è¨ˆæ¨¡çµ„\", test_statistics_comprehensive),\n",
    "        \"2\": (\"æœå°‹æ¨¡çµ„\", test_search_comprehensive),\n",
    "        \"3\": (\"è½‰æ›æ¨¡çµ„\", test_transform_comprehensive),\n",
    "        \"4\": (\"æ ¼å¼åŒ–æ¨¡çµ„\", test_format_comprehensive),\n",
    "        \"5\": (\"åˆ†ææ¨¡çµ„\", test_analysis_comprehensive),\n",
    "        \"6\": (\"éè¿´æ¨¡çµ„\", test_recursive_comprehensive),\n",
    "        \"7\": (\"å‡½å¼å¼æ¨¡çµ„\", test_functional_comprehensive)\n",
    "    }\n",
    "    \n",
    "    if choice in menu_map:\n",
    "        module_name, test_func = menu_map[choice]\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ğŸ§ª æ¸¬è©¦ {module_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            test_func()\n",
    "            print(f\"\\nâœ… {module_name} æ¸¬è©¦å®Œæˆ\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ {module_name} æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        input(\"\\næŒ‰ Enter ç¹¼çºŒ...\")\n",
    "\n",
    "def demonstrate_concept_integration(toolkit):\n",
    "    \"\"\"\n",
    "    å±•ç¤º Ch12-15 æ¦‚å¿µæ•´åˆçš„ç¯„ä¾‹\n",
    "    \n",
    "    å±•ç¤ºå¦‚ä½•å°‡æ‰€æœ‰å­¸éçš„æ¦‚å¿µçµ„åˆä½¿ç”¨\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¯ Ch12-15 æ¦‚å¿µæ•´åˆå±•ç¤º\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ç¯„ä¾‹æ–‡å­—\n",
    "    sample_text = \"\"\"\n",
    "    Python æ˜¯ä¸€ç¨®é«˜ç´šç¨‹å¼èªè¨€ã€‚å®ƒæ˜“æ–¼å­¸ç¿’ä¸”åŠŸèƒ½å¼·å¤§ã€‚\n",
    "    è¨±å¤šå…¬å¸éƒ½ä½¿ç”¨ Python é–‹ç™¼è»Ÿé«”ã€‚Python çš„èªæ³•ç°¡æ½”å„ªé›…ã€‚\n",
    "    å‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ˜¯ Python çš„å¼·é …ä¹‹ä¸€ã€‚\n",
    "    éè¿´æ˜¯è§£æ±ºè¤‡é›œå•é¡Œçš„é‡è¦æŠ€å·§ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ“ ç¯„ä¾‹æ–‡å­—:\")\n",
    "    print(sample_text.strip())\n",
    "    \n",
    "    # Ch12 å±•ç¤ºï¼šå‡½å¼çµ„åˆ\n",
    "    print(\"\\nğŸ”§ Ch12 å±•ç¤ºï¼šå‡½å¼çµ„åˆ\")\n",
    "    stats = toolkit['statistics']['analyze_text_statistics'](sample_text)\n",
    "    print(f\"ç¸½å­—å…ƒæ•¸: {stats['characters']['total_chars']}\")\n",
    "    print(f\"ç¸½è©æ•¸: {stats['words']['total_words']}\")\n",
    "    print(f\"ç¸½è¡Œæ•¸: {stats['lines']['total_lines']}\")\n",
    "    \n",
    "    # Ch13 å±•ç¤ºï¼šé–‰åŒ…æ‡‰ç”¨\n",
    "    print(\"\\nğŸ”„ Ch13 å±•ç¤ºï¼šé–‰åŒ…æ‡‰ç”¨\")\n",
    "    transformer = toolkit['transform']['create_text_transformer']({\n",
    "        'case_mode': 'lower',\n",
    "        'remove_punctuation': True\n",
    "    })\n",
    "    transformed = transformer(\"Hello, World! é€™æ˜¯æ¸¬è©¦ã€‚\")\n",
    "    print(f\"é–‰åŒ…è½‰æ›çµæœ: '{transformed}'\")\n",
    "    \n",
    "    # Ch14 å±•ç¤ºï¼šé«˜éšå‡½å¼\n",
    "    print(\"\\nğŸ§© Ch14 å±•ç¤ºï¼šé«˜éšå‡½å¼\")\n",
    "    lines = sample_text.strip().split('\\n')\n",
    "    filters = toolkit['functional']['create_custom_filters']()\n",
    "    \n",
    "    # ä½¿ç”¨å¤šå€‹éæ¿¾å™¨\n",
    "    filtered_lines = toolkit['functional']['apply_text_filters'](\n",
    "        lines,\n",
    "        filters['non_empty'],\n",
    "        filters['contains_keyword']('Python')\n",
    "    )\n",
    "    print(f\"åŒ…å« 'Python' çš„è¡Œæ•¸: {len(filtered_lines)}\")\n",
    "    \n",
    "    # Ch15 å±•ç¤ºï¼šéè¿´æ€ç¶­\n",
    "    print(\"\\nğŸ”„ Ch15 å±•ç¤ºï¼šéè¿´æ€ç¶­\")\n",
    "    recursive_count = toolkit['recursive']['count_pattern_recursive'](sample_text, \"Python\")\n",
    "    print(f\"éè¿´è¨ˆç®— 'Python' å‡ºç¾æ¬¡æ•¸: {recursive_count}\")\n",
    "    \n",
    "    # ç¶œåˆå±•ç¤ºï¼šä½¿ç”¨æµæ°´ç·šè™•ç†\n",
    "    print(\"\\nğŸš€ ç¶œåˆå±•ç¤ºï¼šæµæ°´ç·šè™•ç†\")\n",
    "    pipeline = toolkit['functional']['create_text_pipeline'](\n",
    "        lambda x: x.lower(),                    # Ch14: Lambda\n",
    "        lambda x: x.replace('python', 'PYTHON'), # å¼·èª¿é—œéµå­—\n",
    "        lambda x: x.strip()                     # æ¸…ç†\n",
    "    )\n",
    "    \n",
    "    for line in lines[:2]:  # è™•ç†å‰å…©è¡Œ\n",
    "        if line.strip():\n",
    "            processed = pipeline(line.strip())\n",
    "            print(f\"åŸæ–‡: {line.strip()[:30]}...\")\n",
    "            print(f\"è™•ç†å¾Œ: {processed[:30]}...\")\n",
    "            print()\n",
    "    \n",
    "    print(\"ğŸ‰ æ¦‚å¿µæ•´åˆå±•ç¤ºå®Œæˆï¼\")\n",
    "    print(\"é€™å€‹ç¯„ä¾‹å±•ç¤ºäº†å¦‚ä½•å°‡ Ch12-15 çš„æ¦‚å¿µçµ„åˆä½¿ç”¨ã€‚\")\n",
    "\n",
    "# ä¸»ç¨‹å¼å…¥å£\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»ç¨‹å¼å…¥å£é»\n",
    "    \n",
    "    å±•ç¤ºå®Œæ•´çš„éŒ¯èª¤è™•ç†èˆ‡ä½¿ç”¨è€…é«”é©—\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ æ­¡è¿ä½¿ç”¨ Milestone 04: æ–‡å­—è™•ç†å·¥å…·ç®±\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸ“š æœ¬å°ˆæ¡ˆå±•ç¤º Ch12-15 çš„æ ¸å¿ƒæ¦‚å¿µï¼š\")\n",
    "    print(\"  â€¢ Ch12: å‡½å¼è¨­è¨ˆåŸºç¤èˆ‡æ¨¡çµ„åŒ–\")\n",
    "    print(\"  â€¢ Ch13: ä½œç”¨åŸŸèˆ‡é–‰åŒ…æ‡‰ç”¨\")\n",
    "    print(\"  â€¢ Ch14: é«˜éšå‡½å¼èˆ‡ Lambda\")\n",
    "    print(\"  â€¢ Ch15: éè¿´æ€ç¶­èˆ‡æ‡‰ç”¨\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸ’¡ é€™æ˜¯ä¸€å€‹ç¶œåˆæ€§çš„å­¸ç¿’å°ˆæ¡ˆï¼Œæ•´åˆäº†å‡½å¼å¼ç¨‹å¼è¨­è¨ˆçš„æ ¸å¿ƒæ¦‚å¿µã€‚\")\n",
    "    \n",
    "    try:\n",
    "        interactive_menu()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nğŸ‘‹ ç¨‹å¼å·²ä¸­æ–·ï¼Œæ„Ÿè¬ä½¿ç”¨ï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ç¨‹å¼ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}\")\n",
    "        print(\"è«‹æª¢æŸ¥ç¨‹å¼ç¢¼ä¸¦ä¿®æ­£éŒ¯èª¤ã€‚\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# å¦‚æœç›´æ¥åŸ·è¡Œæ­¤æª”æ¡ˆï¼Œå•Ÿå‹•ä¸»ç¨‹å¼\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª å®Œæ•´æ¸¬è©¦å¥—ä»¶\n",
    "\n",
    "### å±•ç¤ºå„æ¨¡çµ„çš„å®Œæ•´æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_statistics_comprehensive():\n",
    "    \"\"\"çµ±è¨ˆæ¨¡çµ„å®Œæ•´æ¸¬è©¦\"\"\"\n",
    "    print(\"ğŸ“Š æ¸¬è©¦çµ±è¨ˆæ¨¡çµ„åŠŸèƒ½\")\n",
    "    \n",
    "    test_text = \"Hello, World! 123\\nPython is great. ç¨‹å¼è¨­è¨ˆå¾ˆæœ‰è¶£ï¼\"\n",
    "    \n",
    "    # æ¸¬è©¦å­—å…ƒçµ±è¨ˆ\n",
    "    char_stats = count_characters(test_text)\n",
    "    print(f\"\\nå­—å…ƒçµ±è¨ˆ: {char_stats}\")\n",
    "    \n",
    "    # æ¸¬è©¦è©å½™çµ±è¨ˆ\n",
    "    word_stats = count_words(test_text)\n",
    "    print(f\"è©å½™çµ±è¨ˆ: {word_stats}\")\n",
    "    \n",
    "    # æ¸¬è©¦è¡Œæ•¸çµ±è¨ˆ\n",
    "    line_stats = count_lines(test_text)\n",
    "    print(f\"è¡Œæ•¸çµ±è¨ˆ: {line_stats}\")\n",
    "    \n",
    "    # æ¸¬è©¦ç¶œåˆçµ±è¨ˆ\n",
    "    full_stats = analyze_text_statistics(test_text)\n",
    "    print(f\"\\nâœ… ç¶œåˆçµ±è¨ˆæ¸¬è©¦å®Œæˆ\")\n",
    "\n",
    "def test_search_comprehensive():\n",
    "    \"\"\"æœå°‹æ¨¡çµ„å®Œæ•´æ¸¬è©¦\"\"\"\n",
    "    print(\"ğŸ” æ¸¬è©¦æœå°‹æ¨¡çµ„åŠŸèƒ½\")\n",
    "    \n",
    "    test_text = \"\"\"Python is awesome!\n",
    "I love Python programming.\n",
    "Python makes coding fun.\n",
    "Java is also good.\"\"\"\n",
    "    \n",
    "    # æ¸¬è©¦å–®ä¸€é—œéµå­—æœå°‹\n",
    "    result = find_keyword(test_text, \"Python\")\n",
    "    print(f\"\\næœå°‹ 'Python': æ‰¾åˆ° {result['count']} æ¬¡\")\n",
    "    \n",
    "    # æ¸¬è©¦å¤šé—œéµå­—æœå°‹ï¼ˆCh14 map æ‡‰ç”¨ï¼‰\n",
    "    keywords = [\"Python\", \"Java\", \"programming\"]\n",
    "    multi_results = find_multiple_keywords(test_text, keywords)\n",
    "    print(f\"å¤šé—œéµå­—æœå°‹çµæœæ•¸é‡: {len(multi_results)}\")\n",
    "    \n",
    "    # æ¸¬è©¦è¡Œæœå°‹èˆ‡éæ¿¾ï¼ˆCh14 filter + Lambda æ‡‰ç”¨ï¼‰\n",
    "    line_results = search_in_lines(test_text, \"Python\", lambda line: len(line) > 15)\n",
    "    print(f\"éæ¿¾å¾Œçš„è¡Œæ•¸: {len(line_results)}\")\n",
    "    print(\"âœ… æœå°‹åŠŸèƒ½æ¸¬è©¦å®Œæˆ\")\n",
    "\n",
    "def test_transform_comprehensive():\n",
    "    \"\"\"è½‰æ›æ¨¡çµ„å®Œæ•´æ¸¬è©¦\"\"\"\n",
    "    print(\"ğŸ”„ æ¸¬è©¦è½‰æ›æ¨¡çµ„åŠŸèƒ½\")\n",
    "    \n",
    "    test_texts = [\"Hello, World!\", \"Python123 is GREAT!\", \"Test... 456\"]\n",
    "    \n",
    "    # æ¸¬è©¦åŸºæœ¬å¤§å°å¯«è½‰æ›\n",
    "    print(\"\\nå¤§å°å¯«è½‰æ›æ¸¬è©¦:\")\n",
    "    for mode in ['lower', 'upper', 'title']:\n",
    "        result = transform_case(test_texts[0], mode)\n",
    "        print(f\"  {mode}: {result}\")\n",
    "    \n",
    "    # æ¸¬è©¦é–‰åŒ…è½‰æ›å™¨ï¼ˆCh13 é‡é»ï¼‰\n",
    "    config = {\n",
    "        'case_mode': 'lower',\n",
    "        'remove_punctuation': True,\n",
    "        'replace_numbers': True\n",
    "    }\n",
    "    transformer = create_text_transformer(config)\n",
    "    transformed = transformer(\"Hello, World! 123\")\n",
    "    print(f\"\\né–‰åŒ…è½‰æ›å™¨çµæœ: '{transformed}'\")\n",
    "    \n",
    "    # æ¸¬è©¦æ‰¹æ¬¡è½‰æ›ï¼ˆCh14 map æ‡‰ç”¨ï¼‰\n",
    "    batch_results = batch_transform_texts(test_texts, lambda x: x.upper())\n",
    "    print(f\"æ‰¹æ¬¡è½‰æ›å®Œæˆ: {len(batch_results)} å€‹æ–‡å­—\")\n",
    "    print(\"âœ… è½‰æ›åŠŸèƒ½æ¸¬è©¦å®Œæˆ\")\n",
    "\n",
    "def test_format_comprehensive():\n",
    "    \"\"\"æ ¼å¼åŒ–æ¨¡çµ„å®Œæ•´æ¸¬è©¦\"\"\"\n",
    "    print(\"ğŸ“ æ¸¬è©¦æ ¼å¼åŒ–æ¨¡çµ„åŠŸèƒ½\")\n",
    "    \n",
    "    # æ¸¬è©¦æ–‡å­—å°é½Š\n",
    "    test_text = \"Hello\\nWorld\"\n",
    "    aligned = align_text(test_text, width=10, alignment='center')\n",
    "    print(f\"\\nå°é½Šæ¸¬è©¦å®Œæˆï¼Œè¡Œæ•¸: {len(aligned.splitlines())}\")\n",
    "    \n",
    "    # æ¸¬è©¦å¤šæ¬„é¡¯ç¤º\n",
    "    items = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n",
    "    columns = create_columns(items, columns=2)\n",
    "    print(f\"å¤šæ¬„é¡¯ç¤ºæ¸¬è©¦å®Œæˆï¼Œè¡Œæ•¸: {len(columns.splitlines())}\")\n",
    "    \n",
    "    # æ¸¬è©¦è¡¨æ ¼æ ¼å¼åŒ–\n",
    "    headers = ['Name', 'Age', 'City']\n",
    "    data = [['Alice', '25', 'New York'], ['Bob', '30', 'London']]\n",
    "    table = format_table(data, headers)\n",
    "    print(f\"è¡¨æ ¼æ ¼å¼åŒ–æ¸¬è©¦å®Œæˆï¼Œè¡Œæ•¸: {len(table.splitlines())}\")\n",
    "    print(\"âœ… æ ¼å¼åŒ–åŠŸèƒ½æ¸¬è©¦å®Œæˆ\")\n",
    "\n",
    "def test_analysis_comprehensive():\n",
    "    \"\"\"åˆ†ææ¨¡çµ„å®Œæ•´æ¸¬è©¦\"\"\"\n",
    "    print(\"ğŸ”¬ æ¸¬è©¦åˆ†ææ¨¡çµ„åŠŸèƒ½\")\n",
    "    \n",
    "    test_text = \"\"\"Hello world! This is a test. \n",
    "    Python is great. Python makes programming fun!\n",
    "    Programming is an art. Art is beautiful.\"\"\"\n",
    "    \n",
    "    # æ¸¬è©¦å¥å­åˆ†æ\n",
    "    sentence_result = analyze_sentences(test_text)\n",
    "    print(f\"\\nå¥å­åˆ†æ: æ‰¾åˆ° {sentence_result['sentence_count']} å€‹å¥å­\")\n",
    "    \n",
    "    # æ¸¬è©¦é‡è¤‡æª¢æ¸¬\n",
    "    duplicate_result = detect_duplicates(test_text, min_length=3)\n",
    "    print(f\"é‡è¤‡æª¢æ¸¬: æ‰¾åˆ° {len(duplicate_result)} å€‹é‡è¤‡ç‰‡æ®µ\")\n",
    "    \n",
    "    # æ¸¬è©¦å¯è®€æ€§åˆ†æ\n",
    "    readability_result = calculate_readability_score(test_text)\n",
    "    print(f\"å¯è®€æ€§åˆ†æ: {readability_result['difficulty_level']} é›£åº¦\")\n",
    "    print(\"âœ… åˆ†æåŠŸèƒ½æ¸¬è©¦å®Œæˆ\")\n",
    "\n",
    "def test_recursive_comprehensive():\n",
    "    \"\"\"éè¿´æ¨¡çµ„å®Œæ•´æ¸¬è©¦\"\"\"\n",
    "    print(\"ğŸ”„ æ¸¬è©¦éè¿´æ¨¡çµ„åŠŸèƒ½ (Ch15 é‡é»)\")\n",
    "    \n",
    "    # æ¸¬è©¦éè¿´è¨ˆæ•¸\n",
    "    test_text = \"Python is great. Python rocks. Python!\"\n",
    "    count = count_pattern_recursive(test_text, \"Python\")\n",
    "    print(f\"\\néè¿´è¨ˆæ•¸ 'Python': {count} æ¬¡\")\n",
    "    \n",
    "    # æ¸¬è©¦å·¢ç‹€çµæ§‹è§£æ\n",
    "    nested_text = \"[A[B[C]D]E]\"\n",
    "    parsed = parse_nested_structure(nested_text)\n",
    "    print(f\"å·¢ç‹€çµæ§‹è§£æ: æœ€å¤§æ·±åº¦ {parsed['max_depth']}, å¹³è¡¡: {parsed['is_balanced']}\")\n",
    "    \n",
    "    print(\"âœ… éè¿´åŠŸèƒ½æ¸¬è©¦å®Œæˆ\")\n",
    "\n",
    "def test_functional_comprehensive():\n",
    "    \"\"\"å‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ¨¡çµ„å®Œæ•´æ¸¬è©¦\"\"\"\n",
    "    print(\"ğŸ§© æ¸¬è©¦å‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ¨¡çµ„ (Ch14 é€²éš)\")\n",
    "    \n",
    "    # æ¸¬è©¦æ–‡å­—è™•ç†æµæ°´ç·š\n",
    "    pipeline = create_text_pipeline(\n",
    "        lambda x: x.lower(),\n",
    "        lambda x: x.replace(' ', '_'),\n",
    "        lambda x: x.strip()\n",
    "    )\n",
    "    result = pipeline(\"  Hello World  \")\n",
    "    print(f\"\\næµæ°´ç·šè™•ç†çµæœ: '{result}'\")\n",
    "    \n",
    "    # æ¸¬è©¦éæ¿¾å™¨é€£é–\n",
    "    texts = [\"hello\", \"\", \"world\", \"python\", \"hi\"]\n",
    "    filtered = apply_text_filters(\n",
    "        texts,\n",
    "        lambda x: x != \"\",      # ç§»é™¤ç©ºå­—ä¸²\n",
    "        lambda x: len(x) > 4    # ä¿ç•™é•·åº¦ > 4 çš„è©\n",
    "    )\n",
    "    print(f\"éæ¿¾å™¨é€£é–çµæœ: {filtered}\")\n",
    "    \n",
    "    # æ¸¬è©¦è‡ªè¨‚éæ¿¾å™¨å·¥å» \n",
    "    filters = create_custom_filters()\n",
    "    min_5_filter = filters['min_length'](5)\n",
    "    print(f\"è‡ªè¨‚éæ¿¾å™¨æ¸¬è©¦: min_5_filter('hello') = {min_5_filter('hello')}\")\n",
    "    \n",
    "    print(\"âœ… å‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ¸¬è©¦å®Œæˆ\")\n",
    "\n",
    "def run_complete_test():\n",
    "    \"\"\"\n",
    "    åŸ·è¡Œå®Œæ•´çš„å·¥å…·ç®±æ¸¬è©¦\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ§ª é–‹å§‹å®Œæ•´æ¸¬è©¦...\")\n",
    "    \n",
    "    test_modules = [\n",
    "        (\"çµ±è¨ˆæ¨¡çµ„ (Ch12)\", test_statistics_comprehensive),\n",
    "        (\"æœå°‹æ¨¡çµ„ (Ch14)\", test_search_comprehensive),\n",
    "        (\"è½‰æ›æ¨¡çµ„ (Ch13)\", test_transform_comprehensive),\n",
    "        (\"æ ¼å¼åŒ–æ¨¡çµ„ (Ch12)\", test_format_comprehensive),\n",
    "        (\"åˆ†ææ¨¡çµ„ (Ch12)\", test_analysis_comprehensive),\n",
    "        (\"éè¿´æ¨¡çµ„ (Ch15)\", test_recursive_comprehensive),\n",
    "        (\"å‡½å¼å¼æ¨¡çµ„ (Ch14)\", test_functional_comprehensive)\n",
    "    ]\n",
    "    \n",
    "    passed_tests = 0\n",
    "    total_tests = len(test_modules)\n",
    "    \n",
    "    for module_name, test_func in test_modules:\n",
    "        try:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"ğŸ”¬ æ¸¬è©¦ {module_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            test_func()\n",
    "            print(f\"\\nâœ… {module_name} æ¸¬è©¦é€šé\")\n",
    "            passed_tests += 1\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ {module_name} æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ‰ æ¸¬è©¦å®Œæˆ: {passed_tests}/{total_tests} å€‹æ¨¡çµ„é€šéæ¸¬è©¦\")\n",
    "    \n",
    "    if passed_tests == total_tests:\n",
    "        print(\"ğŸ† æ­å–œï¼æ‰€æœ‰æ¨¡çµ„éƒ½æ­£å¸¸é‹ä½œï¼\")\n",
    "        print(\"æ‚¨å·²æˆåŠŸå®Œæˆ Milestone 04ï¼ŒæŒæ¡äº† Ch12-15 çš„æ ¸å¿ƒæ¦‚å¿µã€‚\")\n",
    "    else:\n",
    "        print(\"âš ï¸ éƒ¨åˆ†æ¨¡çµ„éœ€è¦æª¢æŸ¥ï¼Œè«‹æª¢è¦–éŒ¯èª¤è¨Šæ¯ä¸¦ä¿®æ­£ã€‚\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦\n",
    "if __name__ == \"__main__\":\n",
    "    run_complete_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ è§£ç­”ç¸½çµ\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µå±•ç¤º\n",
    "\n",
    "æœ¬åƒè€ƒè§£ç­”å®Œæ•´å±•ç¤ºäº† **Ch12-15** çš„æ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µï¼š\n",
    "\n",
    "#### Ch12: å‡½å¼è¨­è¨ˆåŸºç¤\n",
    "- âœ… **å–®ä¸€è·è²¬åŸå‰‡**: æ¯å€‹å‡½å¼åªåšä¸€ä»¶äº‹\n",
    "- âœ… **æ¨¡çµ„åŒ–è¨­è¨ˆ**: å°‡ç›¸é—œåŠŸèƒ½çµ„ç¹”ç‚ºæ¨¡çµ„\n",
    "- âœ… **åƒæ•¸è™•ç†**: é è¨­å€¼ã€å¯é¸åƒæ•¸ã€éŒ¯èª¤è™•ç†\n",
    "- âœ… **æ–‡ä»¶å­—ä¸²**: å®Œæ•´çš„ docstring èˆ‡ç¯„ä¾‹\n",
    "- âœ… **å‡½å¼çµ„åˆ**: å¤§å‡½å¼ç”±å°å‡½å¼çµ„åˆè€Œæˆ\n",
    "\n",
    "#### Ch13: ä½œç”¨åŸŸèˆ‡ç”Ÿå‘½é€±æœŸ\n",
    "- âœ… **å…§åµŒå‡½å¼**: `_recursive_search`, `transformer`\n",
    "- âœ… **é–‰åŒ…æ‡‰ç”¨**: `create_text_transformer` ä¿å­˜é…ç½®ç‹€æ…‹\n",
    "- âœ… **è®Šæ•¸ä½œç”¨åŸŸ**: æ­£ç¢ºå­˜å–å¤–å±¤è®Šæ•¸\n",
    "- âœ… **ç‹€æ…‹ä¿å­˜**: é–‰åŒ…è¨˜ä½é…ç½®åƒæ•¸\n",
    "\n",
    "#### Ch14: é«˜éšå‡½å¼èˆ‡ Lambda\n",
    "- âœ… **map å‡½å¼**: `find_multiple_keywords`, `batch_transform_texts`\n",
    "- âœ… **filter å‡½å¼**: `search_in_lines`, `apply_text_filters`\n",
    "- âœ… **reduce å‡½å¼**: `reduce_text_analysis`, `compose_functions`\n",
    "- âœ… **Lambda å‡½å¼**: éæ¿¾å™¨å·¥å» ã€æµæ°´ç·šè™•ç†\n",
    "- âœ… **å‡½å¼çµ„åˆ**: æµæ°´ç·šèˆ‡çµ„åˆå™¨\n",
    "\n",
    "#### Ch15: éè¿´æ€ç¶­\n",
    "- âœ… **åŸºæœ¬æƒ…æ³**: æ˜ç¢ºçš„çµ‚æ­¢æ¢ä»¶\n",
    "- âœ… **éè¿´æƒ…æ³**: æ­£ç¢ºçš„ç‹€æ…‹æ”¹è®Š\n",
    "- âœ… **å¯¦éš›æ‡‰ç”¨**: æª”æ¡ˆæœå°‹ã€å·¢ç‹€çµæ§‹è§£æ\n",
    "- âœ… **é˜²è­·æ©Ÿåˆ¶**: æœ€å¤§æ·±åº¦é™åˆ¶\n",
    "\n",
    "### ğŸ’¡ ç¨‹å¼è¨­è¨ˆæœ€ä½³å¯¦è¸\n",
    "\n",
    "1. **éŒ¯èª¤è™•ç†**: å®Œå–„çš„ä¾‹å¤–è™•ç†èˆ‡é‚Šç•Œæ¢ä»¶æª¢æŸ¥\n",
    "2. **ç¨‹å¼ç¢¼å“è³ª**: æ¸…æ™°çš„è®Šæ•¸å‘½åèˆ‡é‚è¼¯çµæ§‹\n",
    "3. **ä½¿ç”¨è€…é«”é©—**: å‹å–„çš„éŒ¯èª¤è¨Šæ¯èˆ‡æ“ä½œæç¤º\n",
    "4. **æ•ˆèƒ½è€ƒé‡**: é©ç•¶çš„æ¼”ç®—æ³•é¸æ“‡èˆ‡å„ªåŒ–\n",
    "5. **å¯ç¶­è­·æ€§**: æ¨¡çµ„åŒ–è¨­è¨ˆèˆ‡æ¸…æ™°çš„ä»‹é¢\n",
    "\n",
    "### ğŸš€ å»¶ä¼¸å­¸ç¿’\n",
    "\n",
    "å®Œæˆæ­¤å°ˆæ¡ˆå¾Œï¼Œæ‚¨å¯ä»¥ï¼š\n",
    "- æ·±å…¥å­¸ç¿’æ­£è¦è¡¨é”å¼ (Ch23 æª”æ¡ˆè™•ç†)\n",
    "- æ¢ç´¢æ›´å¤šè¨­è¨ˆæ¨¡å¼ (Ch27 æ¨¡çµ„è¨­è¨ˆ)\n",
    "- å¯¦ä½œæ›´è¤‡é›œçš„æ¼”ç®—æ³•\n",
    "- å­¸ç¿’å¹³è¡Œè™•ç†èˆ‡æ•ˆèƒ½å„ªåŒ–\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆ Milestone 04ï¼æ‚¨å·²ç¶“æŒæ¡äº†å‡½å¼å¼ç¨‹å¼è¨­è¨ˆçš„æ ¸å¿ƒæ¦‚å¿µï¼Œå…·å‚™äº†å»ºæ§‹è¤‡é›œè»Ÿé«”ç³»çµ±çš„èƒ½åŠ›ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}