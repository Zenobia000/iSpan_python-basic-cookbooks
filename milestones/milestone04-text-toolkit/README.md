# Milestone 04: æ–‡å­—è™•ç†å·¥å…·ç®± | Text Processing Toolkit

## ğŸ¯ å°ˆæ¡ˆç›®æ¨™ï¼ˆProject Objectivesï¼‰

æœ¬å°ˆæ¡ˆæ—¨åœ¨æ•´åˆä»¥ä¸‹ç« ç¯€æ‰€å­¸ï¼š
- **Chapter 12ï¼ˆå‡½å¼è¨­è¨ˆåŸºç¤ï¼‰**ï¼šæ¨¡çµ„åŒ–è¨­è¨ˆã€åƒæ•¸è™•ç†ã€è¿”å›å€¼
- **Chapter 13ï¼ˆä½œç”¨åŸŸèˆ‡ç”Ÿå‘½é€±æœŸï¼‰**ï¼šè®Šæ•¸ä½œç”¨åŸŸã€å…§åµŒå‡½å¼
- **Chapter 14ï¼ˆé«˜éšå‡½å¼èˆ‡ Lambdaï¼‰**ï¼šmap/filter/reduceã€å‡½å¼çµ„åˆ
- **Chapter 15ï¼ˆéè¿´æ€ç¶­ï¼‰**ï¼šéè¿´è™•ç†ã€åˆ†æ²»æ³•ã€æ¨¹ç‹€çµæ§‹

---

## ğŸ“‹ å°ˆæ¡ˆæè¿°ï¼ˆProject Descriptionï¼‰

### æƒ…å¢ƒèªªæ˜

æ‚¨æ˜¯ä¸€åè³‡æ–™åˆ†æå¸«ï¼Œç¶“å¸¸éœ€è¦è™•ç†å„ç¨®æ–‡å­—è³‡æ–™ã€‚ç‚ºäº†æé«˜å·¥ä½œæ•ˆç‡ï¼Œæ‚¨æ±ºå®šå»ºç«‹ä¸€å€‹ç¶œåˆæ€§çš„æ–‡å­—è™•ç†å·¥å…·ç®±ï¼ŒåŒ…å«æ–‡å­—çµ±è¨ˆã€æ ¼å¼åŒ–ã€æœå°‹ã€è½‰æ›ç­‰åŠŸèƒ½ã€‚

é€™å€‹å·¥å…·ç®±å°‡å±•ç¤ºå‡½å¼è¨­è¨ˆçš„å¨åŠ›ï¼šé€éçµ„åˆç°¡å–®çš„å‡½å¼ï¼Œå»ºæ§‹å‡ºè¤‡é›œè€Œå¯¦ç”¨çš„æ–‡å­—è™•ç†ç³»çµ±ã€‚

### åŠŸèƒ½éœ€æ±‚

#### åŸºæœ¬éœ€æ±‚ï¼ˆå¿…é ˆå®Œæˆ - 70åˆ†ï¼‰
1. **æ–‡å­—çµ±è¨ˆæ¨¡çµ„**ï¼šå­—æ•¸ã€è¡Œæ•¸ã€è©é »çµ±è¨ˆ
2. **æ–‡å­—æœå°‹æ¨¡çµ„**ï¼šé—œéµå­—æœå°‹ã€æ¨¡å¼åŒ¹é…
3. **æ–‡å­—è½‰æ›æ¨¡çµ„**ï¼šå¤§å°å¯«è½‰æ›ã€å­—å…ƒæ›¿æ›
4. **æ–‡å­—æ ¼å¼åŒ–æ¨¡çµ„**ï¼šå°é½Šã€ç¸®æ’ã€åˆ†æ¬„é¡¯ç¤º
5. **æ–‡å­—åˆ†ææ¨¡çµ„**ï¼šå¥å­åˆ†æã€é‡è¤‡æª¢æ¸¬

#### é€²éšéœ€æ±‚ï¼ˆé¸åš - é¡å¤–30åˆ†ï¼‰
1. **éè¿´æª”æ¡ˆè™•ç†**ï¼šéè¿´æœå°‹ç›®éŒ„ä¸­çš„æ–‡å­—æª”æ¡ˆ
2. **é«˜éšå‡½å¼æ‡‰ç”¨**ï¼šä½¿ç”¨ map/filter/reduce é€²è¡Œè¤‡é›œè™•ç†
3. **æ–‡å­—å£“ç¸®æ¼”ç®—æ³•**ï¼šå¯¦ä½œç°¡å–®çš„ RLE å£“ç¸®
4. **äº’å‹•å¼é¸å–®**ï¼šå»ºç«‹å‘½ä»¤åˆ—ç•Œé¢
5. **æ•ˆèƒ½å„ªåŒ–**ï¼šå¤§æª”æ¡ˆè™•ç†å„ªåŒ–

### æŠ€è¡“è¦æ ¼
- **Python ç‰ˆæœ¬**ï¼š3.8+
- **æ ¸å¿ƒæŠ€è¡“**ï¼šå‡½å¼è¨­è¨ˆã€éè¿´ã€é«˜éšå‡½å¼ã€å­—ä¸²è™•ç†
- **é è¨ˆç¨‹å¼ç¢¼è¡Œæ•¸**ï¼š300-500 è¡Œï¼ˆåŸºæœ¬éœ€æ±‚ï¼‰ï¼Œ600-800 è¡Œï¼ˆå«é€²éšï¼‰
- **é è¨ˆé–‹ç™¼æ™‚é–“**ï¼š20-30 å°æ™‚

---

## ğŸ—ï¸ å°ˆæ¡ˆçµæ§‹ï¼ˆProject Structureï¼‰

```
milestone04-text-toolkit/
â”œâ”€â”€ README.md              # å°ˆæ¡ˆèªªæ˜ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â”œâ”€â”€ requirements.ipynb     # è©³ç´°éœ€æ±‚è¦æ ¼èˆ‡ç¯„ä¾‹
â”œâ”€â”€ starter-code.ipynb     # èµ·å§‹ç¨‹å¼ç¢¼æ¡†æ¶èˆ‡æç¤º
â””â”€â”€ solution.ipynb         # å®Œæ•´åƒè€ƒè§£ç­”
```

---

## ğŸ“š å­¸ç¿’æˆæœï¼ˆLearning Outcomesï¼‰

å®Œæˆæ­¤å°ˆæ¡ˆå¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

### çŸ¥è­˜é¢ï¼ˆKnowledgeï¼‰
- **æ•´åˆé‹ç”¨** Ch12-15 çš„æ‰€æœ‰æ¦‚å¿µ
- **ç†è§£** æ¨¡çµ„åŒ–ç¨‹å¼è¨­è¨ˆçš„é‡è¦æ€§
- **æŒæ¡** å‡½å¼é–“çš„å”ä½œèˆ‡çµ„åˆæ–¹å¼

### æŠ€èƒ½é¢ï¼ˆSkillsï¼‰
- **è¨­è¨ˆ** æ¸…æ™°çš„å‡½å¼ä»‹é¢ï¼ˆAPIï¼‰
- **å¯¦ä½œ** éè¿´æª”æ¡ˆç³»çµ±éæ­·
- **é‹ç”¨** é«˜éšå‡½å¼é€²è¡Œè³‡æ–™è½‰æ›
- **å»ºç«‹** å¯é‡ç”¨çš„å‡½å¼åº«

### æ‡‰ç”¨é¢ï¼ˆApplicationï¼‰
- **è§£æ±º** å¯¦éš›çš„æ–‡å­—è™•ç†éœ€æ±‚
- **å»ºæ§‹** å®Œæ•´çš„è»Ÿé«”å°ˆæ¡ˆ
- **é«”é©—** å‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ€ç¶­

---

## ğŸ”§ é–‹ç™¼æ­¥é©Ÿå»ºè­°ï¼ˆDevelopment Guideï¼‰

### Phase 1: åˆ†æèˆ‡è¨­è¨ˆï¼ˆ60 åˆ†é˜ï¼‰

#### æ­¥é©Ÿ 1.1: æ¨¡çµ„æ¶æ§‹è¨­è¨ˆ
```python
# æ ¸å¿ƒæ¨¡çµ„çµæ§‹
text_toolkit = {
    'statistics': {    # æ–‡å­—çµ±è¨ˆæ¨¡çµ„
        'count_words', 'count_lines', 'word_frequency'
    },
    'search': {        # æ–‡å­—æœå°‹æ¨¡çµ„
        'find_keywords', 'pattern_match', 'search_lines'
    },
    'transform': {     # æ–‡å­—è½‰æ›æ¨¡çµ„
        'case_transform', 'replace_text', 'normalize'
    },
    'format': {        # æ–‡å­—æ ¼å¼åŒ–æ¨¡çµ„
        'align_text', 'indent_text', 'columnize'
    },
    'analysis': {      # æ–‡å­—åˆ†ææ¨¡çµ„
        'sentence_analysis', 'duplicate_detection'
    }
}
```

#### æ­¥é©Ÿ 1.2: å‡½å¼ä»‹é¢è¨­è¨ˆï¼ˆAPI Designï¼‰
è¨­è¨ˆæ¸…æ™°çš„å‡½å¼ç°½åï¼Œéµå¾ªå–®ä¸€è·è²¬åŸå‰‡ï¼š
```python
def count_words(text: str) -> dict:
    """çµ±è¨ˆæ–‡å­—ä¸­çš„è©å½™æ•¸é‡"""
    pass

def find_keywords(text: str, keywords: list) -> dict:
    """åœ¨æ–‡å­—ä¸­æœå°‹é—œéµå­—"""
    pass

def transform_case(text: str, mode: str) -> str:
    """è½‰æ›æ–‡å­—å¤§å°å¯«"""
    pass
```

#### æ­¥é©Ÿ 1.3: è³‡æ–™çµæ§‹è¦åŠƒ
```python
# æ–‡å­—çµ±è¨ˆçµæœ
stats_result = {
    'total_chars': 1250,
    'total_words': 200,
    'total_lines': 15,
    'word_freq': {'python': 15, 'function': 8}
}

# æœå°‹çµæœ
search_result = {
    'keyword': 'python',
    'count': 5,
    'positions': [12, 45, 89, 123, 167],
    'lines': [1, 3, 7, 9, 12]
}
```

**âœ… æª¢æŸ¥é»**:
- [ ] å®Œæˆæ¨¡çµ„æ¶æ§‹è¨­è¨ˆåœ–
- [ ] å®šç¾©æ‰€æœ‰æ ¸å¿ƒå‡½å¼ç°½å
- [ ] è¨­è¨ˆçµ±ä¸€çš„å›å‚³è³‡æ–™çµæ§‹

---

### Phase 2: æ ¸å¿ƒåŠŸèƒ½å¯¦ä½œï¼ˆ180 åˆ†é˜ï¼‰

#### æ­¥é©Ÿ 2.1: æ–‡å­—çµ±è¨ˆæ¨¡çµ„ï¼ˆ45 åˆ†é˜ï¼‰
```python
def count_words(text):
    """çµ±è¨ˆæ–‡å­—ä¸­çš„è©å½™æ•¸é‡ - Ch12 å‡½å¼åŸºç¤æ‡‰ç”¨"""
    # ä½¿ç”¨å­—ä¸²æ–¹æ³•èˆ‡å­—å…¸
    words = text.lower().split()
    word_count = {}

    for word in words:
        # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼ˆç¤ºç¯„å­—ä¸²è™•ç†ï¼‰
        clean_word = ''.join(c for c in word if c.isalnum())
        if clean_word:
            word_count[clean_word] = word_count.get(clean_word, 0) + 1

    return word_count

def analyze_text_statistics(text):
    """ç¶œåˆæ–‡å­—çµ±è¨ˆ - å±•ç¤ºå‡½å¼çµ„åˆ"""
    return {
        'char_count': len(text),
        'word_count': len(text.split()),
        'line_count': len(text.splitlines()),
        'word_frequency': count_words(text)
    }
```

#### æ­¥é©Ÿ 2.2: é«˜éšå‡½å¼æ‡‰ç”¨ï¼ˆ45 åˆ†é˜ï¼‰- Ch14 é‡é»
```python
def apply_text_filters(text_list, *filters):
    """ä½¿ç”¨é«˜éšå‡½å¼è™•ç†æ–‡å­—åˆ—è¡¨"""
    result = text_list

    # ä¾åºå¥—ç”¨æ‰€æœ‰éæ¿¾å™¨
    for filter_func in filters:
        result = list(filter(filter_func, result))

    return result

def transform_text_batch(text_list, transform_func):
    """æ‰¹æ¬¡è½‰æ›æ–‡å­— - map å‡½å¼æ‡‰ç”¨"""
    return list(map(transform_func, text_list))

# Lambda å‡½å¼ç¤ºç¯„
def create_text_filters():
    """å»ºç«‹å¸¸ç”¨éæ¿¾å™¨ - Lambda æ‡‰ç”¨"""
    return {
        'non_empty': lambda text: text.strip() != '',
        'min_length': lambda min_len: lambda text: len(text) >= min_len,
        'contains_keyword': lambda keyword: lambda text: keyword in text.lower()
    }
```

#### æ­¥é©Ÿ 2.3: ä½œç”¨åŸŸèˆ‡å…§åµŒå‡½å¼ï¼ˆ45 åˆ†é˜ï¼‰- Ch13 é‡é»
```python
def create_text_processor(config):
    """ä½¿ç”¨é–‰åŒ…å»ºç«‹å®¢è£½åŒ–æ–‡å­—è™•ç†å™¨"""
    # å¤–å±¤å‡½å¼è®Šæ•¸ï¼ˆé–‰åŒ…ç’°å¢ƒï¼‰
    separator = config.get('separator', ' ')
    case_mode = config.get('case', 'none')

    def process_line(line):
        """å…§å±¤å‡½å¼ - å…·æœ‰å¤–å±¤ä½œç”¨åŸŸå­˜å–æ¬Š"""
        # ä½¿ç”¨å¤–å±¤è®Šæ•¸
        words = line.split(separator)

        if case_mode == 'upper':
            words = [word.upper() for word in words]
        elif case_mode == 'lower':
            words = [word.lower() for word in words]

        return separator.join(words)

    def process_text(text):
        """è™•ç†æ•´å€‹æ–‡å­—"""
        lines = text.splitlines()
        processed_lines = [process_line(line) for line in lines]
        return '\n'.join(processed_lines)

    # è¿”å›å…§å±¤å‡½å¼ï¼ˆé–‰åŒ…ï¼‰
    return process_text
```

#### æ­¥é©Ÿ 2.4: éè¿´æª”æ¡ˆè™•ç†ï¼ˆ45 åˆ†é˜ï¼‰- Ch15 é‡é»
```python
def search_files_recursive(directory, pattern, file_extension='.txt'):
    """éè¿´æœå°‹ç›®éŒ„ä¸­çš„æ–‡å­—æª”æ¡ˆ"""
    import os

    def _search_directory(current_dir, depth=0):
        """å…§éƒ¨éè¿´å‡½å¼"""
        results = []

        try:
            # ç²å–ç›®éŒ„å…§å®¹
            items = os.listdir(current_dir)

            for item in items:
                item_path = os.path.join(current_dir, item)

                if os.path.isfile(item_path) and item.endswith(file_extension):
                    # è™•ç†æª”æ¡ˆ
                    try:
                        with open(item_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            if pattern in content:
                                results.append({
                                    'file': item_path,
                                    'matches': content.count(pattern),
                                    'depth': depth
                                })
                    except UnicodeDecodeError:
                        # å¿½ç•¥ç„¡æ³•è®€å–çš„æª”æ¡ˆ
                        continue

                elif os.path.isdir(item_path) and depth < 5:  # é™åˆ¶æ·±åº¦
                    # éè¿´è™•ç†å­ç›®éŒ„
                    results.extend(_search_directory(item_path, depth + 1))

        except PermissionError:
            # è™•ç†æ¬Šé™å•é¡Œ
            pass

        return results

    return _search_directory(directory)

def count_text_patterns_recursive(text, pattern):
    """éè¿´è¨ˆç®—æ¨¡å¼å‡ºç¾æ¬¡æ•¸"""
    if not text or not pattern:
        return 0

    if text.startswith(pattern):
        # æ‰¾åˆ°æ¨¡å¼ï¼Œç¹¼çºŒæœå°‹å‰©é¤˜éƒ¨åˆ†
        return 1 + count_text_patterns_recursive(text[len(pattern):], pattern)
    else:
        # æ²’æ‰¾åˆ°ï¼Œç¹¼çºŒæœå°‹ä¸‹ä¸€å€‹å­—å…ƒ
        return count_text_patterns_recursive(text[1:], pattern)
```

**âœ… æª¢æŸ¥é»**:
- [ ] æ–‡å­—çµ±è¨ˆåŠŸèƒ½æ­£å¸¸é‹ä½œ
- [ ] é«˜éšå‡½å¼æ­£ç¢ºæ‡‰ç”¨ map/filter
- [ ] é–‰åŒ…èˆ‡ä½œç”¨åŸŸé‹ç”¨å¾—ç•¶
- [ ] éè¿´å‡½å¼é‹ä½œæ­£ç¢º

---

### Phase 3: æ•´åˆèˆ‡å„ªåŒ–ï¼ˆ120 åˆ†é˜ï¼‰

#### æ­¥é©Ÿ 3.1: å»ºç«‹ä¸»ä»‹é¢ï¼ˆ40 åˆ†é˜ï¼‰
```python
def create_text_toolkit():
    """å»ºç«‹æ–‡å­—è™•ç†å·¥å…·ç®±ä¸»ä»‹é¢"""

    # ä½¿ç”¨å­—å…¸å„²å­˜æ‰€æœ‰åŠŸèƒ½ï¼ˆæ¨¡çµ„åŒ–è¨­è¨ˆï¼‰
    toolkit = {
        'statistics': {
            'word_count': count_words,
            'text_stats': analyze_text_statistics,
            'frequency_analysis': word_frequency_analysis
        },
        'search': {
            'find_pattern': find_text_pattern,
            'search_lines': search_in_lines,
            'file_search': search_files_recursive
        },
        'transform': {
            'case_convert': convert_case,
            'replace_text': replace_text_advanced,
            'normalize': normalize_text
        },
        'format': {
            'align': align_text,
            'indent': indent_lines,
            'columnize': create_columns
        }
    }

    return toolkit

def interactive_menu():
    """äº’å‹•å¼é¸å–®ç³»çµ±"""
    toolkit = create_text_toolkit()

    while True:
        print("\n" + "="*50)
        print("ğŸ”§ æ–‡å­—è™•ç†å·¥å…·ç®± v1.0")
        print("="*50)
        print("1. æ–‡å­—çµ±è¨ˆåˆ†æ")
        print("2. æ–‡å­—æœå°‹åŠŸèƒ½")
        print("3. æ–‡å­—è½‰æ›å·¥å…·")
        print("4. æ–‡å­—æ ¼å¼åŒ–")
        print("5. éè¿´æª”æ¡ˆè™•ç†")
        print("0. çµæŸç¨‹å¼")

        choice = input("\nè«‹é¸æ“‡åŠŸèƒ½ (0-5): ")

        if choice == "0":
            print("æ„Ÿè¬ä½¿ç”¨ï¼")
            break
        elif choice in ["1", "2", "3", "4", "5"]:
            handle_menu_choice(choice, toolkit)
        else:
            print("ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥ï¼")
```

#### æ­¥é©Ÿ 3.2: æ•ˆèƒ½å„ªåŒ–ï¼ˆ40 åˆ†é˜ï¼‰
```python
def optimize_large_file_processing(file_path, chunk_size=8192):
    """å¤§æª”æ¡ˆåˆ†å¡Šè™•ç† - è¨˜æ†¶é«”å„ªåŒ–"""
    def process_chunks(processor_func):
        """ä½¿ç”¨ç”Ÿæˆå™¨è™•ç†å¤§æª”æ¡ˆ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                yield processor_func(chunk)

    return process_chunks

# ä½¿ç”¨è£é£¾å™¨å¯¦ä½œå¿«å–åŠŸèƒ½
def memoize(func):
    """è¨˜æ†¶åŒ–è£é£¾å™¨ - é¿å…é‡è¤‡è¨ˆç®—"""
    cache = {}

    def wrapper(*args):
        if args in cache:
            return cache[args]
        result = func(*args)
        cache[args] = result
        return result

    return wrapper

@memoize
def expensive_text_analysis(text):
    """æ˜‚è²´çš„æ–‡å­—åˆ†ææ“ä½œ"""
    # æ¨¡æ“¬è¤‡é›œè¨ˆç®—
    return complex_analysis(text)
```

#### æ­¥é©Ÿ 3.3: æ¸¬è©¦èˆ‡é©—è­‰ï¼ˆ40 åˆ†é˜ï¼‰
```python
def test_text_toolkit():
    """å®Œæ•´åŠŸèƒ½æ¸¬è©¦"""
    print("ğŸ§ª é–‹å§‹æ¸¬è©¦æ–‡å­—è™•ç†å·¥å…·ç®±...")

    # æ¸¬è©¦è³‡æ–™
    test_text = """
    Python æ˜¯ä¸€ç¨®é«˜éšç¨‹å¼èªè¨€ã€‚
    Python æ˜“æ–¼å­¸ç¿’ä¸”åŠŸèƒ½å¼·å¤§ã€‚
    è¨±å¤šå…¬å¸éƒ½ä½¿ç”¨ Python é–‹ç™¼è»Ÿé«”ã€‚
    """

    # æ¸¬è©¦ 1: æ–‡å­—çµ±è¨ˆ
    stats = analyze_text_statistics(test_text)
    assert stats['word_count'] > 0, "è©å½™çµ±è¨ˆå¤±æ•—"
    print("âœ… æ–‡å­—çµ±è¨ˆæ¸¬è©¦é€šé")

    # æ¸¬è©¦ 2: æœå°‹åŠŸèƒ½
    search_result = find_text_pattern(test_text, "Python")
    assert search_result['count'] == 3, "æœå°‹åŠŸèƒ½å¤±æ•—"
    print("âœ… æœå°‹åŠŸèƒ½æ¸¬è©¦é€šé")

    # æ¸¬è©¦ 3: é«˜éšå‡½å¼
    lines = test_text.strip().split('\n')
    filtered = apply_text_filters(lines, lambda x: 'Python' in x)
    assert len(filtered) == 3, "é«˜éšå‡½å¼æ¸¬è©¦å¤±æ•—"
    print("âœ… é«˜éšå‡½å¼æ¸¬è©¦é€šé")

    # æ¸¬è©¦ 4: éè¿´åŠŸèƒ½
    recursive_count = count_text_patterns_recursive(test_text, "Python")
    assert recursive_count == 3, "éè¿´åŠŸèƒ½å¤±æ•—"
    print("âœ… éè¿´åŠŸèƒ½æ¸¬è©¦é€šé")

    print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼")

if __name__ == "__main__":
    test_text_toolkit()
    interactive_menu()
```

**âœ… æª¢æŸ¥é»**:
- [ ] ä¸»ä»‹é¢æµæš¢é‹ä½œ
- [ ] æ‰€æœ‰æ¨¡çµ„æ­£ç¢ºæ•´åˆ
- [ ] æ•ˆèƒ½å„ªåŒ–å¯¦ä½œå®Œæˆ
- [ ] æ¸¬è©¦æ¡ˆä¾‹å…¨éƒ¨é€šé

---

## âœ… è©•åˆ†æ¨™æº–ï¼ˆGrading Rubricï¼‰

### åŸºæœ¬åŠŸèƒ½å¯¦ä½œ (70%)

| é …ç›® | å„ªç§€ (90-100) | è‰¯å¥½ (75-89) | åŠæ ¼ (60-74) | ä¸åŠæ ¼ (<60) |
|:-----|:-------------|:------------|:------------|:------------|
| **æ–‡å­—çµ±è¨ˆæ¨¡çµ„** (15%) | å®Œæ•´å¯¦ä½œè©é »ã€å­—æ•¸ã€è¡Œæ•¸çµ±è¨ˆï¼Œæœ‰éŒ¯èª¤è™•ç† | åŸºæœ¬çµ±è¨ˆåŠŸèƒ½å®Œæ•´ï¼Œéƒ¨åˆ†éŒ¯èª¤è™•ç† | åŸºæœ¬çµ±è¨ˆåŠŸèƒ½æ­£å¸¸ | çµ±è¨ˆåŠŸèƒ½ä¸å®Œæ•´æˆ–æœ‰éŒ¯èª¤ |
| **æ–‡å­—æœå°‹æ¨¡çµ„** (15%) | æ”¯æ´å¤šç¨®æœå°‹æ¨¡å¼ï¼Œçµæœè©³ç´° | åŸºæœ¬æœå°‹åŠŸèƒ½å®Œæ•´ | ç°¡å–®é—œéµå­—æœå°‹ | æœå°‹åŠŸèƒ½ä¸æ­£å¸¸ |
| **æ–‡å­—è½‰æ›æ¨¡çµ„** (15%) | å¤šç¨®è½‰æ›æ–¹å¼ï¼Œæ”¯æ´æ‰¹æ¬¡è™•ç† | åŸºæœ¬è½‰æ›åŠŸèƒ½å®Œæ•´ | ç°¡å–®å¤§å°å¯«è½‰æ› | è½‰æ›åŠŸèƒ½æœ‰å•é¡Œ |
| **æ ¼å¼åŒ–æ¨¡çµ„** (10%) | å°é½Šã€ç¸®æ’ã€åˆ†æ¬„åŠŸèƒ½å®Œå–„ | åŸºæœ¬æ ¼å¼åŒ–åŠŸèƒ½ | ç°¡å–®æ ¼å¼åŒ– | æ ¼å¼åŒ–ä¸æ­£å¸¸ |
| **åˆ†ææ¨¡çµ„** (15%) | å¥å­åˆ†æã€é‡è¤‡æª¢æ¸¬å®Œæ•´ | åŸºæœ¬åˆ†æåŠŸèƒ½ | ç°¡å–®æ–‡å­—åˆ†æ | åˆ†æåŠŸèƒ½ä¸è¶³ |

### é€²éšæŠ€è¡“æ‡‰ç”¨ (20%)

| é …ç›® | å„ªç§€ (90-100) | è‰¯å¥½ (75-89) | åŠæ ¼ (60-74) | ä¸åŠæ ¼ (<60) |
|:-----|:-------------|:------------|:------------|:------------|
| **é«˜éšå‡½å¼é‹ç”¨** (5%) | ç†Ÿç·´ä½¿ç”¨ map/filter/reduce, Lambda | æ­£ç¢ºä½¿ç”¨é«˜éšå‡½å¼ | åŸºæœ¬ä½¿ç”¨ map/filter | ä¸æœƒä½¿ç”¨é«˜éšå‡½å¼ |
| **ä½œç”¨åŸŸèˆ‡é–‰åŒ…** (5%) | æ­£ç¢ºé‹ç”¨é–‰åŒ…ï¼Œç†è§£ä½œç”¨åŸŸ | åŸºæœ¬ä½¿ç”¨å…§åµŒå‡½å¼ | ç°¡å–®å…§åµŒå‡½å¼ | ä¸ç†è§£ä½œç”¨åŸŸ |
| **éè¿´æ‡‰ç”¨** (5%) | è¤‡é›œéè¿´è™•ç†ï¼Œæœ‰çµ‚æ­¢æ¢ä»¶ | åŸºæœ¬éè¿´åŠŸèƒ½ | ç°¡å–®éè¿´ | éè¿´é‚è¼¯éŒ¯èª¤ |
| **å‡½å¼è¨­è¨ˆ** (5%) | æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œä»‹é¢æ¸…æ™° | å‡½å¼åŒ–ç¨‹å¼è¨­è¨ˆ | åŸºæœ¬å‡½å¼åˆ†é›¢ | ç¨‹å¼çµæ§‹æ··äº‚ |

### ç¨‹å¼å“è³ª (10%)

| é …ç›® | å„ªç§€ (90-100) | è‰¯å¥½ (75-89) | åŠæ ¼ (60-74) | ä¸åŠæ ¼ (<60) |
|:-----|:-------------|:------------|:------------|:------------|
| **ç¨‹å¼æ¶æ§‹** (3%) | æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œçµæ§‹æ¸…æ™° | åŸºæœ¬æ¨¡çµ„åŒ– | å‡½å¼åˆ†é›¢ | çµæ§‹æ··äº‚ |
| **æ–‡ä»¶è¨»è§£** (3%) | å®Œæ•´ docstringï¼Œè¨»è§£æ¸…æ¥š | ä¸»è¦å‡½å¼æœ‰æ–‡ä»¶ | åŸºæœ¬è¨»è§£ | ç¼ºä¹è¨»è§£ |
| **éŒ¯èª¤è™•ç†** (2%) | å®Œå–„çš„ä¾‹å¤–è™•ç† | åŸºæœ¬éŒ¯èª¤è™•ç† | ç°¡å–®é©—è­‰ | ç„¡éŒ¯èª¤è™•ç† |
| **ç¨‹å¼é¢¨æ ¼** (2%) | éµå¾ª PEP 8ï¼Œå‘½åè¦ç¯„ | åŸºæœ¬ç¨‹å¼é¢¨æ ¼ | å¯è®€æ€§è‰¯å¥½ | é¢¨æ ¼æ··äº‚ |

---

## ğŸ“ æ•™å¸«æŒ‡å¼•ï¼ˆInstructor Notesï¼‰

### è©•åˆ†é‡é»

#### 1. Ch12-15 æ¦‚å¿µæ•´åˆåº¦ (é‡è¦æŒ‡æ¨™)
- âœ… **å„ªç§€**: æ¸…æ¥šå±•ç¤ºæ¯ç« ç¯€æ ¸å¿ƒæ¦‚å¿µçš„æ‡‰ç”¨
  - Ch12: å‡½å¼è¨­è¨ˆåŸå‰‡ã€åƒæ•¸è™•ç†ã€æ¨¡çµ„åŒ–
  - Ch13: ä½œç”¨åŸŸé‹ç”¨ã€å…§åµŒå‡½å¼ã€é–‰åŒ…
  - Ch14: map/filter/reduceã€Lambdaã€å‡½å¼çµ„åˆ
  - Ch15: éè¿´æ€ç¶­ã€åˆ†æ²»æ³•ã€çµ‚æ­¢æ¢ä»¶
- â­ **è‰¯å¥½**: éƒ¨åˆ†æ¦‚å¿µé‹ç”¨å¾—ç•¶ï¼Œä½†ä¸å¤ å…¨é¢
- âŒ **ä¸ä½³**: åªä½¿ç”¨åŸºæœ¬åŠŸèƒ½ï¼Œæ²’æœ‰å±•ç¤ºé€²éšæ¦‚å¿µ

#### 2. å‡½å¼è¨­è¨ˆå“è³ª (é—œéµè©•åˆ†é»)
- âœ… **å„ªç§€**: å–®ä¸€è·è²¬ã€ä»‹é¢æ¸…æ™°ã€å¯é‡ç”¨æ€§é«˜
- â­ **è‰¯å¥½**: åŸºæœ¬å‡½å¼åŒ–è¨­è¨ˆ
- âŒ **ä¸ä½³**: å‡½å¼éæ–¼è¤‡é›œæˆ–è·è²¬ä¸æ¸…

#### 3. é«˜éšå‡½å¼é‹ç”¨ (Ch14 é‡é»)
- âœ… **å„ªç§€**:
  ```python
  # å‡½å¼çµ„åˆç¤ºä¾‹
  def process_pipeline(text, *operations):
      return reduce(lambda result, op: op(result), operations, text)

  # ä½¿ç”¨ç¤ºä¾‹
  result = process_pipeline(text,
                          lambda x: x.lower(),
                          lambda x: filter_words(x),
                          lambda x: format_output(x))
  ```
- â­ **è‰¯å¥½**: åŸºæœ¬ä½¿ç”¨ map/filter
- âŒ **ä¸ä½³**: æ²’æœ‰ä½¿ç”¨é«˜éšå‡½å¼

#### 4. éè¿´å¯¦ä½œ (Ch15 é‡é»)
- âœ… **å„ªç§€**: æ¸…æ¥šçš„éè¿´çµæ§‹ï¼Œæ­£ç¢ºçš„çµ‚æ­¢æ¢ä»¶ï¼Œå¯¦éš›æ‡‰ç”¨å ´æ™¯
- â­ **è‰¯å¥½**: åŸºæœ¬éè¿´åŠŸèƒ½æ­£ç¢º
- âŒ **ä¸ä½³**: éè¿´é‚è¼¯éŒ¯èª¤æˆ–æ²’æœ‰çµ‚æ­¢æ¢ä»¶

### å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

#### å•é¡Œ 1: æ²’æœ‰å±•ç¤º Ch13 ä½œç”¨åŸŸæ¦‚å¿µ
**å­¸ç”Ÿè¡¨ç¾**: æ‰€æœ‰å‡½å¼éƒ½åœ¨å…¨åŸŸç¯„åœï¼Œæ²’æœ‰ä½¿ç”¨å…§åµŒå‡½å¼æˆ–é–‰åŒ…
**è§£æ±ºæ–¹æ¡ˆ**: å¼•å°å­¸ç”Ÿè¨­è¨ˆé…ç½®åŒ–çš„æ–‡å­—è™•ç†å™¨
```python
def create_text_processor(config):
    encoding = config.get('encoding', 'utf-8')

    def process_file(filename):  # å…§åµŒå‡½å¼å­˜å–å¤–å±¤è®Šæ•¸
        with open(filename, encoding=encoding) as f:
            return f.read()

    return process_file  # å›å‚³é–‰åŒ…
```

#### å•é¡Œ 2: é«˜éšå‡½å¼ä½¿ç”¨ä¸ç•¶
**å­¸ç”Ÿè¡¨ç¾**: ä½¿ç”¨å‚³çµ±è¿´åœˆè€Œé map/filter
```python
# ä¸ä½³å¯«æ³•
result = []
for item in items:
    if condition(item):
        result.append(transform(item))
```
**å»ºè­°æ”¹é€²**:
```python
# å„ªç§€å¯«æ³•
result = list(map(transform, filter(condition, items)))
# æˆ–ä½¿ç”¨æ¨å°å¼
result = [transform(item) for item in items if condition(item)]
```

#### å•é¡Œ 3: éè¿´æ²’æœ‰å¯¦éš›æ‡‰ç”¨å ´æ™¯
**å­¸ç”Ÿè¡¨ç¾**: ç‚ºäº†éè¿´è€Œéè¿´ï¼Œå¦‚éè¿´è¨ˆç®—å­—ä¸²é•·åº¦
**å»ºè­°å ´æ™¯**:
- ç›®éŒ„æ¨¹éè¿´æœå°‹
- å·¢ç‹€è³‡æ–™çµæ§‹è™•ç†
- æ–‡å­—è§£ææ¨¹èµ°è¨ª

#### å•é¡Œ 4: å‡½å¼è¨­è¨ˆç¼ºä¹æ¨¡çµ„åŒ–æ€ç¶­
**å­¸ç”Ÿè¡¨ç¾**: ä¸€å€‹å‡½å¼åšå¤ªå¤šäº‹æƒ…
**å»ºè­°åŸå‰‡**:
- å–®ä¸€è·è²¬åŸå‰‡ (Single Responsibility)
- å‡½å¼é•·åº¦æ§åˆ¶åœ¨ 20-30 è¡Œå…§
- æ¸…æ¥šçš„è¼¸å…¥è¼¸å‡ºè¦æ ¼
- é¿å…å‰¯ä½œç”¨ (side effects)

### æ•™å­¸æ™‚é–“åˆ†é…å»ºè­°

| éšæ®µ | æ™‚é–“ | å…§å®¹ | é‡é» |
|:-----|:-----|:-----|:-----|
| **è¬›è§£ç¤ºç¯„** | 2 å°æ™‚ | Ch12-15 æ¦‚å¿µå›é¡§ï¼Œå°ˆæ¡ˆæ¶æ§‹è¨­è¨ˆ | å¼·èª¿æ¦‚å¿µæ•´åˆ |
| **Phase 1 æŒ‡å°** | 1 å°æ™‚ | æ¨¡çµ„è¨­è¨ˆèˆ‡å‡½å¼ä»‹é¢è¦åŠƒ | è¨­è¨ˆæ€ç¶­åŸ¹é¤Š |
| **Phase 2 å¯¦ä½œ** | 4 å°æ™‚ | æ ¸å¿ƒåŠŸèƒ½é–‹ç™¼ï¼ŒæŠ€è¡“æ¦‚å¿µæ‡‰ç”¨ | å€‹åˆ¥æŒ‡å°é‡é» |
| **Phase 3 æ•´åˆ** | 2 å°æ™‚ | ç³»çµ±æ•´åˆèˆ‡æ¸¬è©¦ | å“è³ªæ§åˆ¶ |
| **ç¨‹å¼ç¢¼å¯©æŸ¥** | 1 å°æ™‚ | å­¸ç”Ÿä½œå“åˆ†äº«èˆ‡è¨è«– | æœ€ä½³å¯¦è¸åˆ†äº« |

### å·®ç•°åŒ–æ•™å­¸å»ºè­°

#### å°æ–¼é€²åº¦è¼ƒå¿«çš„å­¸ç”Ÿ:
- æŒ‘æˆ°å»¶ä¼¸åŠŸèƒ½ (æª”æ¡ˆæ‰¹æ¬¡è™•ç†ã€æ­£è¦è¡¨é”å¼)
- æ•ˆèƒ½å„ªåŒ–å¯¦ä½œ (ç”Ÿæˆå™¨ã€è¨˜æ†¶åŒ–)
- è¨­è¨ˆæ¨¡å¼æ‡‰ç”¨ (ç­–ç•¥æ¨¡å¼ã€å·¥å» æ¨¡å¼)

#### å°æ–¼éœ€è¦é¡å¤–å”åŠ©çš„å­¸ç”Ÿ:
- æä¾›æ›´è©³ç´°çš„å‡½å¼ç¯„ä¾‹
- é‡é»è¼”å° Ch14-15 çš„æ¦‚å¿µç†è§£
- ç°¡åŒ–éœ€æ±‚ï¼Œå…ˆå®ŒæˆåŸºæœ¬åŠŸèƒ½

---

## ğŸš€ å»¶ä¼¸æŒ‘æˆ°ï¼ˆExtension Challengesï¼‰

### æŒ‘æˆ° 1: æ­£è¦è¡¨é”å¼æ•´åˆ â­â­â­
æ•´åˆ Python `re` æ¨¡çµ„ï¼Œæä¾›é€²éšæ–‡å­—è™•ç†ï¼š
```python
import re

def advanced_pattern_search(text, pattern, flags=0):
    """ä½¿ç”¨æ­£è¦è¡¨é”å¼é€²è¡Œé€²éšæœå°‹"""
    matches = re.finditer(pattern, text, flags)
    return [{
        'match': match.group(),
        'start': match.start(),
        'end': match.end(),
        'groups': match.groups()
    } for match in matches]

def extract_email_addresses(text):
    """æå–æ–‡å­—ä¸­çš„é›»å­éƒµä»¶åœ°å€"""
    pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    return advanced_pattern_search(text, pattern, re.IGNORECASE)
```

### æŒ‘æˆ° 2: æ–‡å­—å£“ç¸®æ¼”ç®—æ³• â­â­â­â­
å¯¦ä½œ Run-Length Encoding (RLE) å£“ç¸®ï¼š
```python
def rle_compress(text):
    """Run-Length Encoding å£“ç¸®"""
    if not text:
        return ""

    result = []
    current_char = text[0]
    count = 1

    for char in text[1:]:
        if char == current_char:
            count += 1
        else:
            result.append(f"{count}{current_char}")
            current_char = char
            count = 1

    result.append(f"{count}{current_char}")
    return ''.join(result)

def rle_decompress(compressed):
    """RLE è§£å£“ç¸®"""
    result = []
    i = 0

    while i < len(compressed):
        # è®€å–æ•¸å­—
        count = ""
        while i < len(compressed) and compressed[i].isdigit():
            count += compressed[i]
            i += 1

        # è®€å–å­—å…ƒ
        if i < len(compressed):
            char = compressed[i]
            result.append(char * int(count))
            i += 1

    return ''.join(result)
```

### æŒ‘æˆ° 3: å¹³è¡Œè™•ç†å„ªåŒ– â­â­â­â­â­
ä½¿ç”¨ `concurrent.futures` é€²è¡Œå¤šåŸ·è¡Œç·’æ–‡å­—è™•ç†ï¼š
```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time

def parallel_file_processing(file_list, processor_func, max_workers=4):
    """å¹³è¡Œè™•ç†å¤šå€‹æª”æ¡ˆ"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        futures = {executor.submit(processor_func, file): file
                  for file in file_list}

        # æ”¶é›†çµæœ
        results = {}
        for future in futures:
            file = futures[future]
            try:
                results[file] = future.result()
            except Exception as e:
                results[file] = f"Error: {e}"

        return results

def benchmark_performance(func, *args, iterations=10):
    """æ•ˆèƒ½æ¸¬è©¦è£é£¾å™¨"""
    def wrapper():
        times = []
        for _ in range(iterations):
            start = time.time()
            result = func(*args)
            end = time.time()
            times.append(end - start)

        avg_time = sum(times) / len(times)
        return {
            'result': result,
            'avg_time': avg_time,
            'times': times
        }
    return wrapper
```

### æŒ‘æˆ° 4: æ©Ÿå™¨å­¸ç¿’æ–‡å­—åˆ†æ â­â­â­â­â­
æ•´åˆç°¡å–®çš„ NLP åŠŸèƒ½ï¼š
```python
def sentiment_analysis_simple(text):
    """ç°¡æ˜“æƒ…æ„Ÿåˆ†æ"""
    positive_words = {'å¥½', 'æ£’', 'å„ªç§€', 'è®š', 'å–œæ­¡', 'é–‹å¿ƒ'}
    negative_words = {'å£', 'ç³Ÿ', 'è¨å­', 'é›£é', 'ç”Ÿæ°£', 'å¤±æœ›'}

    words = set(text.lower().split())

    positive_score = len(words & positive_words)
    negative_score = len(words & negative_words)

    if positive_score > negative_score:
        return 'positive'
    elif negative_score > positive_score:
        return 'negative'
    else:
        return 'neutral'

def text_similarity(text1, text2):
    """è¨ˆç®—æ–‡å­—ç›¸ä¼¼åº¦ï¼ˆç°¡æ˜“ç‰ˆæœ¬ï¼‰"""
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())

    intersection = words1 & words2
    union = words1 | words2

    # Jaccard ç›¸ä¼¼åº¦
    return len(intersection) / len(union) if union else 0
```

### æŒ‘æˆ° 5: Web API æ•´åˆ â­â­â­â­
å»ºç«‹ RESTful API ä»‹é¢ï¼š
```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/api/analyze', methods=['POST'])
def analyze_text():
    """æ–‡å­—åˆ†æ API ç«¯é»"""
    data = request.get_json()
    text = data.get('text', '')

    if not text:
        return jsonify({'error': 'No text provided'}), 400

    # ä½¿ç”¨å·¥å…·ç®±é€²è¡Œåˆ†æ
    toolkit = create_text_toolkit()
    results = {
        'statistics': toolkit['statistics']['text_stats'](text),
        'sentiment': sentiment_analysis_simple(text),
        'word_count': len(text.split())
    }

    return jsonify(results)

if __name__ == '__main__':
    app.run(debug=True)
```

---

**å°ˆæ¡ˆé›£åº¦**ï¼šâ­â­â­â­ (ä¸­é«˜ç´š)
**é è¨ˆå®Œæˆæ™‚é–“**ï¼š20-30 å°æ™‚
**å»ºè­°å®ŒæˆæœŸé™**ï¼š2 é€±
**å…ˆå‚™çŸ¥è­˜**ï¼šå®Œæˆ Ch12-15 æ‰€æœ‰å…§å®¹
**å»¶ä¼¸å­¸ç¿’**ï¼šCh23 æª”æ¡ˆè™•ç†ã€Ch27 æ¨¡çµ„è¨­è¨ˆ
