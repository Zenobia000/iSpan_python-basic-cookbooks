{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 04: æ–‡å­—è™•ç†å·¥å…·ç®± - èµ·å§‹ç¨‹å¼ç¢¼\n",
    "\n",
    "## ğŸ¯ é–‹ç™¼æŒ‡å—\n",
    "\n",
    "æœ¬æª”æ¡ˆæä¾›å®Œæ•´çš„ç¨‹å¼ç¢¼æ¡†æ¶ï¼ŒåŒ…å«ï¼š\n",
    "- æ‰€æœ‰å‡½å¼çš„å‡½å¼ç°½åèˆ‡æ–‡ä»¶å­—ä¸²\n",
    "- å¯¦ä½œæç¤ºèˆ‡ TODO æ¨™è¨˜\n",
    "- æ¸¬è©¦ç¨‹å¼ç¢¼ç¯„ä¾‹\n",
    "- æ®µéšå¼é–‹ç™¼å»ºè­°\n",
    "\n",
    "### ğŸ“‹ é–‹ç™¼æª¢æ ¸è¡¨\n",
    "å®Œæˆä¸€å€‹æ¨¡çµ„å¾Œï¼Œè«‹å‹¾é¸å°æ‡‰é …ç›®ï¼š\n",
    "- [ ] æ¨¡çµ„ 1: æ–‡å­—çµ±è¨ˆ (statistics)\n",
    "- [ ] æ¨¡çµ„ 2: æ–‡å­—æœå°‹ (search)\n",
    "- [ ] æ¨¡çµ„ 3: æ–‡å­—è½‰æ› (transform)\n",
    "- [ ] æ¨¡çµ„ 4: æ–‡å­—æ ¼å¼åŒ– (format)\n",
    "- [ ] æ¨¡çµ„ 5: æ–‡å­—åˆ†æ (analysis)\n",
    "- [ ] æ¨¡çµ„ 6: éè¿´è™•ç† (recursive) [é€²éš]\n",
    "- [ ] æ¨¡çµ„ 7: å‡½å¼å¼ç¨‹å¼è¨­è¨ˆ (functional) [é€²éš]\n",
    "- [ ] ç³»çµ±æ•´åˆèˆ‡æ¸¬è©¦\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ æ¨¡çµ„ 1: æ–‡å­—çµ±è¨ˆæ¨¡çµ„ (Statistics)\n",
    "\n",
    "### ğŸ’¡ Ch12 é‡é»ï¼šå‡½å¼è¨­è¨ˆåŸºç¤\n",
    "- å–®ä¸€è·è²¬åŸå‰‡ï¼šæ¯å€‹å‡½å¼åªåšä¸€ä»¶äº‹\n",
    "- æ¸…æ¥šçš„åƒæ•¸èˆ‡å›å‚³å€¼\n",
    "- å®Œæ•´çš„æ–‡ä»¶å­—ä¸²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_characters(text):\n",
    "    \"\"\"\n",
    "    çµ±è¨ˆæ–‡å­—å­—å…ƒæ•¸é‡ï¼ˆå«ç©ºæ ¼èˆ‡æ¨™é»ï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦çµ±è¨ˆçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: åŒ…å«å„ç¨®å­—å…ƒçµ±è¨ˆè³‡è¨Š\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œå­—å…ƒçµ±è¨ˆé‚è¼¯\n",
    "    # æç¤ºï¼šä½¿ç”¨ str.isalpha(), str.isdigit(), str.isspace()\n",
    "    \n",
    "    if not text:  # é‚Šç•Œæ¢ä»¶è™•ç†\n",
    "        return {\n",
    "            'total_chars': 0,\n",
    "            'alphabetic': 0,\n",
    "            'numeric': 0,\n",
    "            'whitespace': 0,\n",
    "            'punctuation': 0\n",
    "        }\n",
    "    \n",
    "    # TODO: åˆå§‹åŒ–è¨ˆæ•¸å™¨\n",
    "    total_chars = len(text)\n",
    "    alphabetic = 0\n",
    "    numeric = 0\n",
    "    whitespace = 0\n",
    "    \n",
    "    # TODO: è¿´åœˆçµ±è¨ˆæ¯å€‹å­—å…ƒ\n",
    "    for char in text:\n",
    "        # TODO: åˆ¤æ–·å­—å…ƒé¡å‹ä¸¦ç´¯è¨ˆ\n",
    "        pass\n",
    "    \n",
    "    # TODO: è¨ˆç®—æ¨™é»ç¬¦è™Ÿæ•¸é‡\n",
    "    punctuation = 0  # total_chars - alphabetic - numeric - whitespace\n",
    "    \n",
    "    return {\n",
    "        'total_chars': total_chars,\n",
    "        'alphabetic': alphabetic,\n",
    "        'numeric': numeric,\n",
    "        'whitespace': whitespace,\n",
    "        'punctuation': punctuation\n",
    "    }\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    çµ±è¨ˆè©å½™æ•¸é‡èˆ‡è©é »\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: è©å½™çµ±è¨ˆçµæœ\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œè©å½™çµ±è¨ˆé‚è¼¯\n",
    "    # æç¤ºï¼šä½¿ç”¨ str.split(), str.lower(), dict.get()\n",
    "    \n",
    "    if not text.strip():  # é‚Šç•Œæ¢ä»¶\n",
    "        return {\n",
    "            'total_words': 0,\n",
    "            'unique_words': 0,\n",
    "            'word_frequency': {},\n",
    "            'average_word_length': 0.0\n",
    "        }\n",
    "    \n",
    "    # TODO: åˆ†å‰²æ–‡å­—ç‚ºè©å½™åˆ—è¡¨\n",
    "    words = []  # text.lower().split()\n",
    "    \n",
    "    # TODO: æ¸…ç†è©å½™ï¼ˆç§»é™¤æ¨™é»ç¬¦è™Ÿï¼‰\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        # TODO: ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼Œåªä¿ç•™å­—æ¯æ•¸å­—\n",
    "        clean_word = ''.join(c for c in word if c.isalnum())\n",
    "        if clean_word:  # éç©ºè©å½™\n",
    "            clean_words.append(clean_word)\n",
    "    \n",
    "    # TODO: è¨ˆç®—è©é »\n",
    "    word_frequency = {}\n",
    "    for word in clean_words:\n",
    "        # TODO: ä½¿ç”¨ dict.get() æ–¹æ³•ç´¯è¨ˆè©é »\n",
    "        pass\n",
    "    \n",
    "    # TODO: è¨ˆç®—å¹³å‡è©é•·\n",
    "    total_word_length = 0  # sum(len(word) for word in clean_words)\n",
    "    average_word_length = 0.0  # total_word_length / len(clean_words) if clean_words else 0\n",
    "    \n",
    "    return {\n",
    "        'total_words': len(clean_words),\n",
    "        'unique_words': len(word_frequency),\n",
    "        'word_frequency': word_frequency,\n",
    "        'average_word_length': average_word_length\n",
    "    }\n",
    "\n",
    "def count_lines(text):\n",
    "    \"\"\"\n",
    "    çµ±è¨ˆè¡Œæ•¸ç›¸é—œè³‡è¨Š\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: è¡Œæ•¸çµ±è¨ˆçµæœ\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œè¡Œæ•¸çµ±è¨ˆé‚è¼¯\n",
    "    # æç¤ºï¼šä½¿ç”¨ str.splitlines(), str.strip()\n",
    "    \n",
    "    if not text:\n",
    "        return {\n",
    "            'total_lines': 0,\n",
    "            'non_empty_lines': 0,\n",
    "            'empty_lines': 0,\n",
    "            'average_line_length': 0.0\n",
    "        }\n",
    "    \n",
    "    # TODO: åˆ†å‰²ç‚ºè¡Œåˆ—è¡¨\n",
    "    lines = []  # text.splitlines()\n",
    "    \n",
    "    # TODO: çµ±è¨ˆéç©ºè¡Œèˆ‡ç©ºè¡Œ\n",
    "    non_empty_lines = 0\n",
    "    total_line_length = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        # TODO: åˆ¤æ–·æ˜¯å¦ç‚ºéç©ºè¡Œ\n",
    "        pass\n",
    "    \n",
    "    empty_lines = len(lines) - non_empty_lines\n",
    "    average_line_length = total_line_length / len(lines) if lines else 0.0\n",
    "    \n",
    "    return {\n",
    "        'total_lines': len(lines),\n",
    "        'non_empty_lines': non_empty_lines,\n",
    "        'empty_lines': empty_lines,\n",
    "        'average_line_length': average_line_length\n",
    "    }\n",
    "\n",
    "def analyze_text_statistics(text):\n",
    "    \"\"\"\n",
    "    ç¶œåˆæ–‡å­—çµ±è¨ˆåˆ†æï¼ˆæ•´åˆä¸Šè¿°æ‰€æœ‰å‡½å¼ï¼‰\n",
    "    å±•ç¤º Ch12 å‡½å¼çµ„åˆæ¦‚å¿µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: å®Œæ•´çµ±è¨ˆå ±å‘Š\n",
    "    \"\"\"\n",
    "    # TODO: å‘¼å«ä¸Šè¿°ä¸‰å€‹å‡½å¼ä¸¦æ•´åˆçµæœ\n",
    "    # é€™å±•ç¤ºäº† Ch12 å‡½å¼çµ„åˆçš„æ¦‚å¿µ\n",
    "    \n",
    "    return {\n",
    "        'characters': count_characters(text),\n",
    "        'words': count_words(text),\n",
    "        'lines': count_lines(text)\n",
    "    }\n",
    "\n",
    "# æ¸¬è©¦çµ±è¨ˆæ¨¡çµ„\n",
    "def test_statistics_module():\n",
    "    \"\"\"æ¸¬è©¦çµ±è¨ˆæ¨¡çµ„çš„åŸºæœ¬åŠŸèƒ½\"\"\"\n",
    "    test_text = \"Hello, World! 123\\nPython is great.\"\n",
    "    \n",
    "    print(\"ğŸ§ª æ¸¬è©¦çµ±è¨ˆæ¨¡çµ„\")\n",
    "    print(\"æ¸¬è©¦æ–‡å­—:\", repr(test_text))\n",
    "    \n",
    "    # æ¸¬è©¦å­—å…ƒçµ±è¨ˆ\n",
    "    char_stats = count_characters(test_text)\n",
    "    print(\"\\nå­—å…ƒçµ±è¨ˆ:\", char_stats)\n",
    "    \n",
    "    # æ¸¬è©¦è©å½™çµ±è¨ˆ\n",
    "    word_stats = count_words(test_text)\n",
    "    print(\"è©å½™çµ±è¨ˆ:\", word_stats)\n",
    "    \n",
    "    # æ¸¬è©¦è¡Œæ•¸çµ±è¨ˆ\n",
    "    line_stats = count_lines(test_text)\n",
    "    print(\"è¡Œæ•¸çµ±è¨ˆ:\", line_stats)\n",
    "    \n",
    "    # æ¸¬è©¦ç¶œåˆçµ±è¨ˆ\n",
    "    full_stats = analyze_text_statistics(test_text)\n",
    "    print(\"\\nå®Œæ•´çµ±è¨ˆ:\", full_stats)\n",
    "\n",
    "# å–æ¶ˆè¨»è§£ä¾†æ¸¬è©¦\n",
    "# test_statistics_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” æ¨¡çµ„ 2: æ–‡å­—æœå°‹æ¨¡çµ„ (Search)\n",
    "\n",
    "### ğŸ’¡ Ch14 é‡é»ï¼šé«˜éšå‡½å¼èˆ‡ Lambda\n",
    "- ä½¿ç”¨ map å‡½å¼è™•ç†å¤šå€‹é—œéµå­—\n",
    "- ä½¿ç”¨ filter å‡½å¼éæ¿¾çµæœ\n",
    "- Lambda å‡½å¼çš„å¯¦éš›æ‡‰ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keyword(text, keyword, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    åœ¨æ–‡å­—ä¸­æœå°‹é—œéµå­—\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æœå°‹çš„æ–‡å­—\n",
    "        keyword (str): é—œéµå­—\n",
    "        case_sensitive (bool): æ˜¯å¦å€åˆ†å¤§å°å¯«\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: æœå°‹çµæœ\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œé—œéµå­—æœå°‹é‚è¼¯\n",
    "    \n",
    "    if not text or not keyword:\n",
    "        return {\n",
    "            'keyword': keyword,\n",
    "            'count': 0,\n",
    "            'positions': [],\n",
    "            'line_numbers': [],\n",
    "            'context_lines': []\n",
    "        }\n",
    "    \n",
    "    # TODO: è™•ç†å¤§å°å¯«æ•æ„Ÿæ€§\n",
    "    search_text = text if case_sensitive else text.lower()\n",
    "    search_keyword = keyword if case_sensitive else keyword.lower()\n",
    "    \n",
    "    # TODO: æ‰¾å‡ºæ‰€æœ‰å‡ºç¾ä½ç½®\n",
    "    positions = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        # TODO: ä½¿ç”¨ str.find() æ–¹æ³•æ‰¾ä¸‹ä¸€å€‹ä½ç½®\n",
    "        pos = search_text.find(search_keyword, start)\n",
    "        if pos == -1:\n",
    "            break\n",
    "        positions.append(pos)\n",
    "        start = pos + 1\n",
    "    \n",
    "    # TODO: æ‰¾å‡ºé—œéµå­—æ‰€åœ¨çš„è¡Œè™Ÿèˆ‡è¡Œå…§å®¹\n",
    "    lines = text.splitlines()\n",
    "    line_numbers = []\n",
    "    context_lines = []\n",
    "    \n",
    "    for i, line in enumerate(lines, 1):\n",
    "        # TODO: æª¢æŸ¥è¡Œä¸­æ˜¯å¦åŒ…å«é—œéµå­—\n",
    "        check_line = line if case_sensitive else line.lower()\n",
    "        if search_keyword in check_line:\n",
    "            line_numbers.append(i)\n",
    "            context_lines.append(line)\n",
    "    \n",
    "    return {\n",
    "        'keyword': keyword,\n",
    "        'count': len(positions),\n",
    "        'positions': positions,\n",
    "        'line_numbers': line_numbers,\n",
    "        'context_lines': context_lines\n",
    "    }\n",
    "\n",
    "def find_multiple_keywords(text, keywords, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    åŒæ™‚æœå°‹å¤šå€‹é—œéµå­—\n",
    "    å±•ç¤º Ch14 map å‡½å¼æ‡‰ç”¨\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æœå°‹çš„æ–‡å­—\n",
    "        keywords (list): é—œéµå­—åˆ—è¡¨\n",
    "        case_sensitive (bool): æ˜¯å¦å€åˆ†å¤§å°å¯«\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: {keyword: search_result} çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # TODO: ä½¿ç”¨ map å‡½å¼å°æ¯å€‹é—œéµå­—é€²è¡Œæœå°‹\n",
    "    # é€™æ˜¯ Ch14 é«˜éšå‡½å¼çš„é‡é»æ‡‰ç”¨\n",
    "    \n",
    "    if not keywords:\n",
    "        return {}\n",
    "    \n",
    "    # TODO: å»ºç«‹æœå°‹å‡½å¼\n",
    "    def search_single_keyword(keyword):\n",
    "        return find_keyword(text, keyword, case_sensitive)\n",
    "    \n",
    "    # TODO: ä½¿ç”¨ map å‡½å¼æ‰¹æ¬¡æœå°‹\n",
    "    search_results = list(map(search_single_keyword, keywords))\n",
    "    \n",
    "    # TODO: å°‡çµæœè½‰æ›ç‚ºå­—å…¸æ ¼å¼\n",
    "    result_dict = {}\n",
    "    for result in search_results:\n",
    "        result_dict[result['keyword']] = result\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def search_in_lines(text, pattern, filter_func=None):\n",
    "    \"\"\"\n",
    "    æŒ‰è¡Œæœå°‹ä¸¦æ”¯æ´è‡ªè¨‚éæ¿¾æ¢ä»¶\n",
    "    å±•ç¤º Ch14 filter å‡½å¼èˆ‡ Lambda æ‡‰ç”¨\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æœå°‹çš„æ–‡å­—\n",
    "        pattern (str): æœå°‹æ¨¡å¼\n",
    "        filter_func (callable): è‡ªè¨‚éæ¿¾å‡½å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: ç¬¦åˆæ¢ä»¶çš„è¡Œåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œæŒ‰è¡Œæœå°‹é‚è¼¯\n",
    "    \n",
    "    if not text or not pattern:\n",
    "        return []\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "    matching_lines = []\n",
    "    \n",
    "    for i, line in enumerate(lines, 1):\n",
    "        # TODO: æª¢æŸ¥è¡Œä¸­æ˜¯å¦åŒ…å«æ¨¡å¼\n",
    "        if pattern.lower() in line.lower():\n",
    "            line_info = {\n",
    "                'line_number': i,\n",
    "                'content': line,\n",
    "                'match_count': line.lower().count(pattern.lower())\n",
    "            }\n",
    "            matching_lines.append(line_info)\n",
    "    \n",
    "    # TODO: å¦‚æœæœ‰éæ¿¾å‡½å¼ï¼Œä½¿ç”¨ filter å‡½å¼éæ¿¾çµæœ\n",
    "    if filter_func:\n",
    "        # é€™å±•ç¤ºäº† Ch14 filter å‡½å¼çš„æ‡‰ç”¨\n",
    "        matching_lines = list(filter(lambda line_info: filter_func(line_info['content']), matching_lines))\n",
    "    \n",
    "    return matching_lines\n",
    "\n",
    "# æ¸¬è©¦æœå°‹æ¨¡çµ„\n",
    "def test_search_module():\n",
    "    \"\"\"æ¸¬è©¦æœå°‹æ¨¡çµ„çš„åŠŸèƒ½\"\"\"\n",
    "    test_text = \"\"\"Python is awesome!\n",
    "I love Python programming.\n",
    "Python makes coding fun.\n",
    "Java is also good.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” æ¸¬è©¦æœå°‹æ¨¡çµ„\")\n",
    "    \n",
    "    # æ¸¬è©¦å–®ä¸€é—œéµå­—æœå°‹\n",
    "    result = find_keyword(test_text, \"Python\")\n",
    "    print(\"\\næœå°‹ 'Python':\", result)\n",
    "    \n",
    "    # æ¸¬è©¦å¤šé—œéµå­—æœå°‹ï¼ˆCh14 map æ‡‰ç”¨ï¼‰\n",
    "    keywords = [\"Python\", \"Java\", \"programming\"]\n",
    "    multi_results = find_multiple_keywords(test_text, keywords)\n",
    "    print(\"\\nå¤šé—œéµå­—æœå°‹:\", multi_results)\n",
    "    \n",
    "    # æ¸¬è©¦è¡Œæœå°‹èˆ‡éæ¿¾ï¼ˆCh14 filter + Lambda æ‡‰ç”¨ï¼‰\n",
    "    line_results = search_in_lines(test_text, \"Python\", lambda line: len(line) > 15)\n",
    "    print(\"\\nè¡Œæœå°‹ï¼ˆé•·åº¦>15ï¼‰:\", line_results)\n",
    "\n",
    "# å–æ¶ˆè¨»è§£ä¾†æ¸¬è©¦\n",
    "# test_search_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ¨¡çµ„ 3: æ–‡å­—è½‰æ›æ¨¡çµ„ (Transform)\n",
    "\n",
    "### ğŸ’¡ Ch13 é‡é»ï¼šä½œç”¨åŸŸèˆ‡é–‰åŒ…\n",
    "- å»ºç«‹å®¢è£½åŒ–è½‰æ›å™¨ï¼ˆé–‰åŒ…æ‡‰ç”¨ï¼‰\n",
    "- å…§åµŒå‡½å¼èˆ‡å¤–å±¤è®Šæ•¸å­˜å–\n",
    "- ç‹€æ…‹ä¿å­˜èˆ‡é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_case(text, mode='lower'):\n",
    "    \"\"\"\n",
    "    è½‰æ›æ–‡å­—å¤§å°å¯«\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦è½‰æ›çš„æ–‡å­—\n",
    "        mode (str): è½‰æ›æ¨¡å¼ ('lower', 'upper', 'title', 'capitalize')\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: è½‰æ›å¾Œçš„æ–‡å­—\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œå¤§å°å¯«è½‰æ›é‚è¼¯\n",
    "    \n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # TODO: æ ¹æ“šæ¨¡å¼é¸æ“‡è½‰æ›æ–¹å¼\n",
    "    if mode == 'lower':\n",
    "        return text.lower()\n",
    "    elif mode == 'upper':\n",
    "        return text.upper()\n",
    "    elif mode == 'title':\n",
    "        return text.title()\n",
    "    elif mode == 'capitalize':\n",
    "        return text.capitalize()\n",
    "    else:\n",
    "        # TODO: è™•ç†ç„¡æ•ˆæ¨¡å¼\n",
    "        raise ValueError(f\"ç„¡æ•ˆçš„è½‰æ›æ¨¡å¼: {mode}\")\n",
    "\n",
    "def replace_text_advanced(text, replacements, use_regex=False):\n",
    "    \"\"\"\n",
    "    é€²éšæ–‡å­—æ›¿æ›åŠŸèƒ½\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): åŸå§‹æ–‡å­—\n",
    "        replacements (dict): æ›¿æ›è¦å‰‡ {old: new}\n",
    "        use_regex (bool): æ˜¯å¦ä½¿ç”¨æ­£è¦è¡¨é”å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: æ›¿æ›å¾Œçš„æ–‡å­—\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œé€²éšæ›¿æ›é‚è¼¯\n",
    "    \n",
    "    if not text or not replacements:\n",
    "        return text\n",
    "    \n",
    "    result = text\n",
    "    \n",
    "    if use_regex:\n",
    "        # TODO: ä½¿ç”¨æ­£è¦è¡¨é”å¼æ›¿æ›ï¼ˆé€²éšåŠŸèƒ½ï¼‰\n",
    "        import re\n",
    "        for pattern, replacement in replacements.items():\n",
    "            result = re.sub(pattern, replacement, result)\n",
    "    else:\n",
    "        # TODO: ä½¿ç”¨ç°¡å–®å­—ä¸²æ›¿æ›\n",
    "        for old, new in replacements.items():\n",
    "            result = result.replace(old, new)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_text_transformer(config):\n",
    "    \"\"\"\n",
    "    å»ºç«‹å®¢è£½åŒ–æ–‡å­—è½‰æ›å™¨ï¼ˆå±•ç¤º Ch13 é–‰åŒ…æ¦‚å¿µï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        config (dict): è½‰æ›è¨­å®š\n",
    "    \n",
    "    å›å‚³:\n",
    "        function: è½‰æ›å‡½å¼ï¼ˆé–‰åŒ…ï¼‰\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œé–‰åŒ…è½‰æ›å™¨\n",
    "    # é€™æ˜¯ Ch13 é–‰åŒ…æ¦‚å¿µçš„é‡é»å±•ç¤º\n",
    "    \n",
    "    # å¤–å±¤å‡½å¼è®Šæ•¸ï¼ˆé–‰åŒ…ç’°å¢ƒï¼‰\n",
    "    case_mode = config.get('case_mode', 'none')\n",
    "    remove_punctuation = config.get('remove_punctuation', False)\n",
    "    replace_numbers = config.get('replace_numbers', False)\n",
    "    custom_replacements = config.get('custom_replacements', {})\n",
    "    \n",
    "    def transformer(text):\n",
    "        \"\"\"\n",
    "        å…§å±¤å‡½å¼ - å…·æœ‰å¤–å±¤ä½œç”¨åŸŸå­˜å–æ¬Š\n",
    "        é€™å±•ç¤ºäº† Ch13 çš„é–‰åŒ…æ¦‚å¿µ\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        result = text\n",
    "        \n",
    "        # TODO: æ‡‰ç”¨å¤§å°å¯«è½‰æ›ï¼ˆä½¿ç”¨å¤–å±¤è®Šæ•¸ï¼‰\n",
    "        if case_mode != 'none':\n",
    "            result = transform_case(result, case_mode)\n",
    "        \n",
    "        # TODO: ç§»é™¤æ¨™é»ç¬¦è™Ÿ\n",
    "        if remove_punctuation:\n",
    "            # åªä¿ç•™å­—æ¯ã€æ•¸å­—å’Œç©ºæ ¼\n",
    "            result = ''.join(c for c in result if c.isalnum() or c.isspace())\n",
    "        \n",
    "        # TODO: æ›¿æ›æ•¸å­—\n",
    "        if replace_numbers:\n",
    "            result = ''.join(c if not c.isdigit() else '#' for c in result)\n",
    "        \n",
    "        # TODO: æ‡‰ç”¨è‡ªè¨‚æ›¿æ›è¦å‰‡\n",
    "        if custom_replacements:\n",
    "            result = replace_text_advanced(result, custom_replacements)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # è¿”å›å…§å±¤å‡½å¼ï¼ˆé–‰åŒ…ï¼‰\n",
    "    return transformer\n",
    "\n",
    "def batch_transform_texts(text_list, transform_func):\n",
    "    \"\"\"\n",
    "    æ‰¹æ¬¡è™•ç†æ–‡å­—åˆ—è¡¨ï¼ˆå±•ç¤º Ch14 map å‡½å¼ï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text_list (list): æ–‡å­—åˆ—è¡¨\n",
    "        transform_func (callable): è½‰æ›å‡½å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: è½‰æ›å¾Œçš„æ–‡å­—åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # TODO: ä½¿ç”¨ map å‡½å¼æ‰¹æ¬¡è½‰æ›\n",
    "    # é€™å±•ç¤ºäº† Ch14 map å‡½å¼çš„æ‡‰ç”¨\n",
    "    \n",
    "    if not text_list or not transform_func:\n",
    "        return text_list\n",
    "    \n",
    "    # TODO: ä½¿ç”¨ map å‡½å¼\n",
    "    return list(map(transform_func, text_list))\n",
    "\n",
    "# æ¸¬è©¦è½‰æ›æ¨¡çµ„\n",
    "def test_transform_module():\n",
    "    \"\"\"æ¸¬è©¦è½‰æ›æ¨¡çµ„çš„åŠŸèƒ½\"\"\"\n",
    "    test_texts = [\"Hello, World!\", \"Python123 is GREAT!\", \"Test... 456\"]\n",
    "    \n",
    "    print(\"ğŸ”„ æ¸¬è©¦è½‰æ›æ¨¡çµ„\")\n",
    "    \n",
    "    # æ¸¬è©¦åŸºæœ¬å¤§å°å¯«è½‰æ›\n",
    "    print(\"\\nå¤§å°å¯«è½‰æ›:\")\n",
    "    for mode in ['lower', 'upper', 'title']:\n",
    "        result = transform_case(test_texts[0], mode)\n",
    "        print(f\"  {mode}: {result}\")\n",
    "    \n",
    "    # æ¸¬è©¦é€²éšæ›¿æ›\n",
    "    replacements = {'Hello': 'Hi', 'World': 'Python'}\n",
    "    result = replace_text_advanced(test_texts[0], replacements)\n",
    "    print(f\"\\né€²éšæ›¿æ›: {result}\")\n",
    "    \n",
    "    # æ¸¬è©¦é–‰åŒ…è½‰æ›å™¨ï¼ˆCh13 é‡é»ï¼‰\n",
    "    config = {\n",
    "        'case_mode': 'lower',\n",
    "        'remove_punctuation': True,\n",
    "        'replace_numbers': True\n",
    "    }\n",
    "    transformer = create_text_transformer(config)\n",
    "    transformed = transformer(\"Hello, World! 123\")\n",
    "    print(f\"\\né–‰åŒ…è½‰æ›å™¨: {transformed}\")\n",
    "    \n",
    "    # æ¸¬è©¦æ‰¹æ¬¡è½‰æ›ï¼ˆCh14 map æ‡‰ç”¨ï¼‰\n",
    "    batch_results = batch_transform_texts(test_texts, lambda x: x.upper())\n",
    "    print(f\"\\næ‰¹æ¬¡è½‰æ›: {batch_results}\")\n",
    "\n",
    "# å–æ¶ˆè¨»è§£ä¾†æ¸¬è©¦\n",
    "# test_transform_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ æ¨¡çµ„ 4: æ–‡å­—æ ¼å¼åŒ–æ¨¡çµ„ (Format)\n",
    "\n",
    "### ğŸ’¡ Ch12 é‡é»ï¼šå‡½å¼è¨­è¨ˆèˆ‡åƒæ•¸è™•ç†\n",
    "- é è¨­åƒæ•¸çš„ä½¿ç”¨\n",
    "- å½ˆæ€§çš„å‡½å¼ä»‹é¢è¨­è¨ˆ\n",
    "- å­—ä¸²æ ¼å¼åŒ–æŠ€å·§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_text(text, width=80, alignment='left'):\n",
    "    \"\"\"\n",
    "    æ–‡å­—å°é½Šè™•ç†\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦å°é½Šçš„æ–‡å­—\n",
    "        width (int): å°é½Šå¯¬åº¦\n",
    "        alignment (str): å°é½Šæ–¹å¼ ('left', 'right', 'center')\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: å°é½Šå¾Œçš„æ–‡å­—\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œæ–‡å­—å°é½Šé‚è¼¯\n",
    "    \n",
    "    if not text:\n",
    "        return ' ' * width\n",
    "    \n",
    "    # TODO: è™•ç†å¤šè¡Œæ–‡å­—\n",
    "    lines = text.splitlines()\n",
    "    aligned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # TODO: æ ¹æ“šå°é½Šæ–¹å¼è™•ç†æ¯ä¸€è¡Œ\n",
    "        if alignment == 'left':\n",
    "            aligned_line = line.ljust(width)\n",
    "        elif alignment == 'right':\n",
    "            aligned_line = line.rjust(width)\n",
    "        elif alignment == 'center':\n",
    "            aligned_line = line.center(width)\n",
    "        else:\n",
    "            raise ValueError(f\"ç„¡æ•ˆçš„å°é½Šæ–¹å¼: {alignment}\")\n",
    "        \n",
    "        aligned_lines.append(aligned_line)\n",
    "    \n",
    "    return '\\n'.join(aligned_lines)\n",
    "\n",
    "def indent_lines(text, indent_size=4, indent_char=' '):\n",
    "    \"\"\"\n",
    "    ç‚ºæ–‡å­—è¡Œæ·»åŠ ç¸®æ’\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦ç¸®æ’çš„æ–‡å­—\n",
    "        indent_size (int): ç¸®æ’å¤§å°\n",
    "        indent_char (str): ç¸®æ’å­—å…ƒ\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: ç¸®æ’å¾Œçš„æ–‡å­—\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œç¸®æ’é‚è¼¯\n",
    "    \n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # TODO: å»ºç«‹ç¸®æ’å­—ä¸²\n",
    "    indent = indent_char * indent_size\n",
    "    \n",
    "    # TODO: ç‚ºæ¯ä¸€è¡Œæ·»åŠ ç¸®æ’\n",
    "    lines = text.splitlines()\n",
    "    indented_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # TODO: ç©ºè¡Œæ˜¯å¦ç¸®æ’ï¼Ÿï¼ˆè¨­è¨ˆæ±ºç­–ï¼‰\n",
    "        if line.strip():  # éç©ºè¡Œæ‰ç¸®æ’\n",
    "            indented_lines.append(indent + line)\n",
    "        else:\n",
    "            indented_lines.append(line)  # ä¿æŒç©ºè¡Œ\n",
    "    \n",
    "    return '\\n'.join(indented_lines)\n",
    "\n",
    "def create_columns(text_list, columns=2, separator='  |  '):\n",
    "    \"\"\"\n",
    "    å°‡æ–‡å­—åˆ—è¡¨æ ¼å¼åŒ–ç‚ºå¤šæ¬„é¡¯ç¤º\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text_list (list): æ–‡å­—åˆ—è¡¨\n",
    "        columns (int): æ¬„æ•¸\n",
    "        separator (str): æ¬„åˆ†éš”ç¬¦\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: æ ¼å¼åŒ–å¾Œçš„å¤šæ¬„æ–‡å­—\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œå¤šæ¬„æ ¼å¼åŒ–é‚è¼¯\n",
    "    \n",
    "    if not text_list or columns <= 0:\n",
    "        return ''\n",
    "    \n",
    "    # TODO: è¨ˆç®—æ¯æ¬„çš„æœ€å¤§å¯¬åº¦\n",
    "    max_width = max(len(str(item)) for item in text_list) if text_list else 0\n",
    "    \n",
    "    # TODO: åˆ†çµ„è™•ç†\n",
    "    rows = []\n",
    "    for i in range(0, len(text_list), columns):\n",
    "        # TODO: å–å¾—ç•¶å‰è¡Œçš„å…ƒç´ \n",
    "        row_items = text_list[i:i + columns]\n",
    "        \n",
    "        # TODO: æ ¼å¼åŒ–æ¯å€‹å…ƒç´ \n",
    "        formatted_items = []\n",
    "        for item in row_items:\n",
    "            formatted_items.append(str(item).ljust(max_width))\n",
    "        \n",
    "        # TODO: ç”¨åˆ†éš”ç¬¦é€£æ¥\n",
    "        row_text = separator.join(formatted_items)\n",
    "        rows.append(row_text)\n",
    "    \n",
    "    return '\\n'.join(rows)\n",
    "\n",
    "def format_table(data, headers=None, align='left'):\n",
    "    \"\"\"\n",
    "    æ ¼å¼åŒ–è³‡æ–™ç‚ºè¡¨æ ¼\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        data (list): è³‡æ–™åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ ç‚ºä¸€è¡Œè³‡æ–™\n",
    "        headers (list): è¡¨é ­åˆ—è¡¨\n",
    "        align (str): å°é½Šæ–¹å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        str: æ ¼å¼åŒ–çš„è¡¨æ ¼å­—ä¸²\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œè¡¨æ ¼æ ¼å¼åŒ–é‚è¼¯\n",
    "    \n",
    "    if not data:\n",
    "        return ''\n",
    "    \n",
    "    # TODO: æº–å‚™å®Œæ•´è³‡æ–™ï¼ˆåŒ…å«è¡¨é ­ï¼‰\n",
    "    all_rows = []\n",
    "    if headers:\n",
    "        all_rows.append(headers)\n",
    "    all_rows.extend(data)\n",
    "    \n",
    "    # TODO: è¨ˆç®—æ¯æ¬„çš„æœ€å¤§å¯¬åº¦\n",
    "    if not all_rows:\n",
    "        return ''\n",
    "    \n",
    "    num_cols = len(all_rows[0]) if all_rows else 0\n",
    "    col_widths = [0] * num_cols\n",
    "    \n",
    "    for row in all_rows:\n",
    "        for i, cell in enumerate(row):\n",
    "            if i < len(col_widths):\n",
    "                col_widths[i] = max(col_widths[i], len(str(cell)))\n",
    "    \n",
    "    # TODO: æ ¼å¼åŒ–æ¯ä¸€è¡Œ\n",
    "    formatted_rows = []\n",
    "    for i, row in enumerate(all_rows):\n",
    "        formatted_cells = []\n",
    "        for j, cell in enumerate(row):\n",
    "            if j < len(col_widths):\n",
    "                # TODO: æ ¹æ“šå°é½Šæ–¹å¼æ ¼å¼åŒ–\n",
    "                if align == 'left':\n",
    "                    formatted_cell = str(cell).ljust(col_widths[j])\n",
    "                elif align == 'right':\n",
    "                    formatted_cell = str(cell).rjust(col_widths[j])\n",
    "                else:  # center\n",
    "                    formatted_cell = str(cell).center(col_widths[j])\n",
    "                formatted_cells.append(formatted_cell)\n",
    "        \n",
    "        row_text = ' | '.join(formatted_cells)\n",
    "        formatted_rows.append(row_text)\n",
    "        \n",
    "        # TODO: åœ¨è¡¨é ­å¾ŒåŠ åˆ†éš”ç·š\n",
    "        if i == 0 and headers:\n",
    "            separator_line = '-' * len(row_text)\n",
    "            formatted_rows.append(separator_line)\n",
    "    \n",
    "    return '\\n'.join(formatted_rows)\n",
    "\n",
    "# æ¸¬è©¦æ ¼å¼åŒ–æ¨¡çµ„\n",
    "def test_format_module():\n",
    "    \"\"\"æ¸¬è©¦æ ¼å¼åŒ–æ¨¡çµ„çš„åŠŸèƒ½\"\"\"\n",
    "    print(\"ğŸ“ æ¸¬è©¦æ ¼å¼åŒ–æ¨¡çµ„\")\n",
    "    \n",
    "    # æ¸¬è©¦æ–‡å­—å°é½Š\n",
    "    test_text = \"Hello\\nWorld\"\n",
    "    print(\"\\næ–‡å­—å°é½Š (center, width=10):\")\n",
    "    aligned = align_text(test_text, width=10, alignment='center')\n",
    "    print(repr(aligned))\n",
    "    \n",
    "    # æ¸¬è©¦ç¸®æ’\n",
    "    print(\"\\nç¸®æ’ (4 spaces):\")\n",
    "    indented = indent_lines(test_text, indent_size=4)\n",
    "    print(repr(indented))\n",
    "    \n",
    "    # æ¸¬è©¦å¤šæ¬„é¡¯ç¤º\n",
    "    items = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n",
    "    print(\"\\nå¤šæ¬„é¡¯ç¤º (2 columns):\")\n",
    "    columns = create_columns(items, columns=2)\n",
    "    print(columns)\n",
    "    \n",
    "    # æ¸¬è©¦è¡¨æ ¼æ ¼å¼åŒ–\n",
    "    headers = ['Name', 'Age', 'City']\n",
    "    data = [['Alice', '25', 'New York'], ['Bob', '30', 'London']]\n",
    "    print(\"\\nè¡¨æ ¼æ ¼å¼åŒ–:\")\n",
    "    table = format_table(data, headers)\n",
    "    print(table)\n",
    "\n",
    "# å–æ¶ˆè¨»è§£ä¾†æ¸¬è©¦\n",
    "# test_format_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ æ¨¡çµ„ 5: æ–‡å­—åˆ†ææ¨¡çµ„ (Analysis)\n",
    "\n",
    "### ğŸ’¡ Ch12 é‡é»ï¼šè¤‡é›œé‚è¼¯çš„å‡½å¼åˆ†è§£\n",
    "- å°‡è¤‡é›œåˆ†æåˆ†è§£ç‚ºå°å‡½å¼\n",
    "- æ¼”ç®—æ³•çš„æ¨¡çµ„åŒ–å¯¦ä½œ\n",
    "- è³‡æ–™çµæ§‹çš„æœ‰æ•ˆé‹ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentences(text):\n",
    "    \"\"\"\n",
    "    å¥å­åˆ†æ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: å¥å­åˆ†æçµæœ\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œå¥å­åˆ†æé‚è¼¯\n",
    "    \n",
    "    if not text.strip():\n",
    "        return {\n",
    "            'sentence_count': 0,\n",
    "            'sentences': [],\n",
    "            'average_sentence_length': 0.0,\n",
    "            'longest_sentence': '',\n",
    "            'shortest_sentence': ''\n",
    "        }\n",
    "    \n",
    "    # TODO: åˆ†å‰²å¥å­ï¼ˆç°¡åŒ–ç‰ˆï¼Œä»¥ .!? ç‚ºåˆ†éš”ï¼‰\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    # TODO: æ¸…ç†å¥å­ï¼ˆç§»é™¤ç©ºç™½å¥å­ï¼‰\n",
    "    clean_sentences = []\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = sentence.strip()\n",
    "        if clean_sentence:\n",
    "            clean_sentences.append(clean_sentence)\n",
    "    \n",
    "    if not clean_sentences:\n",
    "        return {\n",
    "            'sentence_count': 0,\n",
    "            'sentences': [],\n",
    "            'average_sentence_length': 0.0,\n",
    "            'longest_sentence': '',\n",
    "            'shortest_sentence': ''\n",
    "        }\n",
    "    \n",
    "    # TODO: è¨ˆç®—å¥å­é•·åº¦çµ±è¨ˆ\n",
    "    sentence_lengths = [len(sentence) for sentence in clean_sentences]\n",
    "    average_length = sum(sentence_lengths) / len(sentence_lengths)\n",
    "    \n",
    "    # TODO: æ‰¾å‡ºæœ€é•·å’Œæœ€çŸ­å¥å­\n",
    "    longest_sentence = max(clean_sentences, key=len)\n",
    "    shortest_sentence = min(clean_sentences, key=len)\n",
    "    \n",
    "    return {\n",
    "        'sentence_count': len(clean_sentences),\n",
    "        'sentences': clean_sentences,\n",
    "        'average_sentence_length': average_length,\n",
    "        'longest_sentence': longest_sentence,\n",
    "        'shortest_sentence': shortest_sentence\n",
    "    }\n",
    "\n",
    "def detect_duplicates(text, min_length=5):\n",
    "    \"\"\"\n",
    "    æª¢æ¸¬é‡è¤‡ç‰‡æ®µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æª¢æ¸¬çš„æ–‡å­—\n",
    "        min_length (int): æœ€å°é‡è¤‡é•·åº¦\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: é‡è¤‡ç‰‡æ®µåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œé‡è¤‡ç‰‡æ®µæª¢æ¸¬é‚è¼¯\n",
    "    \n",
    "    if not text or min_length <= 0:\n",
    "        return []\n",
    "    \n",
    "    # TODO: ç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„å­å­—ä¸²\n",
    "    fragments = {}\n",
    "    text_length = len(text)\n",
    "    \n",
    "    for i in range(text_length):\n",
    "        for j in range(i + min_length, text_length + 1):\n",
    "            fragment = text[i:j]\n",
    "            \n",
    "            # TODO: çµ±è¨ˆç‰‡æ®µå‡ºç¾æ¬¡æ•¸\n",
    "            if fragment in fragments:\n",
    "                fragments[fragment]['count'] += 1\n",
    "                fragments[fragment]['positions'].append(i)\n",
    "            else:\n",
    "                fragments[fragment] = {\n",
    "                    'count': 1,\n",
    "                    'positions': [i]\n",
    "                }\n",
    "    \n",
    "    # TODO: éæ¿¾å‡ºçœŸæ­£çš„é‡è¤‡ç‰‡æ®µ\n",
    "    duplicates = []\n",
    "    for fragment, info in fragments.items():\n",
    "        if info['count'] > 1:\n",
    "            duplicates.append({\n",
    "                'fragment': fragment,\n",
    "                'count': info['count'],\n",
    "                'positions': info['positions']\n",
    "            })\n",
    "    \n",
    "    # TODO: æŒ‰é‡è¤‡æ¬¡æ•¸æ’åº\n",
    "    duplicates.sort(key=lambda x: x['count'], reverse=True)\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "def calculate_readability_score(text):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ–‡å­—å¯è®€æ€§åˆ†æ•¸ï¼ˆç°¡åŒ–ç‰ˆï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦åˆ†æçš„æ–‡å­—\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: å¯è®€æ€§åˆ†æçµæœ\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œå¯è®€æ€§åˆ†æé‚è¼¯\n",
    "    \n",
    "    if not text.strip():\n",
    "        return {\n",
    "            'avg_sentence_length': 0.0,\n",
    "            'avg_word_length': 0.0,\n",
    "            'complexity_score': 0.0,\n",
    "            'difficulty_level': 'Easy'\n",
    "        }\n",
    "    \n",
    "    # TODO: ä½¿ç”¨å·²æœ‰çš„åˆ†æå‡½å¼\n",
    "    sentence_analysis = analyze_sentences(text)\n",
    "    word_analysis = count_words(text)  # éœ€è¦å¾æ¨¡çµ„ 1 åŒ¯å…¥\n",
    "    \n",
    "    # TODO: è¨ˆç®—å¹³å‡å¥é•·å’Œè©é•·\n",
    "    avg_sentence_length = sentence_analysis['average_sentence_length']\n",
    "    avg_word_length = word_analysis['average_word_length']\n",
    "    \n",
    "    # TODO: è¨ˆç®—è¤‡é›œåº¦åˆ†æ•¸ï¼ˆç°¡åŒ–å…¬å¼ï¼‰\n",
    "    # è¤‡é›œåº¦ = (å¹³å‡å¥é•· Ã— 0.4) + (å¹³å‡è©é•· Ã— 0.6) Ã— 10\n",
    "    complexity_score = (avg_sentence_length * 0.4 + avg_word_length * 0.6) * 10\n",
    "    \n",
    "    # TODO: åˆ¤å®šé›£åº¦ç­‰ç´š\n",
    "    if complexity_score < 30:\n",
    "        difficulty_level = 'Easy'\n",
    "    elif complexity_score < 60:\n",
    "        difficulty_level = 'Medium'\n",
    "    else:\n",
    "        difficulty_level = 'Hard'\n",
    "    \n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'complexity_score': complexity_score,\n",
    "        'difficulty_level': difficulty_level\n",
    "    }\n",
    "\n",
    "# æ¸¬è©¦åˆ†ææ¨¡çµ„\n",
    "def test_analysis_module():\n",
    "    \"\"\"æ¸¬è©¦åˆ†ææ¨¡çµ„çš„åŠŸèƒ½\"\"\"\n",
    "    test_text = \"\"\"Hello world! This is a test. \n",
    "    Python is great. Python makes programming fun!\n",
    "    Programming is an art. Art is beautiful.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¬ æ¸¬è©¦åˆ†ææ¨¡çµ„\")\n",
    "    \n",
    "    # æ¸¬è©¦å¥å­åˆ†æ\n",
    "    sentence_result = analyze_sentences(test_text)\n",
    "    print(\"\\nå¥å­åˆ†æ:\", sentence_result)\n",
    "    \n",
    "    # æ¸¬è©¦é‡è¤‡æª¢æ¸¬\n",
    "    duplicate_result = detect_duplicates(test_text, min_length=3)\n",
    "    print(\"\\né‡è¤‡æª¢æ¸¬:\", duplicate_result[:3])  # åªé¡¯ç¤ºå‰3å€‹\n",
    "    \n",
    "    # æ¸¬è©¦å¯è®€æ€§åˆ†æ\n",
    "    readability_result = calculate_readability_score(test_text)\n",
    "    print(\"\\nå¯è®€æ€§åˆ†æ:\", readability_result)\n",
    "\n",
    "# å–æ¶ˆè¨»è§£ä¾†æ¸¬è©¦\n",
    "# test_analysis_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ¨¡çµ„ 6: éè¿´æª”æ¡ˆè™•ç† (Recursive) [é€²éš]\n",
    "\n",
    "### ğŸ’¡ Ch15 é‡é»ï¼šéè¿´æ€ç¶­\n",
    "- éè¿´ä¸‰è¦ç´ ï¼šåŸºæœ¬æƒ…æ³ã€éè¿´æƒ…æ³ã€ç‹€æ…‹æ”¹è®Š\n",
    "- éè¿´åœ¨å¯¦éš›å•é¡Œä¸­çš„æ‡‰ç”¨\n",
    "- é¿å…ç„¡é™éè¿´çš„é˜²è­·æ©Ÿåˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_files_recursive(directory, pattern, file_extension='.txt', max_depth=5):\n",
    "    \"\"\"\n",
    "    éè¿´æœå°‹ç›®éŒ„ä¸­çš„æ–‡å­—æª”æ¡ˆ\n",
    "    å±•ç¤º Ch15 éè¿´æ€ç¶­çš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        directory (str): æœå°‹ç›®éŒ„è·¯å¾‘\n",
    "        pattern (str): æœå°‹æ¨¡å¼\n",
    "        file_extension (str): æª”æ¡ˆå‰¯æª”å\n",
    "        max_depth (int): æœ€å¤§æœå°‹æ·±åº¦\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: æœå°‹çµæœåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    def _recursive_search(current_dir, current_depth):\n",
    "        \"\"\"\n",
    "        å…§éƒ¨éè¿´å‡½å¼\n",
    "        å±•ç¤º Ch15 éè¿´çš„ä¸‰è¦ç´ \n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # TODO: åŸºæœ¬æƒ…æ³ (Base Case) - Ch15 é‡é»\n",
    "        if current_depth >= max_depth:\n",
    "            return results\n",
    "        \n",
    "        try:\n",
    "            items = os.listdir(current_dir)\n",
    "        except (PermissionError, FileNotFoundError):\n",
    "            return results\n",
    "        \n",
    "        for item in items:\n",
    "            item_path = os.path.join(current_dir, item)\n",
    "            \n",
    "            if os.path.isfile(item_path) and item.endswith(file_extension):\n",
    "                # TODO: è™•ç†æª”æ¡ˆ\n",
    "                try:\n",
    "                    with open(item_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        if pattern in content:\n",
    "                            results.append({\n",
    "                                'file_path': item_path,\n",
    "                                'matches': content.count(pattern),\n",
    "                                'depth': current_depth,\n",
    "                                'file_size': len(content)\n",
    "                            })\n",
    "                except (UnicodeDecodeError, PermissionError):\n",
    "                    continue\n",
    "            \n",
    "            elif os.path.isdir(item_path):\n",
    "                # TODO: éè¿´æƒ…æ³ (Recursive Case) - Ch15 é‡é»\n",
    "                # ç‹€æ…‹æ”¹è®Šï¼šæ·±åº¦éå¢\n",
    "                sub_results = _recursive_search(item_path, current_depth + 1)\n",
    "                results.extend(sub_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # TODO: é–‹å§‹éè¿´æœå°‹\n",
    "    return _recursive_search(directory, 0)\n",
    "\n",
    "def count_pattern_recursive(text, pattern):\n",
    "    \"\"\"\n",
    "    éè¿´è¨ˆç®—æ¨¡å¼å‡ºç¾æ¬¡æ•¸\n",
    "    å±•ç¤º Ch15 éè¿´æ€ç¶­ï¼ˆæ•™å­¸ç›®çš„ï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦æœå°‹çš„æ–‡å­—\n",
    "        pattern (str): æœå°‹æ¨¡å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        int: æ¨¡å¼å‡ºç¾æ¬¡æ•¸\n",
    "    \n",
    "    æ³¨æ„: é€™æ˜¯å±•ç¤ºéè¿´æ€ç¶­çš„ç·´ç¿’ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­ç›´æ¥ä½¿ç”¨ text.count() æ›´é«˜æ•ˆ\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œéè¿´è¨ˆæ•¸é‚è¼¯\n",
    "    # å±•ç¤º Ch15 éè¿´çš„ä¸‰è¦ç´ \n",
    "    \n",
    "    # åŸºæœ¬æƒ…æ³ 1: æ–‡å­—æˆ–æ¨¡å¼ç‚ºç©º\n",
    "    if not text or not pattern:\n",
    "        return 0\n",
    "    \n",
    "    # åŸºæœ¬æƒ…æ³ 2: æ–‡å­—é•·åº¦å°æ–¼æ¨¡å¼é•·åº¦\n",
    "    if len(text) < len(pattern):\n",
    "        return 0\n",
    "    \n",
    "    # TODO: æª¢æŸ¥æ–‡å­—é–‹é ­æ˜¯å¦åŒ¹é…æ¨¡å¼\n",
    "    if text.startswith(pattern):\n",
    "        # éè¿´æƒ…æ³ 1: æ‰¾åˆ°åŒ¹é…ï¼Œç¹¼çºŒæœå°‹å‰©é¤˜éƒ¨åˆ†\n",
    "        return 1 + count_pattern_recursive(text[len(pattern):], pattern)\n",
    "    else:\n",
    "        # éè¿´æƒ…æ³ 2: æ²’æ‰¾åˆ°åŒ¹é…ï¼Œæœå°‹ä¸‹ä¸€å€‹å­—å…ƒé–‹å§‹çš„å­å­—ä¸²\n",
    "        return count_pattern_recursive(text[1:], pattern)\n",
    "\n",
    "def parse_nested_structure(text, open_tag='[', close_tag=']'):\n",
    "    \"\"\"\n",
    "    éè¿´è§£æå·¢ç‹€çµæ§‹ï¼ˆå¦‚æ‹¬è™ŸåŒ¹é…ï¼‰\n",
    "    å±•ç¤º Ch15 éè¿´åœ¨è¤‡é›œè³‡æ–™çµæ§‹è™•ç†ä¸­çš„æ‡‰ç”¨\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text (str): è¦è§£æçš„æ–‡å­—\n",
    "        open_tag (str): é–‹å§‹æ¨™è¨˜\n",
    "        close_tag (str): çµæŸæ¨™è¨˜\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: è§£æçµæœ\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œå·¢ç‹€çµæ§‹è§£æ\n",
    "    # é€™æ˜¯éè¿´çš„é€²éšæ‡‰ç”¨\n",
    "    \n",
    "    def _parse_recursive(text, pos, depth):\n",
    "        \"\"\"\n",
    "        å…§éƒ¨éè¿´è§£æå‡½å¼\n",
    "        \"\"\"\n",
    "        structure = []\n",
    "        current_text = ''\n",
    "        max_depth_seen = depth\n",
    "        \n",
    "        i = pos\n",
    "        while i < len(text):\n",
    "            char = text[i]\n",
    "            \n",
    "            if char == open_tag:\n",
    "                # TODO: é‡åˆ°é–‹å§‹æ¨™è¨˜ï¼Œéè¿´è™•ç†å…§éƒ¨çµæ§‹\n",
    "                if current_text:\n",
    "                    structure.append(current_text)\n",
    "                    current_text = ''\n",
    "                \n",
    "                # éè¿´è§£æå…§éƒ¨çµæ§‹\n",
    "                inner_structure, next_pos, inner_max_depth = _parse_recursive(text, i + 1, depth + 1)\n",
    "                structure.append(inner_structure)\n",
    "                max_depth_seen = max(max_depth_seen, inner_max_depth)\n",
    "                i = next_pos\n",
    "                \n",
    "            elif char == close_tag:\n",
    "                # TODO: é‡åˆ°çµæŸæ¨™è¨˜ï¼ŒçµæŸç•¶å‰å±¤ç´š\n",
    "                if current_text:\n",
    "                    structure.append(current_text)\n",
    "                return structure, i + 1, max_depth_seen\n",
    "            \n",
    "            else:\n",
    "                # TODO: æ™®é€šå­—å…ƒï¼Œç´¯ç©åˆ°ç•¶å‰æ–‡å­—\n",
    "                current_text += char\n",
    "                i += 1\n",
    "        \n",
    "        # TODO: è™•ç†æ–‡å­—çµå°¾\n",
    "        if current_text:\n",
    "            structure.append(current_text)\n",
    "        \n",
    "        return structure, len(text), max_depth_seen\n",
    "    \n",
    "    # TODO: æª¢æŸ¥çµæ§‹æ˜¯å¦å¹³è¡¡\n",
    "    open_count = text.count(open_tag)\n",
    "    close_count = text.count(close_tag)\n",
    "    is_balanced = (open_count == close_count)\n",
    "    \n",
    "    # TODO: é–‹å§‹è§£æ\n",
    "    structure, _, max_depth = _parse_recursive(text, 0, 0)\n",
    "    \n",
    "    return {\n",
    "        'is_balanced': is_balanced,\n",
    "        'max_depth': max_depth,\n",
    "        'structure': structure\n",
    "    }\n",
    "\n",
    "# æ¸¬è©¦éè¿´æ¨¡çµ„\n",
    "def test_recursive_module():\n",
    "    \"\"\"æ¸¬è©¦éè¿´æ¨¡çµ„çš„åŠŸèƒ½\"\"\"\n",
    "    print(\"ğŸ”„ æ¸¬è©¦éè¿´æ¨¡çµ„ (Ch15 é‡é»)\")\n",
    "    \n",
    "    # æ¸¬è©¦éè¿´è¨ˆæ•¸\n",
    "    test_text = \"Python is great. Python rocks. Python!\"\n",
    "    count = count_pattern_recursive(test_text, \"Python\")\n",
    "    print(f\"\\néè¿´è¨ˆæ•¸ 'Python': {count}\")\n",
    "    \n",
    "    # æ¸¬è©¦å·¢ç‹€çµæ§‹è§£æ\n",
    "    nested_text = \"[A[B[C]D]E]\"\n",
    "    parsed = parse_nested_structure(nested_text)\n",
    "    print(f\"\\nå·¢ç‹€çµæ§‹è§£æ: {parsed}\")\n",
    "    \n",
    "    # æ¸¬è©¦æª”æ¡ˆæœå°‹ï¼ˆæ³¨æ„ï¼šéœ€è¦å¯¦éš›ç›®éŒ„ï¼‰\n",
    "    print(\"\\næª”æ¡ˆæœå°‹åŠŸèƒ½å·²å¯¦ä½œï¼Œéœ€è¦æŒ‡å®šå¯¦éš›ç›®éŒ„è·¯å¾‘é€²è¡Œæ¸¬è©¦\")\n",
    "\n",
    "# å–æ¶ˆè¨»è§£ä¾†æ¸¬è©¦\n",
    "# test_recursive_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§© æ¨¡çµ„ 7: å‡½å¼å¼ç¨‹å¼è¨­è¨ˆ (Functional) [é€²éš]\n",
    "\n",
    "### ğŸ’¡ Ch14 é‡é»ï¼šé«˜éšå‡½å¼é€²éšæ‡‰ç”¨\n",
    "- å‡½å¼çµ„åˆèˆ‡æµæ°´ç·šè¨­è¨ˆ\n",
    "- reduce å‡½å¼çš„å¯¦éš›æ‡‰ç”¨\n",
    "- é«˜éšå‡½å¼å·¥å» çš„å»ºç«‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_pipeline(*operations):\n",
    "    \"\"\"\n",
    "    å»ºç«‹æ–‡å­—è™•ç†æµæ°´ç·šï¼ˆå±•ç¤ºå‡½å¼çµ„åˆï¼‰\n",
    "    å±•ç¤º Ch14 å‡½å¼çµ„åˆçš„é€²éšæ¦‚å¿µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        *operations: å¯è®Šæ•¸é‡çš„è™•ç†å‡½å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        function: çµ„åˆå¾Œçš„è™•ç†å‡½å¼\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œå‡½å¼çµ„åˆé‚è¼¯\n",
    "    # é€™å±•ç¤ºäº† Ch14 å‡½å¼å¼ç¨‹å¼è¨­è¨ˆçš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "    \n",
    "    def pipeline(text):\n",
    "        \"\"\"\n",
    "        æµæ°´ç·šè™•ç†å‡½å¼\n",
    "        ä¾åºå¥—ç”¨æ‰€æœ‰æ“ä½œ\n",
    "        \"\"\"\n",
    "        result = text\n",
    "        \n",
    "        # TODO: ä¾åºå¥—ç”¨æ‰€æœ‰æ“ä½œ\n",
    "        for operation in operations:\n",
    "            result = operation(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def apply_text_filters(text_list, *filters):\n",
    "    \"\"\"\n",
    "    ä¾åºå¥—ç”¨å¤šå€‹éæ¿¾å™¨ï¼ˆå±•ç¤º filter å‡½å¼é€£é–ï¼‰\n",
    "    å±•ç¤º Ch14 filter å‡½å¼çš„é€²éšæ‡‰ç”¨\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text_list (list): æ–‡å­—åˆ—è¡¨\n",
    "        *filters: å¯è®Šæ•¸é‡çš„éæ¿¾å‡½å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        list: éæ¿¾å¾Œçš„æ–‡å­—åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œéæ¿¾å™¨é€£é–é‚è¼¯\n",
    "    \n",
    "    result = text_list\n",
    "    \n",
    "    # TODO: ä¾åºå¥—ç”¨æ‰€æœ‰éæ¿¾å™¨\n",
    "    for filter_func in filters:\n",
    "        result = list(filter(filter_func, result))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def reduce_text_analysis(text_list, analysis_func):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ reduce é€²è¡Œæ–‡å­—åˆ†æèšåˆ\n",
    "    å±•ç¤º Ch14 reduce å‡½å¼çš„å¯¦éš›æ‡‰ç”¨\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        text_list (list): æ–‡å­—åˆ—è¡¨\n",
    "        analysis_func (callable): åˆ†æå‡½å¼\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: èšåˆåˆ†æçµæœ\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œ reduce èšåˆé‚è¼¯\n",
    "    from functools import reduce\n",
    "    \n",
    "    if not text_list:\n",
    "        return {}\n",
    "    \n",
    "    # TODO: ä½¿ç”¨ reduce å‡½å¼èšåˆåˆ†æçµæœ\n",
    "    def combine_analysis(acc, text):\n",
    "        \"\"\"\n",
    "        çµ„åˆåˆ†æçµæœçš„å‡½å¼\n",
    "        \"\"\"\n",
    "        return analysis_func(acc, text)\n",
    "    \n",
    "    # TODO: å¾ç©ºå­—å…¸é–‹å§‹èšåˆ\n",
    "    initial_value = {}\n",
    "    result = reduce(combine_analysis, text_list, initial_value)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_custom_filters():\n",
    "    \"\"\"\n",
    "    å»ºç«‹å¸¸ç”¨çš„ Lambda éæ¿¾å™¨é›†åˆ\n",
    "    å±•ç¤º Ch14 é«˜éšå‡½å¼å·¥å» çš„æ¦‚å¿µ\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: éæ¿¾å™¨å­—å…¸\n",
    "    \"\"\"\n",
    "    # TODO: å»ºç«‹éæ¿¾å™¨å·¥å» \n",
    "    # é€™å±•ç¤ºäº† Ch14 é«˜éšå‡½å¼è¿”å›å‡½å¼çš„æ¦‚å¿µ\n",
    "    \n",
    "    filters = {\n",
    "        # åŸºæœ¬éæ¿¾å™¨\n",
    "        'non_empty': lambda text: text.strip() != '',\n",
    "        \n",
    "        # é«˜éšéæ¿¾å™¨å·¥å» ï¼ˆè¿”å›å‡½å¼çš„å‡½å¼ï¼‰\n",
    "        'min_length': lambda min_len: lambda text: len(text) >= min_len,\n",
    "        \n",
    "        'max_length': lambda max_len: lambda text: len(text) <= max_len,\n",
    "        \n",
    "        'contains_keyword': lambda keyword: lambda text: keyword.lower() in text.lower(),\n",
    "        \n",
    "        'starts_with': lambda prefix: lambda text: text.lower().startswith(prefix.lower()),\n",
    "        \n",
    "        'ends_with': lambda suffix: lambda text: text.lower().endswith(suffix.lower()),\n",
    "        \n",
    "        # è¤‡é›œéæ¿¾å™¨\n",
    "        'word_count_range': lambda min_words, max_words: (\n",
    "            lambda text: min_words <= len(text.split()) <= max_words\n",
    "        ),\n",
    "        \n",
    "        'contains_any': lambda keywords: (\n",
    "            lambda text: any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "        ),\n",
    "        \n",
    "        'contains_all': lambda keywords: (\n",
    "            lambda text: all(keyword.lower() in text.lower() for keyword in keywords)\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return filters\n",
    "\n",
    "# é€²éšåŠŸèƒ½ï¼šå‡½å¼çµ„åˆå™¨\n",
    "def compose_functions(*functions):\n",
    "    \"\"\"\n",
    "    å‡½å¼çµ„åˆå™¨ - é€²éšå‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ¦‚å¿µ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        *functions: è¦çµ„åˆçš„å‡½å¼åˆ—è¡¨\n",
    "    \n",
    "    å›å‚³:\n",
    "        function: çµ„åˆå¾Œçš„å‡½å¼\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œå‡½å¼çµ„åˆå™¨\n",
    "    from functools import reduce\n",
    "    \n",
    "    def compose_two(f, g):\n",
    "        \"\"\"çµ„åˆå…©å€‹å‡½å¼\"\"\"\n",
    "        return lambda x: f(g(x))\n",
    "    \n",
    "    # TODO: ä½¿ç”¨ reduce çµ„åˆå¤šå€‹å‡½å¼\n",
    "    return reduce(compose_two, functions, lambda x: x)\n",
    "\n",
    "# æ¸¬è©¦å‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ¨¡çµ„\n",
    "def test_functional_module():\n",
    "    \"\"\"æ¸¬è©¦å‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ¨¡çµ„çš„åŠŸèƒ½\"\"\"\n",
    "    print(\"ğŸ§© æ¸¬è©¦å‡½å¼å¼ç¨‹å¼è¨­è¨ˆæ¨¡çµ„ (Ch14 é€²éš)\")\n",
    "    \n",
    "    # æ¸¬è©¦æ–‡å­—è™•ç†æµæ°´ç·š\n",
    "    pipeline = create_text_pipeline(\n",
    "        lambda x: x.lower(),\n",
    "        lambda x: x.replace(' ', '_'),\n",
    "        lambda x: x.strip()\n",
    "    )\n",
    "    result = pipeline(\"  Hello World  \")\n",
    "    print(f\"\\næµæ°´ç·šè™•ç†: '{result}'\")\n",
    "    \n",
    "    # æ¸¬è©¦éæ¿¾å™¨é€£é–\n",
    "    texts = [\"hello\", \"\", \"world\", \"python\", \"hi\"]\n",
    "    filtered = apply_text_filters(\n",
    "        texts,\n",
    "        lambda x: x != \"\",      # ç§»é™¤ç©ºå­—ä¸²\n",
    "        lambda x: len(x) > 4    # ä¿ç•™é•·åº¦ > 4 çš„è©\n",
    "    )\n",
    "    print(f\"\\néæ¿¾å™¨é€£é–: {filtered}\")\n",
    "    \n",
    "    # æ¸¬è©¦è‡ªè¨‚éæ¿¾å™¨å·¥å» \n",
    "    filters = create_custom_filters()\n",
    "    \n",
    "    # å»ºç«‹é•·åº¦éæ¿¾å™¨\n",
    "    min_5_filter = filters['min_length'](5)\n",
    "    print(f\"\\né•·åº¦éæ¿¾å™¨æ¸¬è©¦: min_5_filter('hello') = {min_5_filter('hello')}\")\n",
    "    \n",
    "    # å»ºç«‹é—œéµå­—éæ¿¾å™¨\n",
    "    python_filter = filters['contains_keyword']('python')\n",
    "    print(f\"é—œéµå­—éæ¿¾å™¨æ¸¬è©¦: python_filter('I love Python') = {python_filter('I love Python')}\")\n",
    "    \n",
    "    # æ¸¬è©¦ reduce èšåˆ\n",
    "    def word_count_aggregator(acc, text):\n",
    "        word_count = len(text.split())\n",
    "        return {\n",
    "            'total_texts': acc.get('total_texts', 0) + 1,\n",
    "            'total_words': acc.get('total_words', 0) + word_count,\n",
    "            'max_words': max(acc.get('max_words', 0), word_count)\n",
    "        }\n",
    "    \n",
    "    aggregate_result = reduce_text_analysis(\n",
    "        [\"Hello world\", \"Python is great\", \"Programming\"],\n",
    "        word_count_aggregator\n",
    "    )\n",
    "    print(f\"\\nReduce èšåˆçµæœ: {aggregate_result}\")\n",
    "\n",
    "# å–æ¶ˆè¨»è§£ä¾†æ¸¬è©¦\n",
    "# test_functional_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç³»çµ±æ•´åˆèˆ‡ä¸»ç¨‹å¼\n",
    "\n",
    "### ğŸ’¡ ç¶œåˆæ‡‰ç”¨ï¼šæ•´åˆæ‰€æœ‰æ¨¡çµ„\n",
    "- å±•ç¤º Ch12 æ¨¡çµ„åŒ–è¨­è¨ˆçš„å¨åŠ›\n",
    "- å»ºç«‹å®Œæ•´çš„ç³»çµ±æ¶æ§‹\n",
    "- æä¾›ä½¿ç”¨è€…å‹å–„çš„ä»‹é¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_toolkit():\n",
    "    \"\"\"\n",
    "    å»ºç«‹æ–‡å­—è™•ç†å·¥å…·ç®±ä¸»ä»‹é¢\n",
    "    å±•ç¤º Ch12 æ¨¡çµ„åŒ–è¨­è¨ˆçš„å®Œæ•´æ‡‰ç”¨\n",
    "    \n",
    "    å›å‚³:\n",
    "        dict: å·¥å…·ç®±å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡çµ„åŠŸèƒ½\n",
    "    \"\"\"\n",
    "    # TODO: å»ºç«‹å®Œæ•´çš„å·¥å…·ç®±å­—å…¸\n",
    "    # é€™å±•ç¤ºäº† Ch12 æ¨¡çµ„åŒ–è¨­è¨ˆçš„æ¦‚å¿µ\n",
    "    \n",
    "    toolkit = {\n",
    "        'statistics': {\n",
    "            'count_characters': count_characters,\n",
    "            'count_words': count_words,\n",
    "            'count_lines': count_lines,\n",
    "            'analyze_text_statistics': analyze_text_statistics\n",
    "        },\n",
    "        'search': {\n",
    "            'find_keyword': find_keyword,\n",
    "            'find_multiple_keywords': find_multiple_keywords,\n",
    "            'search_in_lines': search_in_lines\n",
    "        },\n",
    "        'transform': {\n",
    "            'transform_case': transform_case,\n",
    "            'replace_text_advanced': replace_text_advanced,\n",
    "            'create_text_transformer': create_text_transformer,\n",
    "            'batch_transform_texts': batch_transform_texts\n",
    "        },\n",
    "        'format': {\n",
    "            'align_text': align_text,\n",
    "            'indent_lines': indent_lines,\n",
    "            'create_columns': create_columns,\n",
    "            'format_table': format_table\n",
    "        },\n",
    "        'analysis': {\n",
    "            'analyze_sentences': analyze_sentences,\n",
    "            'detect_duplicates': detect_duplicates,\n",
    "            'calculate_readability_score': calculate_readability_score\n",
    "        },\n",
    "        # é€²éšæ¨¡çµ„ï¼ˆé¸åšï¼‰\n",
    "        'recursive': {\n",
    "            'search_files_recursive': search_files_recursive,\n",
    "            'count_pattern_recursive': count_pattern_recursive,\n",
    "            'parse_nested_structure': parse_nested_structure\n",
    "        },\n",
    "        'functional': {\n",
    "            'create_text_pipeline': create_text_pipeline,\n",
    "            'apply_text_filters': apply_text_filters,\n",
    "            'reduce_text_analysis': reduce_text_analysis,\n",
    "            'create_custom_filters': create_custom_filters\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return toolkit\n",
    "\n",
    "def interactive_menu():\n",
    "    \"\"\"\n",
    "    äº’å‹•å¼é¸å–®ç³»çµ±\n",
    "    \"\"\"\n",
    "    toolkit = create_text_toolkit()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ”§ æ–‡å­—è™•ç†å·¥å…·ç®± v1.0\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1. æ–‡å­—çµ±è¨ˆåˆ†æ\")\n",
    "        print(\"2. æ–‡å­—æœå°‹åŠŸèƒ½\")\n",
    "        print(\"3. æ–‡å­—è½‰æ›å·¥å…·\")\n",
    "        print(\"4. æ–‡å­—æ ¼å¼åŒ–\")\n",
    "        print(\"5. æ–‡å­—æ·±åº¦åˆ†æ\")\n",
    "        print(\"6. éè¿´æª”æ¡ˆè™•ç† (é€²éš)\")\n",
    "        print(\"7. å‡½å¼å¼è™•ç† (é€²éš)\")\n",
    "        print(\"8. åŸ·è¡Œå®Œæ•´æ¸¬è©¦\")\n",
    "        print(\"0. çµæŸç¨‹å¼\")\n",
    "        \n",
    "        choice = input(\"\\nè«‹é¸æ“‡åŠŸèƒ½ (0-8): \")\n",
    "        \n",
    "        if choice == \"0\":\n",
    "            print(\"\\næ„Ÿè¬ä½¿ç”¨æ–‡å­—è™•ç†å·¥å…·ç®±ï¼\")\n",
    "            break\n",
    "        elif choice in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]:\n",
    "            handle_menu_choice(choice, toolkit)\n",
    "        elif choice == \"8\":\n",
    "            run_complete_test()\n",
    "        else:\n",
    "            print(\"âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥ï¼\")\n",
    "\n",
    "def handle_menu_choice(choice, toolkit):\n",
    "    \"\"\"\n",
    "    è™•ç†é¸å–®é¸æ“‡\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        choice (str): ä½¿ç”¨è€…é¸æ“‡\n",
    "        toolkit (dict): å·¥å…·ç®±ç‰©ä»¶\n",
    "    \"\"\"\n",
    "    # TODO: å¯¦ä½œé¸å–®è™•ç†é‚è¼¯\n",
    "    # é€™è£¡å¯ä»¥æ ¹æ“šé¸æ“‡å‘¼å«å°æ‡‰çš„æ¸¬è©¦å‡½å¼\n",
    "    \n",
    "    menu_map = {\n",
    "        \"1\": test_statistics_module,\n",
    "        \"2\": test_search_module,\n",
    "        \"3\": test_transform_module,\n",
    "        \"4\": test_format_module,\n",
    "        \"5\": test_analysis_module,\n",
    "        \"6\": test_recursive_module,\n",
    "        \"7\": test_functional_module\n",
    "    }\n",
    "    \n",
    "    if choice in menu_map:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        menu_map[choice]()\n",
    "        input(\"\\næŒ‰ Enter ç¹¼çºŒ...\")\n",
    "\n",
    "def run_complete_test():\n",
    "    \"\"\"\n",
    "    åŸ·è¡Œå®Œæ•´çš„å·¥å…·ç®±æ¸¬è©¦\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ§ª é–‹å§‹å®Œæ•´æ¸¬è©¦...\")\n",
    "    \n",
    "    # TODO: é‹è¡Œæ‰€æœ‰æ¸¬è©¦\n",
    "    test_modules = [\n",
    "        (\"çµ±è¨ˆæ¨¡çµ„\", test_statistics_module),\n",
    "        (\"æœå°‹æ¨¡çµ„\", test_search_module),\n",
    "        (\"è½‰æ›æ¨¡çµ„\", test_transform_module),\n",
    "        (\"æ ¼å¼åŒ–æ¨¡çµ„\", test_format_module),\n",
    "        (\"åˆ†ææ¨¡çµ„\", test_analysis_module),\n",
    "        (\"éè¿´æ¨¡çµ„\", test_recursive_module),\n",
    "        (\"å‡½å¼å¼æ¨¡çµ„\", test_functional_module)\n",
    "    ]\n",
    "    \n",
    "    passed_tests = 0\n",
    "    total_tests = len(test_modules)\n",
    "    \n",
    "    for module_name, test_func in test_modules:\n",
    "        try:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"æ¸¬è©¦ {module_name}...\")\n",
    "            test_func()\n",
    "            print(f\"âœ… {module_name} æ¸¬è©¦é€šé\")\n",
    "            passed_tests += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {module_name} æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ æ¸¬è©¦å®Œæˆ: {passed_tests}/{total_tests} å€‹æ¨¡çµ„é€šéæ¸¬è©¦\")\n",
    "    \n",
    "    if passed_tests == total_tests:\n",
    "        print(\"ğŸ† æ­å–œï¼æ‰€æœ‰æ¨¡çµ„éƒ½æ­£å¸¸é‹ä½œï¼\")\n",
    "    else:\n",
    "        print(\"âš ï¸ éƒ¨åˆ†æ¨¡çµ„éœ€è¦æª¢æŸ¥ï¼Œè«‹æª¢è¦–éŒ¯èª¤è¨Šæ¯ä¸¦ä¿®æ­£\")\n",
    "\n",
    "# ä¸»ç¨‹å¼å…¥å£\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»ç¨‹å¼å…¥å£é»\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ æ­¡è¿ä½¿ç”¨ Milestone 04: æ–‡å­—è™•ç†å·¥å…·ç®±\")\n",
    "    print(\"é€™å€‹å°ˆæ¡ˆå±•ç¤ºäº† Ch12-15 çš„æ ¸å¿ƒæ¦‚å¿µï¼š\")\n",
    "    print(\"  â€¢ Ch12: å‡½å¼è¨­è¨ˆåŸºç¤èˆ‡æ¨¡çµ„åŒ–\")\n",
    "    print(\"  â€¢ Ch13: ä½œç”¨åŸŸèˆ‡é–‰åŒ…æ‡‰ç”¨\")\n",
    "    print(\"  â€¢ Ch14: é«˜éšå‡½å¼èˆ‡ Lambda\")\n",
    "    print(\"  â€¢ Ch15: éè¿´æ€ç¶­èˆ‡æ‡‰ç”¨\")\n",
    "    \n",
    "    try:\n",
    "        interactive_menu()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nç¨‹å¼å·²ä¸­æ–·ï¼Œå†è¦‹ï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ç¨‹å¼ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        print(\"è«‹æª¢æŸ¥ç¨‹å¼ç¢¼ä¸¦ä¿®æ­£éŒ¯èª¤ã€‚\")\n",
    "\n",
    "# å–æ¶ˆè¨»è§£ä¾†åŸ·è¡Œä¸»ç¨‹å¼\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ é–‹ç™¼æç¤ºèˆ‡æ³¨æ„äº‹é …\n",
    "\n",
    "### ğŸ” é™¤éŒ¯æŠ€å·§\n",
    "1. **é€æ­¥å¯¦ä½œ**: å®Œæˆä¸€å€‹å‡½å¼å°±ç«‹å³æ¸¬è©¦\n",
    "2. **ä½¿ç”¨ print()**: åœ¨é—œéµä½ç½®è¼¸å‡ºä¸­é–“çµæœ\n",
    "3. **é‚Šç•Œæ¸¬è©¦**: æ¸¬è©¦ç©ºå­—ä¸²ã€Noneã€æ¥µç«¯å€¼\n",
    "4. **ä¾‹å¤–è™•ç†**: ä½¿ç”¨ try-except æ•æ‰å¯èƒ½çš„éŒ¯èª¤\n",
    "\n",
    "### ğŸ“š ç« ç¯€é‡é»å›é¡§\n",
    "- **Ch12**: æ³¨é‡å‡½å¼çš„å–®ä¸€è·è²¬èˆ‡æ¸…æ¥šä»‹é¢\n",
    "- **Ch13**: å–„ç”¨é–‰åŒ…ä¿å­˜ç‹€æ…‹èˆ‡é…ç½®\n",
    "- **Ch14**: ç†Ÿç·´é‹ç”¨ map/filter/reduce èˆ‡ Lambda\n",
    "- **Ch15**: ç¢ºä¿éè¿´æœ‰æ˜ç¢ºçš„çµ‚æ­¢æ¢ä»¶\n",
    "\n",
    "### ğŸ† å®Œæˆæª¢æ ¸\n",
    "åœ¨æäº¤ä½œæ¥­å‰ï¼Œè«‹ç¢ºèªï¼š\n",
    "- [ ] æ‰€æœ‰ TODO éƒ½å·²å®Œæˆ\n",
    "- [ ] æ‰€æœ‰æ¸¬è©¦éƒ½èƒ½é€šé\n",
    "- [ ] ç¨‹å¼ç¢¼æœ‰é©ç•¶çš„è¨»è§£\n",
    "- [ ] å‡½å¼éƒ½æœ‰å®Œæ•´çš„æ–‡ä»¶å­—ä¸²\n",
    "- [ ] å±•ç¤ºäº†å„ç« ç¯€çš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ¯ é–‹ç™¼ç›®æ¨™**: ä¸åªæ˜¯å®ŒæˆåŠŸèƒ½ï¼Œæ›´è¦å±•ç¤ºå° Ch12-15 æ¦‚å¿µçš„æ·±åº¦ç†è§£ï¼\n",
    "\n",
    "**ğŸ’ª åŠ æ²¹ï¼è¨˜ä½ï¼šå„ªç§€çš„ç¨‹å¼è¨­è¨ˆå¸«ä¸åªå¯«å‡ºèƒ½é‹è¡Œçš„ç¨‹å¼ç¢¼ï¼Œæ›´å¯«å‡ºæ¸…æ™°ã€å¯ç¶­è­·ã€å¯æ“´å±•çš„ç¨‹å¼ç¢¼ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}